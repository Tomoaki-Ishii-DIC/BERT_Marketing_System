{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセット作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path_fine_train = (\"./datasets_csv/finetuning/train\")\n",
    "csv_path_fine_test = (\"./datasets_csv/finetuning/test\")\n",
    "csv_path_pred_labeling = (\"./datasets_csv/pred_labeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   date                                              title  \\\n",
      "0   2020-07-25 11:08:00  エンタメ業界は苦しい。でも…　「新型コロナ以前の世界が戻ってくることはない」エイベックスが見...   \n",
      "1   2020-08-03 06:10:00                    オンラインライブ配信はウィズコロナ時代の救世主になれるのか\\n   \n",
      "2   2020-08-11 19:35:00  米Twitchでの音楽使用料に関してミュージシャンから親会社アマゾンのジェフ・ベゾスへ公開書簡\\n   \n",
      "3   2020-09-28 18:20:00  クラシック音楽界の概念を超えた大型オンラインフェスのレポート到着　アーティストによるライブ＆...   \n",
      "4   2020-10-28 08:00:00    「鬼滅の刃」人気は株式市場にも影響　たまごっち登場、グッズ関連にぎわう…“鬼滅銘柄”を物色\\n   \n",
      "5   2020-10-29 08:10:00                  ソニー好決算「上方修正」。その強みはこの1枚のスライドでわかる\\n   \n",
      "6   2020-11-05 16:24:00                                  エイベックス、初の希望退職募集\\n   \n",
      "7   2020-11-06 12:46:00               エイベックスが初の希望退職募集　コロナ禍でライブ・舞台関連事業に打撃\\n   \n",
      "8   2020-11-07 17:01:00                 嵐や白石麻衣は大成功、音楽業界の「勝ち組」がごく一部だという現実\\n   \n",
      "9   2020-11-17 07:00:00           電子チケット販売に自由を。ZAIKO COOが見据える「D2F」という勝ち筋\\n   \n",
      "10  2020-12-10 19:15:00                      音楽業界の未来、実はストリーミングではなくSNSが重要\\n   \n",
      "11  2020-12-21 18:42:00     浜崎あゆみ、クリスマスと大みそかライブの有観客断念を発表　無観客生配信へ「残念ながら…」\\n   \n",
      "12  2020-12-24 15:24:00            エイベックス、本社ビルの売却を発表　通期の純利益は150億円の黒字に転換へ\\n   \n",
      "13  2020-12-25 01:06:00  エイベックス(7860)、「増配」を発表し、配当利回り11.1％に！ 年間配当は1年間で2....   \n",
      "14  2021-01-02 08:02:00          エイベックス本社ビルが「築3年」で売却　入札額トップが落札できなかった奇々怪々\\n   \n",
      "15  2021-01-06 19:00:00  芸能イベントも続々中止・無観客に　今週末開催の『銀魂』『約ネバ』『セーラームーン』舞台あいさ...   \n",
      "16  2021-01-08 09:00:00                     2021年、日本アニメが世界トレンドへ飛躍する節目の年に\\n   \n",
      "17  2021-01-13 11:26:00     ソニー、クリエイティブなエンターテインメント企業へ--世界4拠点からプレスカンファレンス\\n   \n",
      "18  2021-01-13 13:45:00  エイベックス、中国bilibiliとライセンス契約　J-POPのMVを提供　日本の大手レーベ...   \n",
      "19  2021-01-19 15:52:00                 コロナで収入源を失ったプロミュージシャン、過去作を現金化する動き\\n   \n",
      "\n",
      "                                                 text  \n",
      "0   新型コロナウイルスは日本のエンターテインメント業界にも大きな影響を及ぼした。相次ぐライブやイ...  \n",
      "1   新型コロナ感染症の影響で、会場で行われる音楽ライブや演劇、スポーツイベントなどが軒並み苦戦を...  \n",
      "2   　アマゾンの創設者でCEOのジェフ・ベゾスは、先月末の議会聴聞会の証言で、同社の傘下にあるラ...  \n",
      "3   クラシック音楽界の概念を超えた大型オンラインフェスのレポート到着　アーティストによるライブ＆...  \n",
      "4   　少し前までは「鬼滅の刃」を、どう読んでいいか分からなかった大人でも、いまではもう「きめつの...  \n",
      "5   ソニーがコロナ禍の中間決算でも好調だ。\\n10月28日に発表した2021年3月期の2Q決算で...  \n",
      "6   　音楽・映像事業を手掛けるエイベックス（株）（TSR企業コード:294000011、港区、東...  \n",
      "7   　音楽関連の事業を手掛けるエイベックスは11月5日、初の希望退職を募集すると発表しました。同...  \n",
      "8   　全国のファンを釘付けにした、活動休止前の嵐による実質的なラストライブ『アラフェス2020 ...  \n",
      "9   ZAIKO取締役COO　Lauren Rose Kocher氏\\nライブの軒並み中止や、配信...  \n",
      "10  先見の明がある投資家たちの資金が集まるオンラインでのカラオケや音楽制作が、音楽業界にとってま...  \n",
      "11  　歌手の浜崎あゆみの公式サイトが21日に更新。クリスマスの「ayumi hamasaki L...  \n",
      "12  希望退職制度の応募人数は103名\\n　音楽・映像事業を手掛けるエイベックス（株）（TSR企業...  \n",
      "13  　エイベックスは、2021年3月期の配当予想を修正し、前期比で「増配」とする予想を、2020...  \n",
      "14  銀行主導のもと進められた売却プラン\\n売却情報が流れ始めたのは、浜崎あゆみがエイベックス・松...  \n",
      "15  　新型コロナウイルス感染の拡大、翌7日に東京都と埼玉、千葉、神奈川3県を対象とする緊急事態宣...  \n",
      "16  　2020年の最大ヒットコンテンツとなった『鬼滅の刃』。原作コミックは最終23巻で1億200...  \n",
      "17  　ソニーは1月11日午後5時（米国東部時間）から、「CES 2021」においてプレスカンファ...  \n",
      "18  　エイベックスは1月13日、同社の子会社を通じて中国の動画配信サイト「bilibili」を運...  \n",
      "19  英国の作曲家でプロデューサーのイアン・レビン氏が英ポップグループ、「テイク・ザット」（写真）...  \n"
     ]
    }
   ],
   "source": [
    "# ニュース記事\n",
    "\n",
    "#df_finetuning_news = pd.read_csv(csv_path_fine + '/news/news_dataset.csv')\n",
    "df_train_news = pd.read_csv(csv_path_pred_labeling + '/news/news_dataset.csv')\n",
    "\n",
    "#print(df_finetuning_news)\n",
    "print(df_train_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuning train:\n",
      "                                                  text reply good bad\n",
      "0   ハロプロの事務所も昨年末に移転しましたよね\\n都内から都内だけど、複数の拠点を1拠点にまとめ...     1  278  53\n",
      "1   日本の芸能プロダクションも\\nアメリカのエージェントシステムに\\n移行していますね！\\nタレ...     0  235  45\n",
      "2   エイベックスの件はともかくこれから都心の地価は下がるのではないか。これだけリモートワークが推...     3  120  17\n",
      "3   その昔大学が都心から郊外や県外に脱出しました（中央大学や筑波大学など）。その結果として、人気...     1  107  20\n",
      "4   河口湖は東京都心からやや遠いけれど、都心回帰で空きが増えた八王子の大学のキャンパスなんか移転...     2   70   5\n",
      "..                                                ...   ...  ...  ..\n",
      "56                            本当ただの妄想コントでしかない記事だね\\n\\n     0   17   0\n",
      "57                                   浅草をバカにしてるのか？\\n\\n     0   18   1\n",
      "58                 日本のショービジネスの人間はアマチュア。\\n辞めた方が良い。\\n\\n     0   11   3\n",
      "59                                     足立区でよいのでは？\\n\\n     1   10   6\n",
      "60                                八王子あたりでよくない？(笑)\\n\\n     0   21   8\n",
      "\n",
      "[61 rows x 4 columns]\n",
      "finetuning test:\n",
      "                                                 text reply good bad\n",
      "0  役員報酬がバカ高く上層部ばかりが得する仕組みをやってる会社はどこも衰退していくよ。今回社員を...     0  148  16\n",
      "1  もう、馬鹿タレを法外な価格で売りつけ・使うビジネスは、コロナ無くても先細りだった。タレを囲い...     0   50  21\n",
      "2          どの会社でも言えることだけど、希望退職募ると有能から抜けてくからなぁ。。。\\n\\n     0   59  11\n",
      "3  コロナと言うよりは、時代の流れ。\\n時代の流れに乗れない企業は、特にコロナと言う波に耐えられ...     0  105  29\n",
      "4          建物があることに価値がある時代が変わってくるんだろう\\nテレワークの時代で\\n\\n     0    1   0\n",
      "5                 まあ\\nこれまでのツケを\\n返してマイナスになる時が来たな。\\n\\n     0   44   8\n",
      "6             芸能界は飽きられた\\n\\n広告もネットに散った\\n\\n先細りの運命か\\n\\n     0   47   7\n",
      "7                              エイベックスのやり方はもう時代遅れ\\n\\n     0   33   5\n",
      "8                          盛者必衰の理を現す。　もう「時代」じゃ無い\\n\\n     0   34  15\n",
      "9  エイベックスってビル建てて引っ越したばっかりだろ？\\n流石にこれは気の毒としか言いようがない...     0    7  17\n"
     ]
    }
   ],
   "source": [
    "# ニュースコメント\n",
    "\n",
    "import os\n",
    "\n",
    "csv_folder = [csv_path_fine_train , csv_path_fine_test, csv_path_pred_labeling]\n",
    "\n",
    "file_count = 0\n",
    "for p in csv_folder:\n",
    "    file_count += sum((len(f) for _, _, f in os.walk(p + '/comments'))) - 1\n",
    "#print(file_count)\n",
    "\n",
    "for j, p in enumerate(csv_folder):\n",
    "    cols = ['text', 'reply', 'good', 'bad']\n",
    "    df_temp = pd.DataFrame(columns=cols)\n",
    "    \n",
    "    for i in range(file_count):\n",
    "        n_file = str(i+1).zfill(3)\n",
    "        file_name = \"comment_dataset_\" + n_file + \".csv\"\n",
    "        file_path = (p + \"/comments/\" + file_name)\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "        #print(str(i+1).zfill(3))#あとで消す\n",
    "        df_cmt = pd.read_csv(file_path, index_col=0)\n",
    "        #df_temp = pd.concat([df_temp, df_cmt], ignore_index=True)\n",
    "              \n",
    "        #代入\n",
    "        if j <= 1:\n",
    "            df_temp = pd.concat([df_temp, df_cmt], ignore_index=True)\n",
    "        else:\n",
    "            df_cmt.columns = [\"feature\", \"reply\", \"good\", \"bad\"]\n",
    "            df_cmt[[\"feature\", \"good\", \"bad\"]].to_csv(\"./datasets/pred_labeling/features_\" + n_file + \".csv\", index=False)\n",
    "            #df_pred_comments = df_temp.copy()\n",
    "        \n",
    "    #代入\n",
    "    if j == 0:\n",
    "        df_fine_train_comments = df_temp.copy()\n",
    "    elif j == 1:\n",
    "        df_fine_test_comments = df_temp.copy()\n",
    "        \n",
    "print(\"finetuning train:\\n\", df_fine_train_comments)\n",
    "print(\"finetuning test:\\n\", df_fine_test_comments)\n",
    "#print(\"pred labeling:\\n\", df_pred_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels train:\n",
      "        label\n",
      "0   positive\n",
      "1   positive\n",
      "2   positive\n",
      "3   negative\n",
      "4   positive\n",
      "..       ...\n",
      "56  negative\n",
      "57  negative\n",
      "58  negative\n",
      "59  negative\n",
      "60  negative\n",
      "\n",
      "[61 rows x 1 columns]\n",
      "labels test:\n",
      "       label\n",
      "0  negative\n",
      "1  negative\n",
      "2  negative\n",
      "3  negative\n",
      "4  positive\n",
      "5  negative\n",
      "6  negative\n",
      "7  negative\n",
      "8  negative\n",
      "9  positive\n"
     ]
    }
   ],
   "source": [
    "#ラベル\n",
    "import os\n",
    "\n",
    "csv_folder = [csv_path_fine_train , csv_path_fine_test]\n",
    "\n",
    "#file_count = 0\n",
    "#for p in csv_folder:\n",
    "#    file_count += sum((len(f) for _, _, f in os.walk(p + '/comments'))) - 1\n",
    "#print(file_count)\n",
    "\n",
    "for j, p in enumerate(csv_folder):\n",
    "    cols = ['label']\n",
    "    df_temp = pd.DataFrame(columns=cols)\n",
    "\n",
    "    for i in range(file_count):\n",
    "        n_file = str(i+1).zfill(3)\n",
    "        file_name = \"comment_labels_\" + n_file + \".csv\"\n",
    "        file_path = (p + \"/labels/\" + file_name)\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "        #print(str(i+1).zfill(3))#あとで消す\n",
    "        df_labels = pd.read_csv(file_path)#, index_col=0\n",
    "        # concat\n",
    "        #print(df_labels)\n",
    "        df_temp = pd.concat([df_temp, df_labels], ignore_index=True)\n",
    "\n",
    "    #代入\n",
    "    if j == 0:\n",
    "        df_fine_train_labels = df_temp.copy()\n",
    "    elif j == 1:\n",
    "        df_fine_test_labels = df_temp.copy()\n",
    "        \n",
    "print(\"labels train:\\n\", df_fine_train_labels)\n",
    "print(\"labels test:\\n\", df_fine_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの作成と保存\n",
    "\n",
    "datasets_folder =  (\"./datasets/finetuning\")\n",
    "\n",
    "\n",
    "#print(\"finetuning train:\\n\", df_fine_train_comments)\n",
    "#print(\"finetuning test:\\n\", df_fine_test_comments)\n",
    "#print(\"pred labeling:\\n\", df_pred_comments)\n",
    "\n",
    "df_fine_train_comments.columns = [\"feature\", \"reply\", \"good\", \"bad\"]\n",
    "df_fine_train_comments[\"feature\"].to_csv(datasets_folder + \"/train/features.csv\", index=False)\n",
    "\n",
    "df_fine_test_comments.columns = [\"feature\", \"reply\", \"good\", \"bad\"]\n",
    "df_fine_test_comments[\"feature\"].to_csv(datasets_folder + \"/test/features.csv\", index=False)\n",
    "\n",
    "#print(\"labels train:\\n\", df_fine_train_labels)\n",
    "#print(\"labels test:\\n\", df_fine_test_labels)\n",
    "\n",
    "df_fine_train_labels[\"label\"].to_csv(datasets_folder + \"/train/labels.csv\", index=False)\n",
    "df_fine_test_labels[\"label\"].to_csv(datasets_folder + \"/test/labels.csv\", index=False)\n",
    "\n",
    "# 上に持っていく\n",
    "#df_pred_comments.columns = [\"feature\", \"reply\", \"good\", \"bad\"]\n",
    "#df_pred_comments[\"feature\"].to_csv(\"./datasets/pred_labeling/features.csv\", index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#表示用\n",
    "news_file_path = (p + \"/labels/\" + file_name)\n",
    "df_cmt = pd.read_csv(news_file_path, index_col=0)\n",
    "df_temp = pd.concat([df_temp, df_cmt], ignore_index=True)\n",
    "\n",
    "dsp_df_fine_train_comments = df_fine_train_comments[\"feature\"]\n",
    "dsp_df_fine_train_labels = df_fine_train_labels[\"label\"]\n",
    "dsp_df_pred_comments = df_pred_comments[\"feature\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ニュースの例\n",
    "dsp_df_pred_comments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#コメントの例（上のニュースとは無関係）\n",
    "dsp_df_fine_train_comments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ラベルの例（上のニュースとは無関係）\n",
    "dsp_df_fine_train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_finetuning_news' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0a96cc6e0fdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# データセットの表示(一旦保留)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_finetuning_news\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"feature\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_finetuning_news\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0mdf_finetuning_news\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_finetuning_news\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_finetuning_news\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"feature\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_finetuning_news\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_finetuning_news' is not defined"
     ]
    }
   ],
   "source": [
    "# データセットの表示(一旦保留)\n",
    "\n",
    "#df_finetuning_news[\"feature\"] = df_finetuning_news[\"title\"] +  df_finetuning_news[\"text\"]\n",
    "#df_finetuning_news = df_finetuning_news[\"feature\"].copy()\n",
    "#print(df_finetuning_news)\n",
    "\n",
    "\n",
    "df_train_news[\"feature\"] = df_train_news[\"title\"] +  df_train_news[\"text\"]\n",
    "df_train_news = df_train_news[\"feature\"].copy()\n",
    "print(df_train_news)\n",
    "\n",
    "df_finetuning_comments.columns = [\"feature\", \"reply\", \"good\", \"bad\"]\n",
    "df_finetuning_comments = df_finetuning_comments[\"feature\"].copy()\n",
    "print(df_finetuning_comments)\n",
    "\n",
    "df_train_comments.columns = [\"feature\", \"reply\", \"good\", \"bad\"]\n",
    "df_train_comments = df_train_comments[\"feature\"].copy()\n",
    "print(df_train_comments)\n",
    "\n",
    "df_finetuning_labels = df_finetuning_labels[\"label\"].copy()\n",
    "print(df_finetuning_labels)\n",
    "\n",
    "# return (df_finetuning_news[\"feature\"], \n",
    "#                df_train_news[\"feature\"], \n",
    "#                df_finetuning_comments[\"text\"], \n",
    "#                df_train_comments[\"text\"],\n",
    "#                df_finetuning_labels[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4404\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4405\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4406\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-234-0c103442aae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_finetuning_news\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4410\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4411\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlibindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4412\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4413\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.validate_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of bounds"
     ]
    }
   ],
   "source": [
    "df_finetuning_news[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ハロプロの事務所も昨年末に移転しましたよね\\n都内から都内だけど、複数の拠点を1拠点にまとめたみたい\\n\\n経営立て直しする上で、固定費の中でもまず着手するのは家賃ですからね。\\nしんどい状況ですが頑張って欲しいですね\\n\\n'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_finetuning_comments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_finetuning_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'日本の芸能プロダクションも\\nアメリカのエージェントシステムに\\n移行していますね！\\nタレントがエージェントを雇う！\\n立場が逆転するわけですが\\n1世紀かけて本来のマネジメントの\\n姿に戻る訳ですね！\\n\\n'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_finetuning_comments[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_finetuning_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'エイベックスの件はともかくこれから都心の地価は下がるのではないか。これだけリモートワークが推奨されかつ円滑に仕事が進むようなら都心に事務所を構えている会社の多くは売却や移転をするなら考えるのは普通の流れ。\\n\\n'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_finetuning_comments[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_finetuning_labels[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bert-tensorflow\n",
    "#!pip install keras-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tech-blog.cloud-config.jp/2020-02-06-category-classification-using-bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone --recurse-submodules https://github.com/yoheikikuta/bert-japanese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定ファイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features_df:\n",
      "                                               feature\n",
      "0   ハロプロの事務所も昨年末に移転しましたよね\\n都内から都内だけど、複数の拠点を1拠点にまとめ...\n",
      "1   日本の芸能プロダクションも\\nアメリカのエージェントシステムに\\n移行していますね！\\nタレ...\n",
      "2   エイベックスの件はともかくこれから都心の地価は下がるのではないか。これだけリモートワークが推...\n",
      "3   その昔大学が都心から郊外や県外に脱出しました（中央大学や筑波大学など）。その結果として、人気...\n",
      "4   河口湖は東京都心からやや遠いけれど、都心回帰で空きが増えた八王子の大学のキャンパスなんか移転...\n",
      "..                                                ...\n",
      "56                            本当ただの妄想コントでしかない記事だね\\n\\n\n",
      "57                                   浅草をバカにしてるのか？\\n\\n\n",
      "58                 日本のショービジネスの人間はアマチュア。\\n辞めた方が良い。\\n\\n\n",
      "59                                     足立区でよいのでは？\\n\\n\n",
      "60                                八王子あたりでよくない？(笑)\\n\\n\n",
      "\n",
      "[61 rows x 1 columns]\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', 'ハロ', 'プロ', 'の', '事務所', 'も', '昨', '年末に', '移転', 'しま', 'した', 'よ', 'ね', '▁', '都内', 'から', '都内', 'だけ', 'ど', '、', '複数の', '拠点', 'を', '1', '拠点', 'に', 'まとめた', 'みたい', '▁', '経営', '立て直し', 'する', '上で', '、', '固定', '費', 'の中でも', 'まず', '着手', 'するのは', '家', '賃', 'です', 'から', 'ね', '。', '▁', 'し', 'んど', 'い', '状況', 'です', 'が', '頑張', 'って', '欲しい', 'です', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁日本の', '芸能', 'プロダクション', 'も', '▁アメリカの', 'エージェント', 'システム', 'に', '▁', '移行', 'し', 'ています', 'ね', '!', '▁', 'タレント', 'が', 'エージェント', 'を', '雇', 'う', '!', '▁', '立場', 'が', '逆転', 'する', 'わけ', 'です', 'が', '▁', '1', '世紀', 'かけて', '本来の', 'マネジメント', 'の', '▁', '姿', 'に戻る', '訳', 'です', 'ね', '!', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', 'エイベックス', 'の', '件', 'は', 'とも', 'かく', 'これから', '都心', 'の', '地価', 'は', '下がる', 'のではないか', '。', 'これ', 'だけ', 'リ', 'モー', 'ト', 'ワーク', 'が', '推奨', 'され', 'かつ', '円', '滑', 'に', '仕事', 'が進む', 'ような', 'ら', '都心', 'に', '事務所', 'を構え', 'ている', '会社', 'の多くは', '売却', 'や', '移転', 'をする', 'なら', '考える', 'の', 'は', '普通', 'の流れ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁その', '昔', '大学', 'が', '都心', 'から', '郊外', 'や', '県', '外に', '脱出', 'しま', 'した', '(', '中央大学', 'や', '筑波大学', 'など', ')', '。', 'その結果', 'として', '、', '人気', 'が出た', '大学', 'はほとんど', 'なく', '、', '結果的に', '大学', 'ランキングで', 'も', '上位', 'を占める', 'ことが', 'だん', 'だん', 'と', '難しく', 'なった', '過去', 'が', 'あります', '。', '今', 'はその', '反省', 'なのか', '少', '子', '化', '対策', 'なのか', '、', '学生', '集め', 'に', '必死', 'な', '大学', 'は', '東京駅', 'のすぐ', '近くに', 'キャンパス', 'とは', '呼', 'べ', 'ない', 'ビル', 'の', 'フロア', 'を借りて', 'る', 'ぐらい', 'です', '。', '芸能事務所', 'が同じ', 'とは', '思い', 'ません', 'が', '、', '単に', '田舎', 'に', '引っ', '越', 'す', 'の', 'であれば', '、', 'それは', 'コスト', '削減', '以外に', 'は', '意味', 'がない', 'でしょう', '。', '空気', 'が', '綺', '麗', 'な', 'の', 'と', '、', 'タレント', 'が', '仕事を', 'し', 'やす', 'かった', 'り', '、', '事務所', 'が', '反映', 'することは', '関係', 'がない', 'です', 'から', '。', '結局', '、', '支社', 'ということで', '東京', 'に', '小さな', 'オフィス', 'は', '借り', 'る', 'んだ', 'と思います', '。', '間', 'をとって', '浅草', 'というのは', 'アイデア', 'の一つ', 'だ', 'と思います', '。', '移動', 'には', '便利', 'で', '不動産', '価格', 'も', 'ビジネス', '街', 'より', 'は', '高く', 'はない', 'です', 'から', '。', 'あと', 'はその', '街', 'を', '好き', 'かどうか', 'だけ', 'なので', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '河口', '湖', 'は', '東京都', '心', 'から', 'やや', '遠い', 'けれど', '、', '都心', '回帰', 'で', '空き', 'が増えた', '八王子', 'の', '大学の', 'キャンパス', 'なん', 'か', '移転', '先の', '候補', '地に', '上', 'が', 'らない', 'の', 'かな', '?', '▁', '所属の', 'タレント', 'が', '日常的に', '使う', 'レッスン', '場', 'や', 'プロモーション', 'ビデオ', 'の撮影', 'などにも', 'って', 'こ', 'いだ', 'と思う', 'けれど', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '会長', 'と', '本社', 'が', '河口', '湖', 'へ移転', 'して', '▁', '実務', 'の', '社員', 'は', 'TV', '局', 'や', '都心', 'に近い', '雑', '居', 'ビル', 'に残る', 'の', 'では', '。', '▁', '取引', '先', 'が', 'そんな', '感じ', 'で', '青山', 'から', '地方', 'と', '郊外', 'に', '分散', '移転', 'した', 'よ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '昨', '年', 'は', '、', '大手', 'の', '芸能事務所', 'から', 'の独立', '・', '退社', 'が相次ぎ', 'ました', '。', '今', '年', 'は', '、', 'さらに', '増え', 'る', 'でしょう', '。', '大手', 'の', '事務所', 'も', '、', '看板', 'と言われる', '人を', '月', '給', '制', 'で', '維持', 'し', 'てゆく', 'には', '環境', 'が', '厳しい', '。', 'と言って', '、', 'いきなり', '歩', '合', '制', 'にも', 'できない', '。', 'ならば', '、', '赤坂', 'や', '青山', 'や', '表', '参道', 'に', '事務所', 'を置く', '理由', 'も', 'ないし', '、', '今', 'の', '次期', 'です', 'から', '、', '事務所', 'も', 'リ', 'モー', 'ト', 'ワーク', 'に', '徹', 'すれば', '良い', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '吉本興業', '東京', '本部', 'が', '2008', '年から', '使', 'っている', 'の', 'は', '「', '旧', '新宿', '区', '立', '四', '谷', '第五', '小学校', '」', '。', '1934', '年', '竣工', 'の', '昭和', 'モ', 'ダ', 'ニズム', '建築', 'に', 'そっくり', 'そのまま', '入', 'っている', '。', '▁', '歌舞伎', '町', '、', '新宿', 'ゴールデン', '街', '、', '花園', '神社', 'のすぐ', '近く', '。', 'これは', '良い', '建物', '良い', '場所', 'を見つけた', 'な', 'と思った', 'な', '。', '▁', '地', '代', 'や', '賃', '料', 'が高い', 'とはいえ', '、', '東京都内', 'に', 'はまだ', 'まだ', '知られていない', '穴', '場', '的', '場所', 'がある', '。', '空き', '物件', 'も増え', 'ている', 'し', '、', '意', '外', 'な', 'ところに', '意', '外', 'な', '事務所', 'がある', 'って', '言う', 'の', 'も', '、', '今後', 'は', '売り', 'になる', 'んじゃ', 'ないか', '?', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '海外', 'ファンド', 'からの', '買い', 'が多く', 'て', '東京', '都心部', 'の', '100', '単位', 'の', '不動産', 'は', '上がって', 'います', '。', 'エイベックス', 'は', '2', '90', '億円', 'もの', '売却', '益', 'を得て', '、', 'イベント', 'の', '売上', '減少', 'に備えて', 'しっかり', 'キャッシュ', 'を', '確保', 'できた', 'の', 'で', '良い', 'ときに', '売', 'った', 'の', 'では', '。', '▁また', 'イベント', '売上', '減', 'で', 'トータル', 'の利益', 'はかなり', '減', 'っている', '今', 'なら', '売却', '益', 'による', '納税', '額', 'も', '圧縮', '出来る', 'でしょう', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '新たな', 'コロ', 'ナ', 'の', '出現', '、', '地球温暖化', 'による', '異常', '気象', 'が', 'も', 'た', 'らす', '台風', 'の', '大型化', '、', '集中', '豪雨', 'に伴う', '都心', 'の', '河川', 'の', '氾濫', '、', '南極', '氷河', 'が', '溶け', 'て', '20', '年後に', 'は', '海面', 'が', '1', 'm', '上昇', 'する', 'と言われている', '。', '21', '年', 'は', '今まで', 'の', '価値観', 'が大きく', '変わる', '年', '。', '都会', '、', '東京', 'に住む', 'ことが', 'ステイ', 'タス', 'だった', 'ことが', 'これから', 'は', 'リスク', 'になる', '時代', 'なん', 'でしょう', '。', 'ここに', '既に', '気', 'がついた', '人', '達が', '避難', 'している', 'の', 'でしょう', '。', '繁栄', 'を', '謳', '歌', 'し', '過ぎた', '人間', 'への', 'し', 'っぺ', '返し', '以外の', '何も', 'の', 'でもない', 'でしょう', '。', '▁', '仕事', 'も', '学校', 'も', 'リ', 'モー', 'ト', '化', 'で', '都心', 'に住む', '必要', 'も', '無い', 'ことに', '気づいた', '人', '達', '、', '言', 'わ', 'ば', '変化', 'に対応', '出来る', '人', 'だけが', '生き延び', 'る', '。', 'そんな', '世界', 'が', 'き', 'ています', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁東京', 'において', '、', '浅草', '無く', 'して', '芸', '事', 'は無い', '。', '▁', 'です', 'の', 'で', '、', '浅草', 'に', '事務所', 'を移す', 'なんて', '、', '▁', '下に', '見た', '言い', '方は', 'まず', 'いか', 'と思います', 'が', '...', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '河口', '湖', 'から', '1.5', '時間', 'で', '東京', '...', '。', '新宿', 'なら', 'いい', 'けど', 'そこから', '先', 'だと', '速度', 'オーバー', 'になる', 'と思う', 'けど', 'な', '。', '▁ただ', '中央', '道', 'の', '渋滞', 'を', '知らない', 'の', 'かな', 'ぁ', '。', '▁', '平日', 'の', '日', '中でも', '上り線', 'は', '混', 'んでいる', 'し', '土', '日', 'も', '朝', '一番', 'は', '上り線', 'は', '混', 'んでいる', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁ある日', '青山', '通り', 'を', '歩いて', 'いた', 'ら', '、', 'なん', 'か', '特別な', '存在感', 'のある', 'ビル', 'の前で', '立ち', '止まった', '。', '▁', 'エイベックス', '?', '?', 'あの', 'エイベックス', 'の', 'ビル', '?', 'と', '視', '線を', '上に', 'して', 'ビル', 'を見', '上げた', '時の', '感覚', 'が', '忘れられ', 'ない', '。', '広く', '取った', '敷地', 'といい', '圧', '巻', 'で', 'した', '。', '▁', 'なんとか', 'の', '夢', 'の', 'あと', '?', 'って', '感じ', 'です', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁お', '店', 'で', '言う', '薄', '利', '多', '売', 'ではない', 'です', 'が', 'オフィス', 'ビル', '、', 'マンション', '、', '店舗', 'の', 'オーナー', 'さん', 'も', '長期', '機関', '借り', 'てくれた', '店', '子', 'さん', '含め', '今後', 'は', '空', 'いた', 'ら', '特に', '都心', 'の', 'ビル', '等', 'は', '厳しい', 'でしょう', 'から', '家', '賃', 'を', '下げ', 'て', 'でも', 'そのまま', '契約を', '更新', 'した', '方が', '家', '賃', '収入', '0', 'より', 'は', '良い', 'の', 'では', '!', '売れ', '残り', 'を残す', 'より', 'セール', 'で', '売り', 'さ', 'ばく', 'の', 'と同じ', 'では', '?', '特に', '家', '賃', '収入', 'は', '商品', 'で', '言えば', '1', '個', 'だけ', '仕', '入れ', 'て', '複数の', '人に', '販売する', 'の', 'と同じ', 'です', 'よ', 'ね', '!', 'これから', 'は大', '家', 'さん', 'も', '欲', 'は', '欠', 'かない', '!', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '浅草', 'は', '江戸時代', 'から', '歌舞伎', 'や', '大', '道', '芸人', 'を', '隔離', 'した', '猿', '若', '町', 'もある', 'し', '。', 'DEEP', 'で', '面白い', '歴史', 'の下', '町', '。', 'たけし', 'の', '足立', '区', 'や', '墨田区', 'も', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '大手', '事務所', 'も', '「', 'コロ', 'ナ', '禍', '」', 'では', '、「', '観客', '動員', '」', 'の', 'イベント', 'が', '出来ない', '。', '▁', '経営', 'の', '負担', 'となる', '都心', 'の', 'び', 'る', 'の', '固定', '資産', '税', 'を考える', 'と', '売却', 'も', '正解', 'だろう', '。', '▁', 'ネット', '配信', 'で', '有', 'れば', '「', '撮影', '資材', 'と', '場所', '」', 'は日本', '中に', '有る', '。', '今後', 'は', '各社', 'ネット', '▁', 'による', '営業', '展開', 'にも', '力', 'が入る', 'だろう', '。', '総務省', 'も', 'NHK', 'が', '使用している', '電波', 'を', '▁', 'こ', 'の様な', '業態', 'に', '入札', 'させて', 'は', 'どう', 'だろうか', '?', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', 'エイベックス', 'は完全に', '音楽', '配信', 'サービス', 'を', '軽', '視', 'して', '、', 'その', '波', 'に乗り', '遅れた', 'ツ', 'ケ', 'が', '回', 'って', '来た', 'って', '感じ', 'かな', '?', '▁', '何', '処', 'の', 'レコード会社', 'も', 'C', 'D', 'や', 'DVD', 'を', '売', 'って', 'は', '、', 'それを', '資金', 'に', 'ライブ', 'で', '儲け', 'る', '手法', 'は', '、', 'この', 'コロ', 'ナ', '禍', 'で', '完璧', 'な', 'までに', '壊れ', 'た', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '都内', 'から', '河口', '湖', '付近に', '通勤', 'するのは', '無理', 'と思う', '社員', 'が多い', 'でしょう', '。', '▁', '出', '勤', 'してから', '都内', 'に', '出張', 'する', 'なんて', '・', '・', '・', '・', '。', '▁', '発案', '者は', '1', '週間', '、', '都内', 'から', '河口', '湖', '近辺', 'に', '通勤', '体験', 'した', 'ら', '良い', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '青山', 'あたり', 'に', '本社', 'を構える', '会社は', '今後', 'は', '蒲田', 'や', '赤羽', 'や', '錦', '糸', '町', 'や', '小', '岩', 'に移転', 'していき', 'そう', 'である', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '事務所', 'の', 'コスト', 'カット', 'による', '移転', 'や', '事業', '縮小', 'による', 'タレント', 'や', 'スタッフ', 'の', 'リスト', 'ラ', 'が発生し', 'てしまった', '時点で', '結局', '耐え', 'られなかった', 'ことになる', 'から', '仕', '方', 'ない', 'では', '済', 'ま', 'されない', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '河口', '湖', 'いい', 'や', 'ん', '!', '▁', '昼', '休み', 'には', '釣り', 'が', '出来る', '▁', 'タレント', 'も', 'マネージャー', 'も', 'オフィス', 'に出', '勤', 'する', '意味', 'はない', 'から', 'い', 'いん', 'じゃない', '▁', 'ま', 'ぁ', '河口', '湖', 'は', '極', '端', 'だけ', 'ど', '千葉', '・', '神奈川', '・', '埼玉', 'なんて', '安い', '土地', 'や', '不動産', 'いく', 'ら', 'でもある', 'から', 'その', '辺', 'で', 'もいい', 'んじゃ', 'ない', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '完全', '所有権', 'から', '区分', '所有権', 'にして', '区分', 'を', '売却', '、', '持', '分割', '合', 'にした', 'ら', 'コ', 'ツ', 'コ', 'ツ', '売却', '資金', '手に', '出来た', 'の', 'に', 'な', '。', '▁', 'ま', 'ぁ', '金融機関', 'が', 'ウン', 'とは', '言', 'わなかった', 'か', '。', '▁', 'アイデア', 'がなかった', 'か', '。', '。', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁同じ', '移転', 'なら', '、', '三島', 'の方が', 'よかった', 'の', 'では', '。', '▁', '三島', 'なら', '、', '東京', '駅まで', 'こ', 'だ', 'まで', 'も', '1', '時間', 'あ', 'れば', '着く', '近', 'さ', 'な', 'の', 'に', '・', '・', '・', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '芸能事務所', 'が', '一般に', '目に', '付き', 'やすい', 'から', '記事', 'に', 'されて', 'る', 'だけで', '、', '▁', '普通', 'に', 'B', 'to', 'B', '企業', 'でも', '脱', '都心', 'の', '案件', 'は', '数', '多い', 'だろう', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '青山', 'に', '本社', 'を構える', 'エイベックス', '、', '今後', 'は', '都内', 'の', '山手線', 'の', '外側', 'の', 'エリア', 'なのか', '、', '埼玉県', 'の', '和', '光', '市', 'や', '朝', '霞', '、', '志', '木', '、', '川越', '、', '森林', '公園', '、', '小川', '町', '、', '寄', '居', '、', '児玉', '、', 'そして', '群馬県', 'の', '高崎', 'に', '本社', 'を', '移転', 'して', 'き', 'そう', '?', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '浅草', 'も', '土地', '代', 'は', '安', 'くない', 'が', '・', '・', '・', '▁', '朝日新聞', '出版', 'も', '、', '郊外', 'に移転', 'すれば', 'いい', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '芸能人', 'にとっては', '、', '演劇', 'や', 'ライブ', 'は', '明日', 'の', '食い', '扶', '持', 'を得る', 'ための', '生きる', '為', 'の', '必要な', '行動', 'だ', '。', '▁', '決して', '不要', '不', '急', 'なん', 'か', 'ではない', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '昔', '、', 'この', 'ビル', 'の', '1', '階', 'で', '無料で', 'インターネット', 'を', 'させていた', 'だ', 'き', 'ました', '。', '感謝', 'し', 'ています', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', 'レ', 'プロ', 'も', '目黒', 'から', 'お茶', 'の水', 'に', '引っ', '越', 'す', 'よ', 'ね', '。', '▁', 'お茶', 'の水', 'は', '出版社', 'が多い', '神', '保', '町', 'にも', '近い', 'し', '、', '局', 'も', 'お', '台', '場', '以外は', '意', '外', 'と', '近い', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '世田谷区', 'や', '大', '田', '区', 'は', '空港', 'も', '近い', 'し', '東', '名', 'も', '近い', 'ぜ', '▁大', '田', '区', 'は', '高速', '乗', 'った', 'ら', '都内', 'の東側', 'でも', '早く', '着く', '▁', 'オス', 'ス', 'メ', 'だ', 'ぜ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '真相', 'は', '実態', 'を', '伴', 'わない', '株', '高', 'と', '近い', '将来の', '不動産', '価格', 'の', '下落', 'を見', '越', 'して', 'だろう', '。', '▁', 'お金', '持ち', 'の', '嗅', '覚', 'は', '鋭い', 'よ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁「', 'ザ', '・', 'ドリ', 'フ', 'ター', 'ズ', '、', 'ビート', 'たけし', '、', '渥美', '清', '、', '萩', '本', '欽', '一', '、', '美', '空', 'ひばり', '、', '浅', '香', '光', '代', '、', '淡', '谷', 'の', 'り', '子', '、', '村田', '英雄', '、', '伊東', '四', '朗', '、', '水の', '江', '瀧', '子', '、', '沢村', '貞', '子', '、', '永', '六', '輔', 'ら', '・', '・', '・', '」', '▁', 'ビート', 'たけし', '▁→', '▁', 'ツー', '・', 'ビート', '▁', '萩', '本', '欽', '一', '▁→', '▁', 'コント', '55', '号', '▁', '伊東', '四', '朗', '▁→', '▁', 'て', 'んぷ', 'く', 'トリオ', '▁', 'と', '書いて', '欲', 'しかった', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '芸能事務所', 'だけの', '問題', 'じゃ', 'なく', '、', '事務所', 'そのもの', 'の需要', 'が', 'コロ', 'ナ', '後', '考える', '会社', '多く', '出てくる', '。', 'コロ', 'ナ', 'の', '感染', 'どこ', 'で', '終', '息', 'なる', 'か', 'だ', 'な', '。', '1', '極', '集中', 'が', '悪', 'だ', 'わ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '浅草', 'に', '来', 'ないと', '歴史', 'が変わる', '!', '葛飾', '郡', 'の歴史', '▁', '埋', '立地', 'はない', '時代', 'を', '話', 'せ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '相撲', '部屋', 'も', '今', 'じゃ', '両国', '辺り', 'だけ', 'じゃ', 'なく', '▁西', '新井', '、', '松戸', 'に', 'もある', 'から', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', 'テレビ', 'に', '出す', 'より', '▁', 'Y', 'out', 'u', 'be', 'を', '頑張', 'った', 'ほう', 'が', '初期', '投資', 'を', '安', 'く', '抑え', 'れる', 'もの', 'な', '。', '▁', '撮影', 'も', '映画', 'も', '▁', 'テレビ', 'で', 'わざ', 'わざ', '流', 'す', '必要', 'がない', '▁', 'Y', 'ou', 'T', 'u', 'be', 'の方が', '遥かに', '優れている', '媒体', 'だ', 'と思う', '。', '▁今', 'の', 'テレビ局', 'って', '人', 'による', '技術', 'とか', 'が', '優位', '性を', '保', 'て', 'ている', 'だけで', '、', '白紙', 'から', '何か', 'を作ること', 'に', 'は全く', '長', 'け', 'ていない', '。', '▁', 'Y', 'ou', 'T', 'u', 'be', 'から', '新しい', 'チャンネル', 'が出来', 'ていくこと', 'を', '期待', 'し', 'ています', '。', '先行', '例は', 'いく', 'ら', 'でも', 'あります', 'し', '芸人', 'さん', 'にとっては', '本当に', '0', 'ではなく', '1', 'からの', 'チャンス', 'だ', 'と思います', 'よ', 'ね', '。', '▁', '撮影', 'できれば', 'なん', 'でも', '出来る', 'わけ', 'です', 'から', '。', '▁', '賢', 'い', '人は', 'それを', '規制', 'しようと', '考える', 'でしょう', 'が', '、', '▁', 'そもそも', '危', 'ないもの', 'は', 'Y', 'ou', 'T', 'u', 'be', 'で', '自動', '削除', 'かかり', 'ます', 'から', 'ご', '安心', 'を', '。', '▁', 'テレビ', 'の', '既存の', '考え方', 'は', '終わり', 'に向かう', 'ということ', 'は', 'もはや', '既', '定', '路線', 'です', 'の', 'で', '、', '▁', '今まで', '他人', 'の', '不幸', 'で', '食', 'ってきた', 'マスコミ', 'の', '罪', '深さ', '、', 'これは', '発信', '者が', 'と', 'こと', 'ん', '自家', '中毒', 'として', '味', 'わ', 'って', '頂', 'き', 'たい', 'と思います', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '河口', '湖', 'なんて', '有り', '得ない', '。', '▁また', '、', '一等', '地', 'から', '浅草', 'へ', 'って', '、', '浅草', 'も', '安', 'く', '見られた', 'も', 'んだ', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', 'ファン', 'が', 'さ', 'さえ', 'てくれる', 'アーティスト', 'は', 'がん', 'が', 'れる', 'と思います', '。', 'JUJU', '長', '渕', 'さだ', 'まさ', 'し', 'ゆ', 'ず', '中島', 'みゆき', 'サザン', '矢', '沢', '長', '渕', 'ゴ', 'スペ', 'ラーズ', 'ユー', 'ミン', 'いき', 'ものが', 'かり', 'mi', 'ya', 'vi', 'オレンジ', 'グリーン', '佐野', 'とか', '思い', 'つく', '。', 'みんな', 'avex', 'じゃない', 'の', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '移転', 'しても', '▁', 'エイベックス', 'は', '持たない', 'だ', 'ろ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '芸能人', 'も', '水', '商売', 'だから', 'ね', '。', '▁元々', '価値', 'の無い', 'ものに', '人為的', 'に', '価値', 'をつけ', 'ている', 'から', '落ちる', '時は', '早い', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', 'レ', 'プロ', 'も', '目黒', 'から', '御', '茶', 'ノ', '水', 'へ移転', 'する', 'し', 'ね', '。', '▁', 'ま', 'あ', '事務所', 'は', '山手線', 'の西側', 'が', 'おお', 'い', 'よ', 'ね', '〜', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', 'ま', 'あ', '港区', 'とか', '目黒', '区', 'とか', 'じゃ', 'なくても', '、', '都内', 'で', '家', '賃', 'が', '安い', 'ところ', 'なら', 'い', 'いん', 'じゃない', '。', '▁', '河口', '湖', 'って', '...', '研修', 'なら', 'いい', 'が', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', 'TBS', 'の', '緑', '山', 'や', 'カン', 'テレ', 'の', 'レモン', 'スタジオ', '&', '東京', 'メディア', 'シティ', 'に近く', '、', '比較的', '撮影', 'の多い', '京王', '線', 'にも', '近い', '狛', '江', 'か', '多摩', 'センター', '、', '町田', 'あたり', 'が', '良', 'さ', 'そう', 'だ', 'な', '。', '▁', '後', '、', '東京', 'にも', '近い', '新', '横浜', 'か', '浦', '安', '、', '幕張', 'あたり', 'も', '。', '▁', '千葉', 'なら', 'ディズニー', 'ランド', 'や', '成田空港', 'がある', 'し', '、', 'この', 'ところ', '北', '総', '線', 'や', '東', '葉', '高速', 'で', 'の撮影', 'も多く', 'なっている', 'から', '最適', 'か', 'も', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', 'avex', 'って', 'レコード会社', 'で', 'しょ', '?', '▁', '事務所', 'でもある', 'んだ', 'ろう', 'けど', '.', '.', '▁', 'ま', 'ぁ', '会長', 'が', '薬物', '疑惑', 'ある', '会社', 'なんて', '、', 'あ', 'ぁ', 'なる', 'よ', 'ね', '。', '笑', 'える', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', 'うち', 'の', '会社', 'も', '移転', 'しま', 'した', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '逮捕', 'は', 'いつ', '?', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '薄', 'っぺ', 'らい', '記事', 'だ', 'な', 'あ', '。', '▁「', '聞き', 'ました', '」', 'とか', '▁「', '某', '事務所', '」', 'とか', '内容', '皆無', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '色', '々', 'もう', '終わり', 'だ', 'な', '、', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '不要', '不', '急', 'だから', 'な', '(', '笑', ')', '...', '都心', '一等', '地', 'なんて', '会社', 'があって', 'いい', '気', 'になって', 'る', 'けど', 'それ', 'で', 'ケ', 'チ', 'ケ', 'チ', 'して', '利益', 'だけ', '伸ばし', 'てる', 'から', 'な', '...', 'バカ', 'バカ', 'しい', '!', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '河口', '湖', '?', '▁', 'ホン', 'マ', 'かい', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '調布', 'あたり', 'がい', 'いん', 'じゃない', '?', '石原', '軍団', 'の', '事務所', 'って', '調布', 'だよ', 'ね', '▁', '映画の', '撮影所', 'とか', 'スタジオ', 'とか', '色', '々', '近い', 'し', '芸能人', '結', '構', '住', 'んで', 'る', 'よ', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁大', '▁', '歓', '▁', '迎', '!', '▁', 'どんどん', '地方', 'へ移転', 'を', 'お', '願い', 'いた', 'します', '。', '▁', '企業', 'も', '地方', '出身者', 'も', '都内', 'ではなく', '生まれた', '地域', 'で学び', '就職', 'し', '起業', 'し', '事業', '活動', 'して', 'ください', '!', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁2010', '年ごろ', '、', 'この', 'ビル', 'に入って', 'る', '企業', 'に行く', '為に', 'エレベーター', 'に乗った', '時に', 'avex', '社員', 'ら', 'しき', '女', 'の', '人が', '薄', '手の', '服', 'に', '巨', '乳', 'を', '露出', 'しま', 'くり', 'で', '入', 'ってきた', '事を', '思い出', 'します', '。', '仕事', '終わり', 'に', '、', 'そのまま', 'クラブ', 'に行く', 'つもり', 'だった', 'んだ', 'ろう', 'な', 'と', '。', 'イ', 'ケイ', 'ケ', 'の時代', 'の名残', 'で', 'したが', '、', 'あの', '時は', '良かった', 'という', '事', 'でしょう', 'か', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '浅草', 'は', '都心', 'の', '一等', '地', 'じゃない', 'のか', 'w', '▁', '年', '寄り', 'には', '人気', 'ある', 'けど', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '仕事', '上は', 'そんな', '一等', '地', 'じゃ', 'なくても', '全', '然', '支障', 'ない', 'から', 'ね', '。', '当然', 'だ', 'ろ', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '都心', 'の', '一等', '地に', '事務所', 'なんて', '見', '栄', '以外の', '何', '者', 'でもない', 'から', 'な', '。', '結果', 'エー', 'ベック', 'ス', 'みたい', 'になる', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁本', '当', 'ただ', 'の', '妄想', 'コント', 'で', 'しかない', '記事', 'だ', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '浅草', 'を', 'バカ', 'にして', 'る', 'のか', '?', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁日本の', 'ショー', 'ビジネス', 'の', '人間', 'は', 'アマチュア', '。', '▁', '辞め', 'た', '方', 'が良い', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '足立', '区', 'で', 'よい', 'の', 'では', '?', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '▁', '八王子', 'あたり', 'で', 'よく', 'ない', '?', '(', '笑', ')', '[SEP]']\n",
      "[60, 46, 52, 178, 53, 46, 87, 109, 71, 139, 33, 58, 63, 112, 36, 89, 64, 48, 26, 35, 62, 48, 36, 33, 57, 24, 33, 23, 41, 37, 33, 94, 42, 21, 21, 206, 27, 54, 11, 24, 30, 35, 85, 37, 11, 7, 23, 10, 41, 11, 36, 38, 69, 21, 22, 24, 13, 10, 16, 10, 12]\n",
      "max_token_number: 206\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "\n",
    "# 学習データX\n",
    "# feature.csvは上記で用意したファイルのパスを指定してください\n",
    "train_features_df = pd.read_csv('./datasets/finetuning/train/features.csv')\n",
    "print(\"train_features_df:\\n\", train_features_df)\n",
    "\n",
    "\n",
    "# テキストの最初に[CLS]、最後に'[SEP]をつけて単語数を数える関数\n",
    "def _get_indice(feature):\n",
    "    tokens = []\n",
    "    tokens.append('[CLS]')\n",
    "    tokens.extend(sp.encode_as_pieces(feature))# sentence piece\n",
    "    tokens.append('[SEP]')\n",
    "    print(\"テキストのデータ :\\n\",tokens)\n",
    "    number = len(tokens)\n",
    "    return number\n",
    "\n",
    "# sentence pieceでは、与えた文書の中で高い頻度で現れるフレーズは、\n",
    "# 多少長くても一つの単位として認識します。\n",
    "# Mecabでは対応する辞書を使って文章を分割します。辞書にはneologdとかがよく使われます。\n",
    "#これでも上手くいくことも多いですが、語彙数が大きくなってしまうことや、\n",
    "# 分割の仕方が分割したいデータセットに適していないこともあり、問題点\n",
    "sp = spm.SentencePieceProcessor()\n",
    "\n",
    "# ダウンロードした事前学習モデルのパスを指定してください\n",
    "sp.Load('./downloads/bert-wiki-ja/wiki-ja.model')\n",
    "\n",
    "numbers = []\n",
    "for feature in train_features_df['feature']:\n",
    "    features_number = _get_indice(feature)\n",
    "    numbers.append(features_number)\n",
    "\n",
    "print(numbers)\n",
    "\n",
    "# 最大トークン数\n",
    "max_token_num = max(numbers)\n",
    "print(\"max_token_number: \" + str(max_token_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTの設定ファイル、モデルのロード  \n",
    "学習回数と事前に調べていた最大トークン数、ファイルパスを自分用に書き換えてください。以下に書き換える箇所を示します。  \n",
    "\n",
    "- config_path：設定ファイルのパス  \n",
    "- checkpoint_path：事前学習モデルのファイルパス  \n",
    "    - 拡張子まで書かないでください  \n",
    "- SEQ_LEN：最大トークン数  \n",
    "- EPOCH：学習回数  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        [(None, 206)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      [(None, 206)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 206, 768), ( 24576000    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 206, 768)     1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 206, 768)     0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 206, 768)     158208      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 206, 768)     0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 206, 768)     1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 206, 768)     0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 206, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 206, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 206, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 206, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 206, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 206, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 206, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 206, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 206, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 206, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 206, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 206, 768)     0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 206, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 206, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 206, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 206, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 206, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 206, 768)     0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 206, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 206, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 206, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 206, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 206, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 206, 768)     0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 206, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 206, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Dense (Dense)               (None, 206, 768)     590592      Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Norm (LayerNormalization)   (None, 206, 768)     1536        MLM-Dense[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Sim (EmbeddingSimilarity)   (None, 206, 32000)   32000       MLM-Norm[0][0]                   \n",
      "                                                                 Embedding-Token[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Masked (InputLayer)       [(None, 206)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "NSP-Dense (Dense)               (None, 768)          590592      Extract[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "MLM (Masked)                    (None, 206, 32000)   0           MLM-Sim[0][0]                    \n",
      "                                                                 Input-Masked[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "NSP (Dense)                     (None, 2)            1538        NSP-Dense[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 111,008,002\n",
      "Trainable params: 111,008,002\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "\n",
    "#sys.pathに追加（必要なのか調査が必要）\n",
    "sys.path.append('modules')\n",
    "\n",
    "#import pprint\n",
    "#pprint.pprint(sys.path)\n",
    "\n",
    "# BERTのロード\n",
    "config_path = './downloads/bert-wiki-ja_config/bert_finetuning_config_v1.json'\n",
    "# 拡張子まで記載しない（.ckptファイルで保存されている）\n",
    "checkpoint_path = './downloads//bert-wiki-ja/model.ckpt-1400000'\n",
    "\n",
    "# 最大のトークン数\n",
    "SEQ_LEN = max_token_num#上の処理の出力\n",
    "BATCH_SIZE = 16\n",
    "BERT_DIM = 768\n",
    "LR = 1e-4\n",
    "# 学習回数\n",
    "EPOCH = 20\n",
    "\n",
    "# 学習ずみモデルでモデル構築\n",
    "bert = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True,  trainable=True, seq_len=SEQ_LEN)\n",
    "bert.summary()\n",
    "\n",
    "# この後に追加する（転移学習）\n",
    "# 分類問題用にモデルの再構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データのロード関数\n",
    "こちらの関数でもモデルの読み込みを行うので、各自ファイルパスの変更をお願いします。\n",
    "\n",
    "- sp.load(\"ファイルパス\")\n",
    "\n",
    "## 文章のベクトル化\n",
    "_get_indice関数では、SentencePieceとwikipediaモデルを使用し文章のベクトル化を行っています\n",
    "\n",
    "## 学習データ読込\n",
    "_load_labeldata関数は学習データを読込、_get_indice関数を用いて特徴量を抽出しています。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import utils\n",
    "import numpy as np\n",
    "\n",
    "# ここでもsentence piece\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('./downloads/bert-wiki-ja/wiki-ja.model')\n",
    "\n",
    "# 上に同じ名前の関数があるので注意\n",
    "# 最大単語数分のID化された文を返す関数\n",
    "# maxlenがなくてエラーになるので勝手に追加（maxlenは最大単語数か？）\n",
    "def _get_indice(feature, maxlen):\n",
    "#def _get_indice(feature):\n",
    "    # インデックス ０で埋める\n",
    "    indices = np.zeros((maxlen), dtype = np.int32)\n",
    "    # 最初に[CLS]、最後に'[SEP]をつけてトークン作る\n",
    "    tokens = []\n",
    "    tokens.append('[CLS]')\n",
    "    tokens.extend(sp.encode_as_pieces(feature))# sentence piece\n",
    "    tokens.append('[SEP]')\n",
    "\n",
    "    for t, token in enumerate(tokens):\n",
    "        # 最大単語数までトークンの単語をindicesに入れていく\n",
    "        if t >= maxlen:\n",
    "            break\n",
    "        try:\n",
    "            indices[t] = sp.piece_to_id(token)# id化してくれる？\n",
    "        except:\n",
    "            logging.warn(f'{token} is unknown.')# コメントしてくれる\n",
    "            indices[t] = sp.piece_to_id('<unk>')# id化してくれる？unknown\n",
    "    \n",
    "    # 最大単語数分のID化された文を返す\n",
    "    return indices\n",
    "\n",
    "\n",
    "#勝手に追加 maxlen=103  \n",
    "# 引数のパスは直接書けばいらないかも\n",
    "def _load_labeldata(train_dir, test_dir, maxlen):\n",
    "    # pandasでcsvの学習データとテストデータを読み込む\n",
    "    train_features_df = pd.read_csv(f'{train_dir}/features.csv')\n",
    "    train_labels_df = pd.read_csv(f'{train_dir}/labels.csv')\n",
    "    test_features_df = pd.read_csv(f'{test_dir}/features.csv')\n",
    "    test_labels_df = pd.read_csv(f'{test_dir}/labels.csv')\n",
    "    \n",
    "    ##### ラベル側の処理 #####\n",
    "    \n",
    "    # ラベルのユニーク値を取り出す（ラベル数）（インデックスとラベル別別に保管）\n",
    "    # ネガポジなら　ポジティブ, ネガティブ と　０、１　を入れてしまえばいいと思われる\n",
    "    #{'スポーツ': 0, '携帯電話': 1},\n",
    "    label2index = {k: i for i, k in enumerate(train_labels_df['label'].unique())}\n",
    "    #{0: 'スポーツ', 1: '携帯電話'}\n",
    "    index2label = {i: k for i, k in enumerate(train_labels_df['label'].unique())}\n",
    "    #　クラス数（何種類に分類するか）ネガポジなら２\n",
    "    class_count = len(label2index)\n",
    "    \n",
    "    # Numpyユーティリティ to_categorical(y, nb_classes=None)\n",
    "    # クラスベクトル（0からnb_classesまでの整数）を categorical_crossentropyとともに用いるためのバイナリのクラス行列に変換します．\n",
    "    # y: 行列に変換するクラスベクトル, nb_classes: 総クラス数\n",
    "    # ↓trainのラベルを文字からインデックスを使用して変換\n",
    "    train_labels = utils.np_utils.to_categorical([label2index[label] for label in train_labels_df['label']], num_classes=class_count)\n",
    "    #　testのインデックスをまず作る\n",
    "    test_label_indices = [label2index[label] for label in test_labels_df['label']]\n",
    "    # ↓testのラベルを文字からインデックスを使用して変換\n",
    "    test_labels = utils.np_utils.to_categorical(test_label_indices, num_classes=class_count)\n",
    "\n",
    "    ##### 特徴量側の処理 #####\n",
    "    \n",
    "    train_features = []\n",
    "    test_features = []\n",
    "    for feature in train_features_df['feature']:\n",
    "        # 上で作った関数 _get_indice  を使ってID化\n",
    "        train_features.append(_get_indice(feature, maxlen))\n",
    "    # shape(len(train_features), maxlen)のゼロの行列作成\n",
    "    train_segments = np.zeros((len(train_features), maxlen), dtype = np.float32)\n",
    "\n",
    "    for feature in test_features_df['feature']:\n",
    "        # 上で作った関数 _get_indice  を使ってID化\n",
    "        test_features.append(_get_indice(feature, maxlen))\n",
    "    # shape(len(test_features), maxlen)のゼロの行列作成\n",
    "    test_segments = np.zeros((len(test_features), maxlen), dtype = np.float32)\n",
    "\n",
    "    print(f'Trainデータ数: {len(train_features_df)}, Testデータ数: {len(test_features_df)}, ラベル数: {class_count}')\n",
    "\n",
    "    return {\n",
    "        'class_count': class_count,\n",
    "        'label2index': label2index,\n",
    "        'index2label': index2label,\n",
    "        'train_labels': train_labels,\n",
    "        'test_labels': test_labels,\n",
    "        'test_label_indices': test_label_indices,\n",
    "        'train_features': np.array(train_features),\n",
    "        'train_segments': np.array(train_segments),\n",
    "        'test_features': np.array(test_features),\n",
    "        'test_segments': np.array(test_segments),\n",
    "        'input_len': maxlen\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,   9, 828,   5,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_indice(\"制作\", maxlen=SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainデータ数: 61, Testデータ数: 10, ラベル数: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'class_count': 2,\n",
       " 'label2index': {'positive': 0, 'negative': 1},\n",
       " 'index2label': {0: 'positive', 1: 'negative'},\n",
       " 'train_labels': array([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]], dtype=float32),\n",
       " 'test_labels': array([[0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.]], dtype=float32),\n",
       " 'test_label_indices': [1, 1, 1, 1, 0, 1, 1, 1, 1, 0],\n",
       " 'train_features': array([[    4,     9, 13657, ...,     0,     0,     0],\n",
       "        [    4,  5498,  6601, ...,     0,     0,     0],\n",
       "        [    4,     9, 28877, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    4,  5498,  1827, ...,     0,     0,     0],\n",
       "        [    4,     9, 16248, ...,     0,     0,     0],\n",
       "        [    4,     9, 13693, ...,     0,     0,     0]], dtype=int32),\n",
       " 'train_segments': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " 'test_features': array([[    4,     9,  8049, ...,     0,     0,     0],\n",
       "        [    4,     9,  2134, ...,     0,     0,     0],\n",
       "        [    4,     9,  4302, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    4,     9, 28877, ...,     0,     0,     0],\n",
       "        [    4,     9,  2067, ...,     0,     0,     0],\n",
       "        [    4,     9, 28877, ...,     0,     0,     0]], dtype=int32),\n",
       " 'test_segments': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " 'input_len': 206}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_load_labeldata('./datasets/finetuning/train', './datasets/finetuning/test', maxlen=SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル作成関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# おそらく、この関数を作った理由は複数分類モデルを自由に作れるようにしたかったからだ。\n",
    "# 単にネガポジにするなら関数にしないで直接書けばいい。\n",
    "\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional, Flatten, GlobalMaxPooling1D\n",
    "from keras_bert.layers import MaskedGlobalMaxPool1D\n",
    "from keras import Input, Model\n",
    "\n",
    "# nadam を選べば使わなくてもいい\n",
    "# https://github.com/CyberZHG/keras-bert\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "\n",
    "def _create_model(input_shape, class_count):\n",
    "    # AdamWarmupをオプティマイザーとして使用するために必要な情報を得る関数\n",
    "    # nadam を選べば使わなくてもいい\n",
    "    decay_steps, warmup_steps = calc_train_steps(\n",
    "        input_shape[0],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCH,\n",
    "    )\n",
    "\n",
    "    # 学習済みモデル 「bert」 の最終出力層のoutputを取り出す\n",
    "    bert_last = bert.get_layer(name='NSP-Dense').output\n",
    "    x1 = bert_last\n",
    "    # 最終出力層のoutputを新規作成した全結合層に入れる\n",
    "    output_tensor = Dense(class_count, activation='softmax')(x1)\n",
    "    \n",
    "    # Trainableの場合は、Input Masked Layerが3番目の入力なりますが、\n",
    "    # FineTuning時には必要無いので1, 2番目の入力だけ使用します。\n",
    "    # Trainableでなければkeras-bertのModel.inputそのままで問題ありません。\n",
    "    model = Model([bert.input[0], bert.input[1]], output_tensor)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=LR),\n",
    "                  #optimizer='nadam',\n",
    "                  metrics=['mae', 'mse', 'acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データのロードとモデルの準備\n",
    "事前準備で作成した学習用データと学習後のモデル名および出力先を指定してください。\n",
    "\n",
    "- trains_dir,tests_dir：学習用データのパス\n",
    "- model_filename：学習後のモデル名、出力先のパス\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras import Input, Model, utils\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainデータ数: 61, Testデータ数: 10, ラベル数: 2\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        [(None, 206)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      [(None, 206)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 206, 768), ( 24576000    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 206, 768)     1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 206, 768)     0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 206, 768)     158208      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 206, 768)     0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 206, 768)     1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 206, 768)     0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 206, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 206, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 206, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 206, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 206, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 206, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 206, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 206, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 206, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 206, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 206, 768)     0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 206, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 206, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 206, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 206, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 206, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 206, 768)     0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 206, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 206, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 206, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 206, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 206, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 206, 768)     0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 206, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 206, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 206, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 206, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 206, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 206, 768)     0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 206, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 206, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "NSP-Dense (Dense)               (None, 768)          590592      Extract[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            1538        NSP-Dense[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 110,383,874\n",
      "Trainable params: 110,383,874\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# データロードとモデルの準備\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "trains_dir = './datasets/finetuning/train'\n",
    "tests_dir = './datasets/finetuning/test'\n",
    "\n",
    "#上で作った関数\n",
    "data = _load_labeldata(trains_dir, tests_dir, SEQ_LEN)\n",
    "\n",
    "# モデルの読み込み\n",
    "model_filename = './downloads/models/knbc_finetuning.model'\n",
    "\n",
    "# 上で作った関数（関数を使わずに直接書くこともできる）\n",
    "# data['train_features'].shape　は　文の数×最大単語数　＝　特徴量Xのインプットshape\n",
    "# data['class_count']　は　クラスの数\n",
    "model = _create_model(data['train_features'].shape, data['class_count'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の実行\n",
    "いよいよ学習の実行です。以下のプログラムを実行した際に画像のような出力が出ると思います。（tensorflowのバージョンでWarningが出ますが問題ありません）あとはお茶でも飲みながら学習経過を観察してみましょう！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ファインチューニング？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4/4 [==============================] - 228s 55s/step - loss: 0.0011 - mae: 0.0011 - mse: 3.4932e-06 - acc: 1.0000 - val_loss: 1.6784 - val_mae: 0.4432 - val_mse: 0.3515 - val_acc: 0.6000\n",
      "INFO:tensorflow:Assets written to: ./downloads/models/knbc_finetuning.model/assets\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 269s 64s/step - loss: 6.2847e-04 - mae: 6.2802e-04 - mse: 8.9575e-07 - acc: 1.0000 - val_loss: 1.7187 - val_mae: 0.4380 - val_mse: 0.3492 - val_acc: 0.6000\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 255s 63s/step - loss: 3.8089e-04 - mae: 3.8076e-04 - mse: 2.6992e-07 - acc: 1.0000 - val_loss: 1.7421 - val_mae: 0.4336 - val_mse: 0.3467 - val_acc: 0.6000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([data['train_features'], data['train_segments']],\n",
    "          data['train_labels'],\n",
    "          epochs = 3,\n",
    "          #epochs = EPOCH,\n",
    "          batch_size = BATCH_SIZE,\n",
    "          validation_data=([data['test_features'], data['test_segments']], data['test_labels']),\n",
    "          shuffle=False,\n",
    "          verbose = 1,\n",
    "          callbacks = [\n",
    "              ModelCheckpoint(monitor='val_acc', mode='max', filepath=model_filename, save_best_only=True)\n",
    "          ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価および予測\n",
    "### 評価の出力（不要）\n",
    "#### 学習経過\n",
    "学習回数毎の精度を算出し、学習経過を見ることが出来ます。  \n",
    "※ 学習を行った後に同じnotebookで実行してください。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>3.493196e-06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.678375</td>\n",
       "      <td>0.443181</td>\n",
       "      <td>0.351455</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>8.957526e-07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.718724</td>\n",
       "      <td>0.438022</td>\n",
       "      <td>0.349179</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>2.699173e-07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.742150</td>\n",
       "      <td>0.433553</td>\n",
       "      <td>0.346655</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss       mae           mse  acc  val_loss   val_mae   val_mse  \\\n",
       "0  0.001078  0.001076  3.493196e-06  1.0  1.678375  0.443181  0.351455   \n",
       "1  0.000628  0.000628  8.957526e-07  1.0  1.718724  0.438022  0.349179   \n",
       "2  0.000381  0.000381  2.699173e-07  1.0  1.742150  0.433553  0.346655   \n",
       "\n",
       "   val_acc  \n",
       "0      0.6  \n",
       "1      0.6  \n",
       "2      0.6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(history.history)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル評価（不要）\n",
    "テストデータを用いて学習結果を算出出来ます。ここで用いられている指標については以下のURLを参照してください。\n",
    "http://tkdmah.hatenablog.com/entry/2014/02/22/193008\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>スポーツ</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>携帯電話</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision  recall  f1-score  support\n",
       "スポーツ           0.250000  0.5000  0.333333      2.0\n",
       "携帯電話           0.833333  0.6250  0.714286      8.0\n",
       "accuracy       0.600000  0.6000  0.600000      0.6\n",
       "macro avg      0.541667  0.5625  0.523810     10.0\n",
       "weighted avg   0.716667  0.6000  0.638095     10.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from keras.models import load_model\n",
    "from keras_bert import get_custom_objects\n",
    "\n",
    "model = load_model(model_filename, custom_objects=get_custom_objects())\n",
    "\n",
    "predicted_test_labels = model.predict([data['test_features'], data['test_segments']]).argmax(axis=1)\n",
    "numeric_test_labels = np.array(data['test_labels']).argmax(axis=1)\n",
    "\n",
    "report = classification_report(\n",
    "        numeric_test_labels, predicted_test_labels, target_names=['スポーツ', '携帯電話'], output_dict=True)\n",
    "# , '京都', 'スポーツ'\n",
    "display(pd.DataFrame(report).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論時にAttention Weightを出力するようにモデルをロードする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "\n",
    "# custom_objects \n",
    "# custom_objects引数を使ってロード機構にそのカスタムレイヤーなどを渡すことができます\n",
    "#model = load_model(model_filename, custom_objects=SeqSelfAttention.get_custom_objects())\n",
    "\n",
    "#ModelクラスAPI   model = Model(inputs=a, outputs=b)  model = Model(inputs=[a1, a2], outputs=[b1, b2, b3])\n",
    "# model.get_layer('attention').output 2つ目のアウトプットを指定\n",
    "#model = Model(inputs=model.input, outputs=[model.output, model.get_layer('attention').output])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測\n",
    "いよいよ学習したモデルを用いた予測です。流れとしては入力された文章の特徴量を抽出し、モデルに入力するだけの簡単なお仕事です！ただ注意点もあります。それについては一度下記のソースを実行した後に解説します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'イギリスから戻って来た\\n布袋寅泰氏はどうするのかね\\n緊急事態宣言に\\n劇場とかは含まれないみたいだけど\\n全国からファンを\\n武道館に来させるのかな\\n中止もしくは\\n武道館に来なくても払い戻しします\\n位の措置は取った方がいいと思う\\n\\n'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests_features_df = pd.read_csv('./datasets/pred_labeling/features_001.csv')\n",
    "tests_features_df.loc[0]['feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd995e2dd30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "完了\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from keras import utils\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_bert import get_custom_objects\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "#sys.pathに追加（必要なのか調査が必要）\n",
    "sys.path.append('modules')\n",
    "\n",
    "# SentencePieceProccerモデルの読込\n",
    "spp = spm.SentencePieceProcessor()\n",
    "spp.Load('./downloads/bert-wiki-ja/wiki-ja.model')\n",
    "\n",
    "# BERTの学習したモデルの読込（ダウンロードした？勝手に保存される？）\n",
    "model_filename = './downloads/models/knbc_finetuning.model'\n",
    "model = load_model(model_filename, custom_objects=get_custom_objects())\n",
    "#model = load_model(model_filename, custom_objects=SeqSelfAttention.get_custom_objects())\n",
    "model = Model(inputs=model.input, outputs=[model.output, model.get_layer('Encoder-12-MultiHeadSelfAttention').output])\n",
    "# ↑ここでmodel = Model(inputs=a, outputs=b) としてAttentionも出すようにする。\n",
    "\n",
    "\n",
    "# 上のと同じのを入れると思われるため、消していいかも(ファイルを分けるなら必要)\n",
    "#SEQ_LEN = 103#206\n",
    "maxlen = SEQ_LEN\n",
    "#\n",
    "# 上にあったのと同じ？ → predictように変更\n",
    "def _get_indice_pred(feature, maxlen):\n",
    "    indices = np.zeros((maxlen), dtype=np.int32)\n",
    "\n",
    "    tokens = []\n",
    "    tokens.append('[CLS]')\n",
    "    tokens.extend(spp.encode_as_pieces(feature))\n",
    "    tokens.append('[SEP]')\n",
    "\n",
    "    for t, token in enumerate(tokens):\n",
    "        if t >= maxlen:\n",
    "            break\n",
    "        try:\n",
    "            indices[t] = spp.piece_to_id(token)\n",
    "        except:\n",
    "            logging.warn('unknown')\n",
    "            indices[t] = spp.piece_to_id('<unk>')\n",
    "            \n",
    "    return indices, tokens\n",
    "\n",
    "tests_features_df = pd.read_csv('./datasets/pred_labeling/features_001.csv')\n",
    "feature = tests_features_df.loc[0]['feature']\n",
    "#feature = \"昨日は携帯電話を買いに行った。\"\n",
    "#feature = \"昨日はスポーツしに行った。\"\n",
    "\n",
    "test_features = []\n",
    "indices, tokens = _get_indice_pred(feature, maxlen)\n",
    "test_features.append(indices)\n",
    "\n",
    "#勝手に追加\n",
    "test_features = np.array(test_features)\n",
    "\n",
    "test_segments = np.zeros(\n",
    "    (len(test_features), maxlen), dtype=np.float32)\n",
    "\n",
    "\n",
    "# model = Modelを使えば推定　predict[0][0]　２次元のリストで返せる。\n",
    "predicted_test_labels = model.predict([test_features, test_segments])#.argmax(axis=1)\n",
    "#predict = model.predict(test_features)\n",
    "\n",
    "\n",
    "print(\"完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁イギリス', 'から', '戻って', '来た', '▁', '布', '袋', '寅', '泰', '氏は', 'どう', 'する', 'の', 'かね', '▁', '緊急', '事態', '宣言', 'に', '▁', '劇場', 'とか', 'は', '含まれ', 'ない', 'みたい', 'だけ', 'ど', '▁', '全国', 'から', 'ファン', 'を', '▁', '武道', '館', 'に', '来', 'させる', 'の', 'かな', '▁', '中止', 'もしくは', '▁', '武道', '館', 'に', '来', 'なくても', '払い', '戻し', 'します', '▁', '位の', '措置', 'は', '取った', '方が', 'いい', 'と思う', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  2\n",
      "predict:  [[0.839672   0.16032797]]\n",
      "predict:  [0]\n",
      "attention: (1, 206, 768)\n",
      "attention:\n",
      " [[[ 9.7782558e-01 -3.6609226e-01  8.0258004e-02 ... -3.8428387e-01\n",
      "   -4.7179323e-01 -3.1073414e-02]\n",
      "  [ 7.9611063e-01  7.4672580e-01 -2.9612947e-01 ...  1.7870662e-01\n",
      "   -1.9683883e-01 -2.8244317e-01]\n",
      "  [ 2.6390794e-01  3.0540866e-01 -2.4646787e-01 ... -4.7400936e-01\n",
      "    5.6182917e-02  8.7877363e-04]\n",
      "  ...\n",
      "  [ 9.2248648e-01  2.1266861e-01  2.1963589e-01 ...  9.6179411e-02\n",
      "   -4.9075924e-02 -3.0132383e-04]\n",
      "  [ 7.6565254e-01  2.8815186e-01 -5.0824367e-02 ...  1.5906708e-01\n",
      "    3.2077126e-02 -5.8061261e-02]\n",
      "  [ 7.2041649e-01  2.9803842e-01 -1.3347173e-01 ...  6.8138547e-02\n",
      "   -1.4585811e-01 -7.6586224e-02]]]\n",
      "attention:\n",
      " [[ 42   7  49   3  40  47   6  44  52   1  42  13  48  41  10  33  24  36\n",
      "    0  42   3  24  41  46  25   7  28  60  40  40  43  40  59  11   4  34\n",
      "    4  28   3  31  47  44  18  33  25  49  34  91   3  34  31  33  48  14\n",
      "   56  19  27 182  15  41  40  19  61  23  44 189   4  19  18  19  31  39\n",
      "   41  26  38  25  32 182  40  10   2  40  31  28  31  41  57  98  40  13\n",
      "   41  31   1  31  15   2  26   7   3  19   7  34  31   3  46  10  44   3\n",
      "    4   4  18  40  72  18  11  19  15  60  18  46  33  27  44  31   1  50\n",
      "   27   4  61  40  46  34   4  40  36  61  50  28  18   2  35  57  31  46\n",
      "   28  34  11  28  18  59  19  42  24  19  44  49   2  40  25  31  27   7\n",
      "   19  24   6  28  49   4  47  44  13  35  41  91  40  31  51  13  49  17\n",
      "   41  38  41  13  28  48  57  25  47  44   2  28  46  44  28  46  33  46\n",
      "   38  45   3  34  27  44  41  31 189  44  13   4  50  28  19   3  57  19\n",
      "   49   4  47  42  57  27   9  46  49  27  40  56  50  40  50   8  44  22\n",
      "    4  33  15  18  18  97  19  20  46  61  27   4  28   7  13  57  31  27\n",
      "   26  31  52  38  48  49  36   3  33  47  44  14  25   4  11  46   3   3\n",
      "   28   4  31  57  21  97  34  18  50  56   7  38  33  91  28  27   3  28\n",
      "   49  44  40   4  38  57  45  18  27  11  42   6   4  19  31   4  13 189\n",
      "   40  50  13  33  41  25  37  52  17  41  18  61  50  49  50  24  40  26\n",
      "    4  61  47  34  44  49  27  43  24   9  31 182  48  40  48  26  28 189\n",
      "   28  47  47  60 182  24  15  38  23  49  20  27  41  38  61  10  61  60\n",
      "   25  33  31   4  23  26  18  33  33  28  31  61   3   1   7  33  23  22\n",
      "  190   4  11  51  19  40  38  31  57  46  57  44   7  50  57  23  52  44\n",
      "   18  44  40  19  48  28  27  20  40  31  42  37  38  52  25   7  57  28\n",
      "   19  49   2  61  19   3   3  17  41  49  40  41  49  57  34  39  97  15\n",
      "   33  46  50  28  19  57  28  33  44  33  51   3  26  33  58  37  19  33\n",
      "   41  24  60  49  60  41  27  18  28  40  15   4  12  44  60  19  11  50\n",
      "   36  37  46  27   6  27  46  42  61  15  46  41  19  43  44  44  60  34\n",
      "   20  58  12  57   6  41  33  49  34  41  34   2  11  18  23  18  20  11\n",
      "   28  25   4  41   1  20  60  44  33   2  25  44  33  41   4  42  40   9\n",
      "   40  47   4  50  52  12  10  19  41  40  25  56   4  41   6  52  34  60\n",
      "    9  40  60   6  43  45  58   4  33   6   6  50  48  28  10  42  44  34\n",
      "   44  40   7  57   7  33  25   3  38   3  40  61   1  14  31  41  50  44\n",
      "   37  31  42  41  25  52  33  47  44  12  42  10  27  24  31  18  51  12\n",
      "   40  40  60   4 200  28  40   0  23  41   4  17   1  21  33  47  40  19\n",
      "   15  48  33  57  18  42  49  14  12  31  48  56  33  33  44  33  18  18\n",
      "   31  19  42  13  61  60  14  31  25  37  61  48  47  44   9  47  13   3\n",
      "    4  28  10  50  28  33  44  40  27  49  28  49  18  49  27 141  19  28\n",
      "   41   6  25  44  13  46   1  33  27  18   2  40   3  27  27  56  37   7\n",
      "   10  25  57   0  40  19  24   3  10   4  31  13  17  42  61  23  37   3\n",
      "   34  25  31  20  57   3   4  24  14  50  28  11   2   7  13   2   4   6\n",
      "   18  40  19  33  57  40  19  61  28  31  50  33  42  56  25  24 182  49\n",
      "   33  19  25  28  38  12  37   0  41 189  18  38  40  28  41   6  50  11\n",
      "    2  54   7   6  36  61  33  31  49  11  13  49]]\n",
      "attention:\n",
      " (1, 768)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"shape: \", len(predicted_test_labels))\n",
    "#print(\"predicted_test_labels: \", predicted_test_labels)\n",
    "print(\"predict: \", predicted_test_labels[0])\n",
    "print(\"predict: \", predicted_test_labels[0].argmax(axis=1))\n",
    "print(\"attention:\", predicted_test_labels[1].shape)\n",
    "print(\"attention:\\n\", predicted_test_labels[1])\n",
    "print(\"attention:\\n\", predicted_test_labels[1].argmax(axis=1))#axis=1\n",
    "print(\"attention:\\n\", predicted_test_labels[1].argmax(axis=1).shape)#axis=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "Encoder-12-MultiHeadSelfAttenti (None, 206, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
    "                                                                 Encoder-12-MultiHeadSelfAttention\n",
    "__________________________________________________________________________________________________\n",
    "Encoder-12-MultiHeadSelfAttenti (None, 206, 768)     1536        Encoder-12-MultiHeadSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row0_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row1_col3 {\n",
       "            background-color:  #dfecf7;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row2_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row3_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row4_col3 {\n",
       "            background-color:  #bcd7eb;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row5_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row6_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row7_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row8_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row9_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row10_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row11_col3 {\n",
       "            background-color:  #a4cce3;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row12_col3 {\n",
       "            background-color:  #f6faff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row13_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row14_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row15_col3 {\n",
       "            background-color:  #eff6fc;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row16_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row17_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row18_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row19_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row20_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row21_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row22_col3 {\n",
       "            background-color:  #7cb7da;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row23_col3 {\n",
       "            background-color:  #dfecf7;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row24_col3 {\n",
       "            background-color:  #dfebf7;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row25_col3 {\n",
       "            background-color:  #4b98ca;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row26_col3 {\n",
       "            background-color:  #9cc9e1;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row27_col3 {\n",
       "            background-color:  #aed1e7;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row28_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row29_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row30_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row31_col3 {\n",
       "            background-color:  #a4cce3;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row32_col3 {\n",
       "            background-color:  #5ca4d0;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row33_col3 {\n",
       "            background-color:  #4896c8;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row34_col3 {\n",
       "            background-color:  #68acd5;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row35_col3 {\n",
       "            background-color:  #a4cce3;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row36_col3 {\n",
       "            background-color:  #82bbdb;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row37_col3 {\n",
       "            background-color:  #3080bd;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row38_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row39_col3 {\n",
       "            background-color:  #08306b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row40_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row41_col3 {\n",
       "            background-color:  #d9e7f5;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row42_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row43_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row44_col3 {\n",
       "            background-color:  #f4f9fe;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row45_col3 {\n",
       "            background-color:  #3484bf;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row46_col3 {\n",
       "            background-color:  #0a539e;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row47_col3 {\n",
       "            background-color:  #79b5d9;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row48_col3 {\n",
       "            background-color:  #eef5fc;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row49_col3 {\n",
       "            background-color:  #9dcae1;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row50_col3 {\n",
       "            background-color:  #a6cee4;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row51_col3 {\n",
       "            background-color:  #0c56a0;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row52_col3 {\n",
       "            background-color:  #b9d6ea;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row53_col3 {\n",
       "            background-color:  #dce9f6;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row54_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row55_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row56_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row57_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row58_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row59_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row60_col3 {\n",
       "            background-color:  #7fb9da;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row61_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_15b05d1c_5e39_11eb_88cc_367dda54a607row62_col3 {\n",
       "            background-color:  #c4daee;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >token</th>        <th class=\"col_heading level0 col1\" >weight</th>        <th class=\"col_heading level0 col2\" >rank</th>        <th class=\"col_heading level0 col3\" >normalized</th>        <th class=\"col_heading level0 col4\" >attention</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row0_col0\" class=\"data row0 col0\" >[CLS]</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row0_col1\" class=\"data row0 col1\" >0.804144</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row0_col2\" class=\"data row0 col2\" >60.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row0_col4\" class=\"data row0 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row1_col0\" class=\"data row1 col0\" >▁イギリス</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row1_col1\" class=\"data row1 col1\" >1.051881</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row1_col2\" class=\"data row1 col2\" >28.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row1_col3\" class=\"data row1 col3\" >0.033012</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row1_col4\" class=\"data row1 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row2_col0\" class=\"data row2 col0\" >から</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row2_col1\" class=\"data row2 col1\" >0.808924</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row2_col2\" class=\"data row2 col2\" >58.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row2_col4\" class=\"data row2 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row3_col0\" class=\"data row3 col0\" >戻って</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row3_col1\" class=\"data row3 col1\" >0.842807</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row3_col2\" class=\"data row3 col2\" >55.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row3_col4\" class=\"data row3 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row4_col0\" class=\"data row4 col0\" >来た</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row4_col1\" class=\"data row4 col1\" >1.098013</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row4_col2\" class=\"data row4 col2\" >22.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row4_col3\" class=\"data row4 col3\" >0.079144</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row4_col4\" class=\"data row4 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row5_col0\" class=\"data row5 col0\" >▁</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row5_col1\" class=\"data row5 col1\" >0.984845</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row5_col2\" class=\"data row5 col2\" >43.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row5_col3\" class=\"data row5 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row5_col4\" class=\"data row5 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row6_col0\" class=\"data row6 col0\" >布</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row6_col1\" class=\"data row6 col1\" >0.859490</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row6_col2\" class=\"data row6 col2\" >54.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row6_col3\" class=\"data row6 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row6_col4\" class=\"data row6 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row7_col0\" class=\"data row7 col0\" >袋</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row7_col1\" class=\"data row7 col1\" >0.868167</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row7_col2\" class=\"data row7 col2\" >52.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row7_col3\" class=\"data row7 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row7_col4\" class=\"data row7 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row8_col0\" class=\"data row8 col0\" >寅</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row8_col1\" class=\"data row8 col1\" >0.712544</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row8_col2\" class=\"data row8 col2\" >63.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row8_col3\" class=\"data row8 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row8_col4\" class=\"data row8 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row9_col0\" class=\"data row9 col0\" >泰</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row9_col1\" class=\"data row9 col1\" >0.819503</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row9_col2\" class=\"data row9 col2\" >57.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row9_col3\" class=\"data row9 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row9_col4\" class=\"data row9 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row10_col0\" class=\"data row10 col0\" >氏は</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row10_col1\" class=\"data row10 col1\" >0.925333</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row10_col2\" class=\"data row10 col2\" >49.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row10_col3\" class=\"data row10 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row10_col4\" class=\"data row10 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row11_col0\" class=\"data row11 col0\" >どう</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row11_col1\" class=\"data row11 col1\" >1.118221</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row11_col2\" class=\"data row11 col2\" >17.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row11_col3\" class=\"data row11 col3\" >0.099353</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row11_col4\" class=\"data row11 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row12_col0\" class=\"data row12 col0\" >する</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row12_col1\" class=\"data row12 col1\" >1.020203</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row12_col2\" class=\"data row12 col2\" >32.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row12_col3\" class=\"data row12 col3\" >0.001334</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row12_col4\" class=\"data row12 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row13_col0\" class=\"data row13 col0\" >の</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row13_col1\" class=\"data row13 col1\" >0.864098</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row13_col2\" class=\"data row13 col2\" >53.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row13_col3\" class=\"data row13 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row13_col4\" class=\"data row13 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row14_col0\" class=\"data row14 col0\" >かね</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row14_col1\" class=\"data row14 col1\" >0.934176</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row14_col2\" class=\"data row14 col2\" >47.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row14_col3\" class=\"data row14 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row14_col4\" class=\"data row14 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row15_col0\" class=\"data row15 col0\" >▁</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row15_col1\" class=\"data row15 col1\" >1.030786</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row15_col2\" class=\"data row15 col2\" >30.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row15_col3\" class=\"data row15 col3\" >0.011918</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row15_col4\" class=\"data row15 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row16_col0\" class=\"data row16 col0\" >緊急</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row16_col1\" class=\"data row16 col1\" >0.992057</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row16_col2\" class=\"data row16 col2\" >38.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row16_col3\" class=\"data row16 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row16_col4\" class=\"data row16 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row17_col0\" class=\"data row17 col0\" >事態</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row17_col1\" class=\"data row17 col1\" >0.991257</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row17_col2\" class=\"data row17 col2\" >39.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row17_col3\" class=\"data row17 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row17_col4\" class=\"data row17 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row18_col0\" class=\"data row18 col0\" >宣言</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row18_col1\" class=\"data row18 col1\" >1.011329</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row18_col2\" class=\"data row18 col2\" >36.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row18_col3\" class=\"data row18 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row18_col4\" class=\"data row18 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row19_col0\" class=\"data row19 col0\" >に</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row19_col1\" class=\"data row19 col1\" >0.921919</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row19_col2\" class=\"data row19 col2\" >50.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row19_col3\" class=\"data row19 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row19_col4\" class=\"data row19 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row20_col0\" class=\"data row20 col0\" >▁</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row20_col1\" class=\"data row20 col1\" >0.930179</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row20_col2\" class=\"data row20 col2\" >48.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row20_col3\" class=\"data row20 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row20_col4\" class=\"data row20 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row21_col0\" class=\"data row21 col0\" >劇場</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row21_col1\" class=\"data row21 col1\" >0.986895</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row21_col2\" class=\"data row21 col2\" >42.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row21_col3\" class=\"data row21 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row21_col4\" class=\"data row21 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row22_col0\" class=\"data row22 col0\" >とか</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row22_col1\" class=\"data row22 col1\" >1.146317</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row22_col2\" class=\"data row22 col2\" >11.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row22_col3\" class=\"data row22 col3\" >0.127448</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row22_col4\" class=\"data row22 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row23_col0\" class=\"data row23 col0\" >は</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row23_col1\" class=\"data row23 col1\" >1.052093</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row23_col2\" class=\"data row23 col2\" >27.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row23_col3\" class=\"data row23 col3\" >0.033224</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row23_col4\" class=\"data row23 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row24_col0\" class=\"data row24 col0\" >含まれ</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row24_col1\" class=\"data row24 col1\" >1.053536</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row24_col2\" class=\"data row24 col2\" >26.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row24_col3\" class=\"data row24 col3\" >0.034667</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row24_col4\" class=\"data row24 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row25_col0\" class=\"data row25 col0\" >ない</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row25_col1\" class=\"data row25 col1\" >1.185243</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row25_col2\" class=\"data row25 col2\" >7.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row25_col3\" class=\"data row25 col3\" >0.166375</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row25_col4\" class=\"data row25 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row26_col0\" class=\"data row26 col0\" >みたい</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row26_col1\" class=\"data row26 col1\" >1.124833</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row26_col2\" class=\"data row26 col2\" >14.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row26_col3\" class=\"data row26 col3\" >0.105964</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row26_col4\" class=\"data row26 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row27_col0\" class=\"data row27 col0\" >だけ</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row27_col1\" class=\"data row27 col1\" >1.109571</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row27_col2\" class=\"data row27 col2\" >20.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row27_col3\" class=\"data row27 col3\" >0.090702</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row27_col4\" class=\"data row27 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row28_col0\" class=\"data row28 col0\" >ど</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row28_col1\" class=\"data row28 col1\" >1.019817</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row28_col2\" class=\"data row28 col2\" >33.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row28_col3\" class=\"data row28 col3\" >0.000948</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row28_col4\" class=\"data row28 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row29_col0\" class=\"data row29 col0\" >▁</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row29_col1\" class=\"data row29 col1\" >0.936667</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row29_col2\" class=\"data row29 col2\" >46.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row29_col3\" class=\"data row29 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row29_col4\" class=\"data row29 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row30_col0\" class=\"data row30 col0\" >全国</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row30_col1\" class=\"data row30 col1\" >1.009395</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row30_col2\" class=\"data row30 col2\" >37.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row30_col3\" class=\"data row30 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row30_col4\" class=\"data row30 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row31_col0\" class=\"data row31 col0\" >から</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row31_col1\" class=\"data row31 col1\" >1.118736</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row31_col2\" class=\"data row31 col2\" >16.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row31_col3\" class=\"data row31 col3\" >0.099868</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row31_col4\" class=\"data row31 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row32_col0\" class=\"data row32 col0\" >ファン</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row32_col1\" class=\"data row32 col1\" >1.171224</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row32_col2\" class=\"data row32 col2\" >8.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row32_col3\" class=\"data row32 col3\" >0.152355</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row32_col4\" class=\"data row32 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row33_col0\" class=\"data row33 col0\" >を</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row33_col1\" class=\"data row33 col1\" >1.188616</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row33_col2\" class=\"data row33 col2\" >6.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row33_col3\" class=\"data row33 col3\" >0.169747</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row33_col4\" class=\"data row33 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row34_col0\" class=\"data row34 col0\" >▁</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row34_col1\" class=\"data row34 col1\" >1.161291</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row34_col2\" class=\"data row34 col2\" >9.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row34_col3\" class=\"data row34 col3\" >0.142422</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row34_col4\" class=\"data row34 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row35_col0\" class=\"data row35 col0\" >武道</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row35_col1\" class=\"data row35 col1\" >1.118182</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row35_col2\" class=\"data row35 col2\" >18.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row35_col3\" class=\"data row35 col3\" >0.099313</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row35_col4\" class=\"data row35 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row36_col0\" class=\"data row36 col0\" >館</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row36_col1\" class=\"data row36 col1\" >1.142991</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row36_col2\" class=\"data row36 col2\" >13.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row36_col3\" class=\"data row36 col3\" >0.124123</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row36_col4\" class=\"data row36 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row37_col0\" class=\"data row37 col0\" >に</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row37_col1\" class=\"data row37 col1\" >1.212232</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row37_col2\" class=\"data row37 col2\" >4.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row37_col3\" class=\"data row37 col3\" >0.193364</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row37_col4\" class=\"data row37 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row38_col0\" class=\"data row38 col0\" >来</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row38_col1\" class=\"data row38 col1\" >0.959642</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row38_col2\" class=\"data row38 col2\" >45.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row38_col3\" class=\"data row38 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row38_col4\" class=\"data row38 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row39_col0\" class=\"data row39 col0\" >させる</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row39_col1\" class=\"data row39 col1\" >1.297711</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row39_col2\" class=\"data row39 col2\" >1.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row39_col3\" class=\"data row39 col3\" >0.278843</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row39_col4\" class=\"data row39 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row40_col0\" class=\"data row40 col0\" >の</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row40_col1\" class=\"data row40 col1\" >1.013566</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row40_col2\" class=\"data row40 col2\" >35.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row40_col3\" class=\"data row40 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row40_col4\" class=\"data row40 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row41_col0\" class=\"data row41 col0\" >かな</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row41_col1\" class=\"data row41 col1\" >1.062327</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row41_col2\" class=\"data row41 col2\" >24.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row41_col3\" class=\"data row41 col3\" >0.043458</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row41_col4\" class=\"data row41 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row42_col0\" class=\"data row42 col0\" >▁</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row42_col1\" class=\"data row42 col1\" >0.807911</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row42_col2\" class=\"data row42 col2\" >59.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row42_col3\" class=\"data row42 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row42_col4\" class=\"data row42 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row43_col0\" class=\"data row43 col0\" >中止</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row43_col1\" class=\"data row43 col1\" >0.836748</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row43_col2\" class=\"data row43 col2\" >56.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row43_col3\" class=\"data row43 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row43_col4\" class=\"data row43 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row44_col0\" class=\"data row44 col0\" >もしくは</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row44_col1\" class=\"data row44 col1\" >1.024247</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row44_col2\" class=\"data row44 col2\" >31.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row44_col3\" class=\"data row44 col3\" >0.005378</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row44_col4\" class=\"data row44 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row45_col0\" class=\"data row45 col0\" >▁</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row45_col1\" class=\"data row45 col1\" >1.208041</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row45_col2\" class=\"data row45 col2\" >5.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row45_col3\" class=\"data row45 col3\" >0.189172</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row45_col4\" class=\"data row45 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row46_col0\" class=\"data row46 col0\" >武道</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row46_col1\" class=\"data row46 col1\" >1.259884</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row46_col2\" class=\"data row46 col2\" >2.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row46_col3\" class=\"data row46 col3\" >0.241015</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row46_col4\" class=\"data row46 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row47_col0\" class=\"data row47 col0\" >館</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row47_col1\" class=\"data row47 col1\" >1.149002</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row47_col2\" class=\"data row47 col2\" >10.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row47_col3\" class=\"data row47 col3\" >0.130133</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row47_col4\" class=\"data row47 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row48_col0\" class=\"data row48 col0\" >に</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row48_col1\" class=\"data row48 col1\" >1.032315</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row48_col2\" class=\"data row48 col2\" >29.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row48_col3\" class=\"data row48 col3\" >0.013446</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row48_col4\" class=\"data row48 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row49_col0\" class=\"data row49 col0\" >来</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row49_col1\" class=\"data row49 col1\" >1.123704</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row49_col2\" class=\"data row49 col2\" >15.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row49_col3\" class=\"data row49 col3\" >0.104836</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row49_col4\" class=\"data row49 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row50\" class=\"row_heading level0 row50\" >50</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row50_col0\" class=\"data row50 col0\" >なくても</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row50_col1\" class=\"data row50 col1\" >1.116030</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row50_col2\" class=\"data row50 col2\" >19.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row50_col3\" class=\"data row50 col3\" >0.097161</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row50_col4\" class=\"data row50 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row51\" class=\"row_heading level0 row51\" >51</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row51_col0\" class=\"data row51 col0\" >払い</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row51_col1\" class=\"data row51 col1\" >1.256584</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row51_col2\" class=\"data row51 col2\" >3.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row51_col3\" class=\"data row51 col3\" >0.237715</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row51_col4\" class=\"data row51 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row52\" class=\"row_heading level0 row52\" >52</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row52_col0\" class=\"data row52 col0\" >戻し</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row52_col1\" class=\"data row52 col1\" >1.100299</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row52_col2\" class=\"data row52 col2\" >21.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row52_col3\" class=\"data row52 col3\" >0.081430</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row52_col4\" class=\"data row52 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row53\" class=\"row_heading level0 row53\" >53</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row53_col0\" class=\"data row53 col0\" >します</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row53_col1\" class=\"data row53 col1\" >1.057777</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row53_col2\" class=\"data row53 col2\" >25.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row53_col3\" class=\"data row53 col3\" >0.038908</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row53_col4\" class=\"data row53 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row54\" class=\"row_heading level0 row54\" >54</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row54_col0\" class=\"data row54 col0\" >▁</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row54_col1\" class=\"data row54 col1\" >0.878682</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row54_col2\" class=\"data row54 col2\" >51.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row54_col3\" class=\"data row54 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row54_col4\" class=\"data row54 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row55\" class=\"row_heading level0 row55\" >55</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row55_col0\" class=\"data row55 col0\" >位の</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row55_col1\" class=\"data row55 col1\" >0.713452</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row55_col2\" class=\"data row55 col2\" >62.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row55_col3\" class=\"data row55 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row55_col4\" class=\"data row55 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row56\" class=\"row_heading level0 row56\" >56</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row56_col0\" class=\"data row56 col0\" >措置</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row56_col1\" class=\"data row56 col1\" >0.774590</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row56_col2\" class=\"data row56 col2\" >61.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row56_col3\" class=\"data row56 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row56_col4\" class=\"data row56 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row57\" class=\"row_heading level0 row57\" >57</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row57_col0\" class=\"data row57 col0\" >は</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row57_col1\" class=\"data row57 col1\" >0.987501</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row57_col2\" class=\"data row57 col2\" >41.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row57_col3\" class=\"data row57 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row57_col4\" class=\"data row57 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row58\" class=\"row_heading level0 row58\" >58</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row58_col0\" class=\"data row58 col0\" >取った</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row58_col1\" class=\"data row58 col1\" >0.960420</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row58_col2\" class=\"data row58 col2\" >44.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row58_col3\" class=\"data row58 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row58_col4\" class=\"data row58 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row59\" class=\"row_heading level0 row59\" >59</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row59_col0\" class=\"data row59 col0\" >方が</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row59_col1\" class=\"data row59 col1\" >0.990892</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row59_col2\" class=\"data row59 col2\" >40.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row59_col3\" class=\"data row59 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row59_col4\" class=\"data row59 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row60\" class=\"row_heading level0 row60\" >60</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row60_col0\" class=\"data row60 col0\" >いい</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row60_col1\" class=\"data row60 col1\" >1.144317</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row60_col2\" class=\"data row60 col2\" >12.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row60_col3\" class=\"data row60 col3\" >0.125448</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row60_col4\" class=\"data row60 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row61\" class=\"row_heading level0 row61\" >61</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row61_col0\" class=\"data row61 col0\" >と思う</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row61_col1\" class=\"data row61 col1\" >1.015395</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row61_col2\" class=\"data row61 col2\" >34.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row61_col3\" class=\"data row61 col3\" >0.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row61_col4\" class=\"data row61 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607level0_row62\" class=\"row_heading level0 row62\" >62</th>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row62_col0\" class=\"data row62 col0\" >[SEP]</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row62_col1\" class=\"data row62 col1\" >1.090184</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row62_col2\" class=\"data row62 col2\" >23.000000</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row62_col3\" class=\"data row62 col3\" >0.071315</td>\n",
       "                        <td id=\"T_15b05d1c_5e39_11eb_88cc_367dda54a607row62_col4\" class=\"data row62 col4\" >True</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdb8a158fa0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 入力シーケンスはpad_sequenceにより、以下の様に0でpre paddingしています。\n",
    "# [0 0 0 0 x1(300) x2(300) x3(300)]\n",
    "# Attention Weightは入力シーケンスに対応して計算されるため、\n",
    "# 入力シーケンスのpadding分シフトします。\n",
    "weights = [w.max() for w in predicted_test_labels[1][0][-len(tokens):]]\n",
    "df = pd.DataFrame([tokens, weights], index=['token', 'weight']).T\n",
    "\n",
    "mean = np.asarray(weights).mean()#np.asarray　参照コピー\n",
    "\n",
    "df['rank'] = df['weight'].rank(ascending=False)#ランキング\n",
    "# wから平均を引いた値が0より大きいものだけ（偏差）\n",
    "df['normalized'] = df['weight'].apply(lambda w: max(w - mean, 0))#行全体や列全体に対して、同じ操作\n",
    "df['weight'] = df['weight'].astype('float32')\n",
    "df['attention'] = df['normalized'] > 0\n",
    "# df.style.background_gradient で色つけ\n",
    "df = df.style.background_gradient(cmap='Blues', subset=['normalized'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測 作り直し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name:  features_001.csv\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd93ecc73a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tokens:  ['[CLS]', '▁イギリス', 'から', '戻って', '来た', '▁', '布', '袋', '寅', '泰', '氏は', 'どう', 'する', 'の', 'かね', '▁', '緊急', '事態', '宣言', 'に', '▁', '劇場', 'とか', 'は', '含まれ', 'ない', 'みたい', 'だけ', 'ど', '▁', '全国', 'から', 'ファン', 'を', '▁', '武道', '館', 'に', '来', 'させる', 'の', 'かな', '▁', '中止', 'もしくは', '▁', '武道', '館', 'に', '来', 'なくても', '払い', '戻し', 'します', '▁', '位の', '措置', 'は', '取った', '方が', 'いい', 'と思う', '[SEP]']\n",
      "predict:  0\n",
      "tokens:  ['[CLS]', '▁', 'こういう', '事がある', 'から', '鬼', '滅', 'は', '面', '白', 'さ', 'も', '運', 'もあった', 'んだ', 'ろう', 'な', '▁', '鬼', '滅', 'が', '今', 'の', '時期に', 'やって', 'たら', '記録は', 'どう', 'なっていた', 'か', '[SEP]']\n",
      "predict:  1\n",
      "pred_list:  [[-1], [1]]\n",
      "good_ratio_list:  [[7.961538461538462], [11.466666666666667]]\n",
      "\n",
      "File Name:  features_003.csv\n",
      "tokens:  ['[CLS]', '▁', '自社', 'ビル', 'を', '所有する', 'の', 'は', '、', '▁', '会社', 'として', '、', '素晴らしい', '事', 'です', 'が', '、', '▁', '経営', 'が', '傾', 'く', '一つの', '原因', 'で', '、', '▁', '自社', 'ビル', 'の建設', '費用', 'が', '重', '荷', 'になり', '、', '▁', 'という', 'の', 'をよく', '見かけ', 'ます', '。', '▁', 'もちろん', '、', '良い', '人材', 'を集める', 'ため', '▁', 'など', 'の目的は', 'ある', 'と思います', 'が', '、', '▁', 'コロ', 'ナ', 'で', '不動産', '価値', 'にまで', '、', '▁', '変化', 'が', '始まって', '、', '何', 'から', '何', 'まで', '、', '▁', '変化', 'が', '始まって', 'しまった', '感', 'が', '▁', '感じ', 'ます', '。', '個人', '的には', '、', '中国の', '人が', '▁', '買', 'わなかった', '事が', '良かった', 'と思います', '。', '[SEP]']\n",
      "predict:  1\n",
      "tokens:  ['[CLS]', '▁', 'レ', 'コ', '大', 'に', '金', 'を出して', '賞', 'を', 'と', 'らせる', '余裕', 'も', 'なかった', 'な', '▁', '力を', '失', 'えば', '、', '様々な', 'ことが', '露', '見', 'していく', 'の', 'でしょう', 'ね', '▁', '裏', 'との', '繋がり', '、', '薬', 'など', 'など', '▁', '業界', 'から', '煩', 'わ', 'しく', 'なれば', '、', '簡単に', '逮捕される', 'だろう', 'し', '[SEP]']\n",
      "predict:  1\n",
      "pred_list:  [[1], [1]]\n",
      "good_ratio_list:  [[6.413897280966768], [9.019292604501608]]\n",
      "\n",
      "y_train:  [1, 1]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from keras import utils\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_bert import get_custom_objects\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "#sys.pathに追加（必要なのか調査が必要）\n",
    "sys.path.append('modules')\n",
    "\n",
    "# 上にあったのと同じ？ → predictように変更\n",
    "def _get_indice_pred(feature, maxlen):\n",
    "    indices = np.zeros((maxlen), dtype=np.int32)\n",
    "\n",
    "    tokens = []\n",
    "    tokens.append('[CLS]')\n",
    "    tokens.extend(spp.encode_as_pieces(feature))\n",
    "    tokens.append('[SEP]')\n",
    "\n",
    "    for t, token in enumerate(tokens):\n",
    "        if t >= maxlen:\n",
    "            break\n",
    "        try:\n",
    "            indices[t] = spp.piece_to_id(token)\n",
    "        except:\n",
    "            logging.warn('unknown')\n",
    "            indices[t] = spp.piece_to_id('<unk>')\n",
    "            \n",
    "    return indices, tokens\n",
    "\n",
    "\n",
    "# SentencePieceProccerモデルの読込\n",
    "spp = spm.SentencePieceProcessor()\n",
    "spp.Load('./downloads/bert-wiki-ja/wiki-ja.model')\n",
    "\n",
    "# BERTの学習したモデルの読込（ダウンロードした？勝手に保存される？）\n",
    "model_filename = './downloads/models/knbc_finetuning.model'\n",
    "model = load_model(model_filename, custom_objects=get_custom_objects())\n",
    "#model = load_model(model_filename, custom_objects=SeqSelfAttention.get_custom_objects())\n",
    "model = Model(inputs=model.input, outputs=[model.output, model.get_layer('Encoder-12-MultiHeadSelfAttention').output])\n",
    "# ↑ここでmodel = Model(inputs=a, outputs=b) としてAttentionも出すようにする。\n",
    "\n",
    "\n",
    "# 上のと同じのを入れると思われるため、消していいかも(ファイルを分けるなら必要)\n",
    "#SEQ_LEN = 103#206\n",
    "maxlen = SEQ_LEN\n",
    "\n",
    "y_train = []\n",
    "#for i in range(file_count):\n",
    "for i in range(3):\n",
    "    n_file = str(i+1).zfill(3)\n",
    "    file_name = \"features_\" + n_file + \".csv\"\n",
    "    f_path = (\"./datasets/pred_labeling/\" + file_name)\n",
    "    if not os.path.isfile(f_path):\n",
    "        continue\n",
    "\n",
    "    df_tests_features = pd.read_csv(f_path)\n",
    "    print(\"File Name: \", file_name)\n",
    "    \n",
    "    #excelファイル保管用\n",
    "    excel_file = './attention_excel/attention_' + n_file + '.xlsx'\n",
    "    writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')\n",
    "        \n",
    "    wb = openpyxl.Workbook()\n",
    "    sheet = wb.active\n",
    "    #sheet.title = 'test_sheet_1'\n",
    "    wb.save(excel_file)\n",
    "        \n",
    "        \n",
    "    pred_list = []\n",
    "    good_ratio_list = []\n",
    "#    for j in range(len(df_tests_features)):\n",
    "    for j in range(2):\n",
    "        feature = df_tests_features.loc[j]['feature']\n",
    "\n",
    "        test_features = []\n",
    "        indices, tokens = _get_indice_pred(feature, maxlen)\n",
    "        test_features.append(indices)\n",
    "\n",
    "        #勝手に追加\n",
    "        test_features = np.array(test_features)\n",
    "\n",
    "        test_segments = np.zeros(\n",
    "            (len(test_features), maxlen), dtype=np.float32)\n",
    "\n",
    "        # model = Modelを使えば推定　predict[0][0]　２次元のリストで返せる。\n",
    "        predicted = model.predict([test_features, test_segments])#.argmax(axis=1)\n",
    "        #predict = model.predict(test_features)\n",
    "\n",
    "        y_pred = predicted[0].argmax(axis=1)\n",
    "        print(\"tokens: \", tokens)\n",
    "        print(\"predict: \", y_pred[0])\n",
    "        \n",
    "        if y_pred[0] > 0.5:\n",
    "            pred_list.append([1])\n",
    "        else:\n",
    "            pred_list.append([-1])\n",
    "        \n",
    "        \n",
    "        # 高評価度算出\n",
    "        good = df_tests_features.loc[j]['good']\n",
    "        bad = df_tests_features.loc[j]['bad']\n",
    "        \n",
    "        if bad == 0:\n",
    "            good_ratio = [0]\n",
    "        else:\n",
    "            good_ratio = [good/bad]\n",
    "        \n",
    "        good_ratio_list.append(good_ratio)\n",
    "        \n",
    "        \n",
    "        # 入力シーケンスはpad_sequenceにより、以下の様に0でpre paddingしています。\n",
    "        # [0 0 0 0 x1(300) x2(300) x3(300)] ←３００は (None, 11, 300) の\n",
    "        # Attention Weightは入力シーケンスに対応して計算されるため、\n",
    "        # 入力シーケンスのpadding分シフトします。\n",
    "        #weights = [w.max() for w in predicted[1][0][-len(tokens):]]\n",
    "        weights = [w.max() for w in predicted[1][0]]#[-len(tokens):]\n",
    "        df = pd.DataFrame([tokens, weights], index=['token', 'weight']).T\n",
    "\n",
    "        mean = np.asarray(weights).mean()#np.asarray　参照コピー\n",
    "\n",
    "        df['rank'] = df['weight'].rank(ascending=False)#ランキング\n",
    "        # wから平均を引いた値が0より大きいものだけ（偏差）\n",
    "        df['normalized'] = df['weight'].apply(lambda w: max(w - mean, 0))#行全体や列全体に対して、同じ操作\n",
    "        df['weight'] = df['weight'].astype('float32')\n",
    "        df['attention'] = df['normalized'] > 0\n",
    "        # df.style.background_gradient で色つけ\n",
    "        df = df.style.background_gradient(cmap='Blues', subset=['normalized'])\n",
    "\n",
    "        # excel に保存\n",
    "        sheetname=\"comment\" + str(j)\n",
    "        with pd.ExcelWriter(excel_file, engine=\"openpyxl\", mode=\"a\") as writer:\n",
    "            df.to_excel(writer, sheet_name=sheetname, index=False)\n",
    "        \n",
    "        #display(df)\n",
    "        \n",
    "    y = np.array(pred_list)*np.array(good_ratio_list)\n",
    "    \n",
    "    if y.mean() > 0:\n",
    "        y_train.append(1) \n",
    "    else:\n",
    "        y_train.append(0) \n",
    "        \n",
    "    print(\"pred_list: \", pred_list)\n",
    "    print(\"good_ratio_list: \", good_ratio_list)\n",
    "    print()\n",
    "\n",
    "print(\"y_train: \", y_train)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力シーケンスはpad_sequenceにより、以下の様に0でpre paddingしています。\n",
    "# [0 0 0 0 x1(300) x2(300) x3(300)]\n",
    "# Attention Weightは入力シーケンスに対応して計算されるため、\n",
    "# 入力シーケンスのpadding分シフトします。\n",
    "weights = [w.max() for w in predicted_test_labels[1][0]]#[-len(tokens):]\n",
    "df = pd.DataFrame([tokens, weights], index=['token', 'weight']).T\n",
    "\n",
    "mean = np.asarray(weights).mean()#np.asarray　参照コピー\n",
    "\n",
    "df['rank'] = df['weight'].rank(ascending=False)#ランキング\n",
    "# wから平均を引いた値が0より大きいものだけ（偏差）\n",
    "df['normalized'] = df['weight'].apply(lambda w: max(w - mean, 0))#行全体や列全体に対して、同じ操作\n",
    "df['weight'] = df['weight'].astype('float32')\n",
    "df['attention'] = df['normalized'] > 0\n",
    "# df.style.background_gradient で色つけ\n",
    "df = df.style.background_gradient(cmap='Blues', subset=['normalized'])\n",
    "\n",
    "\n",
    "\n",
    "# 入力シーケンスはpad_sequenceにより、以下の様に0でpre paddingしています。\n",
    "# [0 0 0 0 x1(300) x2(300) x3(300)]\n",
    "# Attention Weightは入力シーケンスに対応して計算されるため、\n",
    "# 入力シーケンスのpadding分シフトします。\n",
    "weights = [w.max() for w in predicted_test_labels[1][0]]#[-len(tokens):]\n",
    "df_2 = pd.DataFrame([tokens, weights], index=['token', 'weight']).T\n",
    "\n",
    "mean = np.asarray(weights).mean()#np.asarray　参照コピー\n",
    "\n",
    "df_2['rank'] = df_2['weight'].rank(ascending=False)#ランキング\n",
    "# wから平均を引いた値が0より大きいものだけ（偏差）\n",
    "df_2['normalized'] = df_2['weight'].apply(lambda w: max(w - mean, 0))#行全体や列全体に対して、同じ操作\n",
    "df_2['weight'] = df_2['weight'].astype('float32')\n",
    "df_2['attention'] = df_2['normalized'] > 0\n",
    "# df.style.background_gradient で色つけ\n",
    "df_2 = df_2.style.background_gradient(cmap='Blues', subset=['normalized'])\n",
    "\n",
    "#df_cat = pd.concat([df, df_2], axis=1)\n",
    "\n",
    "\n",
    "#display(df_cat)\n",
    "\n",
    "\n",
    "\n",
    "writer = pd.ExcelWriter('./attention_excel/try.xlsx', engine='xlsxwriter')\n",
    "\n",
    "df.to_excel(writer, sheet_name='Sheet 1')\n",
    "df_2.to_excel(writer, sheet_name='Sheet 2')\n",
    "writer.save()\n",
    "\n",
    "sheetname=\"test1\"\n",
    "# savenameは例えば\"../output/sample.xlsx\"のような、エクセルの出力に対応した名称\n",
    "with pd.ExcelWriter('try.xlsx', engine=\"openpyxl\", mode=\"a\") as writer:\n",
    "    df.to_excel(writer, sheet_name=sheetname, index=False)\n",
    "\n",
    "sheetname=\"test2\"\n",
    "    # savenameは例えば\"../output/sample.xlsx\"のような、エクセルの出力に対応した名称\n",
    "with pd.ExcelWriter('try.xlsx', engine=\"openpyxl\", mode=\"a\") as writer:\n",
    "    df_2.to_excel(writer, sheet_name=sheetname, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.41389728]\n",
      " [9.0192926 ]]\n",
      "7.716594942734188\n"
     ]
    }
   ],
   "source": [
    "pred_list=[[1], [1]]\n",
    "good_ratio_list=[[6.413897280966768], [9.019292604501608]]\n",
    "\n",
    "y = np.array(pred_list)*np.array(good_ratio_list)\n",
    "\n",
    "print(y)\n",
    "print(y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論時にAttention Weightを可視化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_features_df = pd.read_csv('./datasets/pred_labeling/features_001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/.keras/.models/wikija-sentencepiece_300.model\n",
      "/tmp/.keras/.models/wikija-sentencepieced_word2vec_300.model\n",
      "/tmp/.keras/.models/wikija-sentencepieced_word2vec_300.model.wv.vectors.npy\n",
      "/tmp/.keras/.models/wikija-sentencepieced_word2vec_300.model.trainables.syn1neg.npy\n"
     ]
    }
   ],
   "source": [
    "from text_vectorian import SentencePieceVectorian\n",
    "vectorian = SentencePieceVectorian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0    14\n",
      "  33519 14827    22  2194  5492     3 19756   525    25  3375  2049  1222\n",
      "   4081    73    14  2194  5492     8  1100 24596 11360  9758 11522  1998\n",
      "  13733   117]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1434 predict_step\n        return self(x, training=False)\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:204 assert_input_compatibility\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer model_9 expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 206) dtype=int32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-275cc8d14cf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# 推定　predict[0][0]　２次元のリストで返る。０：モデルのアウトプット、１：Attentionのアウトプット\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# input を線区切りで表示\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m-> 2941\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3355\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3356\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[0;32m-> 3357\u001b[0;31m             return self._define_function_with_shape_relaxation(\n\u001b[0m\u001b[1;32m   3358\u001b[0m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3277\u001b[0m           expand_composites=True)\n\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3279\u001b[0;31m     graph_function = self._create_graph_function(\n\u001b[0m\u001b[1;32m   3280\u001b[0m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[1;32m   3281\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1434 predict_step\n        return self(x, training=False)\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:204 assert_input_compatibility\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer model_9 expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 206) dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "# predictしたいテスト用featureファイルのindexを指定\n",
    "input_index = 1#83\n",
    "maxlen = 206\n",
    "\n",
    "# テスト用featureファイルの該当のテキスト\n",
    "input_text = tests_features_df['feature'][input_index]\n",
    "# text-vectgorian のインスタンス\n",
    "# 形態素分解？spp = spm.SentencePieceProcessor()で代用\n",
    "#tokens = vectorian.tokenizer._tokenizer.encode_as_pieces(input_text)\n",
    "#test_feature = vectorian.fit(input_text).indices\n",
    "#test_feature = pad_sequences([test_feature], maxlen=data['input_len'])\n",
    "#print(test_feature)\n",
    "\n",
    "\n",
    "test_features = []\n",
    "test_features.append(_get_indice(input_text, maxlen))\n",
    "\n",
    "#勝手に追加\n",
    "test_features = np.array(test_features)\n",
    "\n",
    "# 推定　predict[0][0]　２次元のリストで返る。０：モデルのアウトプット、１：Attentionのアウトプット\n",
    "predict = model.predict(test_features)\n",
    "\n",
    "# input を線区切りで表示\n",
    "display('input: ' + '|'.join(tokens))\n",
    "\n",
    "# predict の結果の最大値\n",
    "prob = predict[0][0].max()\n",
    "\n",
    "# predict[0][0].argmax() をインデックスとしてラベル名を出力。\n",
    "# prob:.2f　 で 確率も出力\n",
    "display('label: (L)' + tests_labels_df['label'][input_index] + ' == (P)' +\n",
    "        data['index2label'][predict[0][0].argmax()] + f'({prob:.2f})')\n",
    "print(predict[0][0].argmax())\n",
    "\n",
    "# 入力シーケンスはpad_sequenceにより、以下の様に0でpre paddingしています。\n",
    "# [0 0 0 0 x1(300) x2(300) x3(300)]\n",
    "# Attention Weightは入力シーケンスに対応して計算されるため、\n",
    "# 入力シーケンスのpadding分シフトします。\n",
    "weights = [w.max() for w in predict[1][0][-len(tokens):]]\n",
    "df = pd.DataFrame([tokens, weights], index=['token', 'weight']).T\n",
    "\n",
    "mean = np.asarray(weights).mean()#np.asarray　参照コピー\n",
    "\n",
    "df['rank'] = df['weight'].rank(ascending=False)#ランキング\n",
    "# wから平均を引いた値が0より大きいものだけ（偏差）\n",
    "df['normalized'] = df['weight'].apply(lambda w: max(w - mean, 0))#行全体や列全体に対して、同じ操作\n",
    "df['weight'] = df['weight'].astype('float32')\n",
    "df['attention'] = df['normalized'] > 0\n",
    "# df.style.background_gradient で色つけ\n",
    "df = df.style.background_gradient(cmap='Blues', subset=['normalized'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "モデルの保存も行う必要がある。  \n",
    "チュートリアルより。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "export_dir='./saved_model'\n",
    "tf.saved_model.save(bert_classifier, export_dir=export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 復元\n",
    "reloaded = tf.saved_model.load(export_dir)\n",
    "reloaded_result = reloaded([my_examples['input_word_ids'],\n",
    "                            my_examples['input_mask'],\n",
    "                            my_examples['input_type_ids']], training=False)\n",
    "\n",
    "original_result = bert_classifier(my_examples, training=False)\n",
    "\n",
    "# The results are (nearly) identical:\n",
    "print(original_result.numpy())\n",
    "print()\n",
    "print(reloaded_result.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

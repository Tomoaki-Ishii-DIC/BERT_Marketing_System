{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセット作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path_fine_train = (\"./datasets_csv/finetuning/train\")\n",
    "csv_path_fine_test = (\"./datasets_csv/finetuning/test\")\n",
    "csv_path_pred_labeling = (\"./datasets_csv/pred_labeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   date                                              title  \\\n",
      "0   2021-01-02 08:02:00          エイベックス本社ビルが「築3年」で売却　入札額トップが落札できなかった奇々怪々\\n   \n",
      "1   2020-12-25 01:06:00  エイベックス(7860)、「増配」を発表し、配当利回り11.1％に！ 年間配当は1年間で2....   \n",
      "2   2020-12-24 15:24:00            エイベックス、本社ビルの売却を発表　通期の純利益は150億円の黒字に転換へ\\n   \n",
      "3   2020-12-21 18:42:00     浜崎あゆみ、クリスマスと大みそかライブの有観客断念を発表　無観客生配信へ「残念ながら…」\\n   \n",
      "4   2020-11-07 17:01:00                 嵐や白石麻衣は大成功、音楽業界の「勝ち組」がごく一部だという現実\\n   \n",
      "5   2020-11-06 12:46:00               エイベックスが初の希望退職募集　コロナ禍でライブ・舞台関連事業に打撃\\n   \n",
      "6   2020-07-25 11:08:00  エンタメ業界は苦しい。でも…　「新型コロナ以前の世界が戻ってくることはない」エイベックスが見...   \n",
      "7   2021-01-13 13:45:00  エイベックス、中国bilibiliとライセンス契約　J-POPのMVを提供　日本の大手レーベ...   \n",
      "8   2020-08-03 06:10:00                    オンラインライブ配信はウィズコロナ時代の救世主になれるのか\\n   \n",
      "9   2021-01-13 11:26:00     ソニー、クリエイティブなエンターテインメント企業へ--世界4拠点からプレスカンファレンス\\n   \n",
      "10  2021-01-08 09:00:00                     2021年、日本アニメが世界トレンドへ飛躍する節目の年に\\n   \n",
      "11  2021-01-19 15:52:00                 コロナで収入源を失ったプロミュージシャン、過去作を現金化する動き\\n   \n",
      "12  2020-12-10 19:15:00                      音楽業界の未来、実はストリーミングではなくSNSが重要\\n   \n",
      "13  2020-11-17 07:00:00           電子チケット販売に自由を。ZAIKO COOが見据える「D2F」という勝ち筋\\n   \n",
      "14  2020-10-28 08:00:00    「鬼滅の刃」人気は株式市場にも影響　たまごっち登場、グッズ関連にぎわう…“鬼滅銘柄”を物色\\n   \n",
      "15  2020-09-28 18:20:00  クラシック音楽界の概念を超えた大型オンラインフェスのレポート到着　アーティストによるライブ＆...   \n",
      "16  2020-08-11 19:35:00  米Twitchでの音楽使用料に関してミュージシャンから親会社アマゾンのジェフ・ベゾスへ公開書簡\\n   \n",
      "\n",
      "                                                 text  \n",
      "0   銀行主導のもと進められた売却プラン\\n売却情報が流れ始めたのは、浜崎あゆみがエイベックス・松...  \n",
      "1   　エイベックスは、2021年3月期の配当予想を修正し、前期比で「増配」とする予想を、2020...  \n",
      "2   希望退職制度の応募人数は103名\\n　音楽・映像事業を手掛けるエイベックス（株）（TSR企業...  \n",
      "3   　歌手の浜崎あゆみの公式サイトが21日に更新。クリスマスの「ayumi hamasaki L...  \n",
      "4   　全国のファンを釘付けにした、活動休止前の嵐による実質的なラストライブ『アラフェス2020 ...  \n",
      "5   　音楽関連の事業を手掛けるエイベックスは11月5日、初の希望退職を募集すると発表しました。同...  \n",
      "6   新型コロナウイルスは日本のエンターテインメント業界にも大きな影響を及ぼした。相次ぐライブやイ...  \n",
      "7   　エイベックスは1月13日、同社の子会社を通じて中国の動画配信サイト「bilibili」を運...  \n",
      "8   新型コロナ感染症の影響で、会場で行われる音楽ライブや演劇、スポーツイベントなどが軒並み苦戦を...  \n",
      "9   　ソニーは1月11日午後5時（米国東部時間）から、「CES 2021」においてプレスカンファ...  \n",
      "10  　2020年の最大ヒットコンテンツとなった『鬼滅の刃』。原作コミックは最終23巻で1億200...  \n",
      "11  英国の作曲家でプロデューサーのイアン・レビン氏が英ポップグループ、「テイク・ザット」（写真）...  \n",
      "12  先見の明がある投資家たちの資金が集まるオンラインでのカラオケや音楽制作が、音楽業界にとってま...  \n",
      "13  ZAIKO取締役COO　Lauren Rose Kocher氏\\nライブの軒並み中止や、配信...  \n",
      "14  　少し前までは「鬼滅の刃」を、どう読んでいいか分からなかった大人でも、いまではもう「きめつの...  \n",
      "15  クラシック音楽界の概念を超えた大型オンラインフェスのレポート到着　アーティストによるライブ＆...  \n",
      "16  　アマゾンの創設者でCEOのジェフ・ベゾスは、先月末の議会聴聞会の証言で、同社の傘下にあるラ...  \n"
     ]
    }
   ],
   "source": [
    "# ニュース記事\n",
    "\n",
    "#df_finetuning_news = pd.read_csv(csv_path_fine + '/news/news_dataset.csv')\n",
    "df_train_news = pd.read_csv(csv_path_pred_labeling + '/news/news_dataset.csv')\n",
    "\n",
    "#print(df_finetuning_news)\n",
    "print(df_train_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuning train:\n",
      "                                                   text reply good bad\n",
      "0    ハロプロの事務所も昨年末に移転しましたよね\\n都内から都内だけど、複数の拠点を1拠点にまとめ...     1  278  53\n",
      "1    日本の芸能プロダクションも\\nアメリカのエージェントシステムに\\n移行していますね！\\nタレ...     0  235  45\n",
      "2    エイベックスの件はともかくこれから都心の地価は下がるのではないか。これだけリモートワークが推...     3  120  17\n",
      "3    その昔大学が都心から郊外や県外に脱出しました（中央大学や筑波大学など）。その結果として、人気...     1  107  20\n",
      "4    河口湖は東京都心からやや遠いけれど、都心回帰で空きが増えた八王子の大学のキャンパスなんか移転...     2   70   5\n",
      "..                                                 ...   ...  ...  ..\n",
      "490    アニプレックス強いよね。単なる円盤屋だったのにいつの間にか市場の先頭にたっていた感じ。\\n\\n     0   49   5\n",
      "491  イメージセンサーの分野でも、ソニーは負けてしまうのでしょうか？\\n誰か詳しい人、教えて下さい...     0    1   1\n",
      "492  ソニーも調子の良いうちにイメセン事業を上場もしくは売却するべき。\\n今なら５兆円前後で売却可...     0    8  26\n",
      "493  プレステ5でさらに勢いに乗りますね\\n世界に通用する日本ブランドはSONYか任天堂だけになっ...     0   37  18\n",
      "494                やはり不景気になっても強いのは子供と女性向けの商品なのだろう。\\n\\n     1    7  20\n",
      "\n",
      "[495 rows x 4 columns]\n",
      "finetuning test:\n",
      "                                                  text reply good bad\n",
      "0   イギリスから戻って来た\\n布袋寅泰氏はどうするのかね\\n緊急事態宣言に\\n劇場とかは含まれな...     2  207  26\n",
      "1   こういう事があるから鬼滅は面白さも運もあったんだろうな\\n鬼滅が今の時期にやってたら記録はど...     1  172  15\n",
      "2   延期・再延期しないだけ良かったと思わなきゃですかね。とは言え音楽界同様、ライブ配信上映を検討...     0   37  18\n",
      "3                           もう、いい加減マウスシールド止めてくれよ。\\n\\n     0   96   7\n",
      "4   銀魂やセーラームーンは運が悪いな。\\n時期的には最悪だよね。\\nまぁ公開延期にならないだけマ...     0   53   4\n",
      "5        10日の生徒会役員共の舞台挨拶はどうなる？\\n今のところ何の動きも見られないが？\\n\\n     0   11   5\n",
      "6                                映画の舞台挨拶中止はかなり痛い。\\n\\n     0   16   0\n",
      "7                  仕方ないね。もうヘラヘラできる状況じゃなくなったってことだ。\\n\\n     0   36   9\n",
      "8   今思う。\\n罹患してしまうことは仕方ない。見えない相手だから、どんなに対策をしても、罹患して...     0    9   0\n",
      "9   ほら\\n緊急事態宣言とかやると\\nこうやって過剰な反応になる\\nつい2日前とかに2万ぐらいの...     1   34   7\n",
      "10  役員報酬がバカ高く上層部ばかりが得する仕組みをやってる会社はどこも衰退していくよ。今回社員を...     0  148  16\n",
      "11  もう、馬鹿タレを法外な価格で売りつけ・使うビジネスは、コロナ無くても先細りだった。タレを囲い...     0   50  21\n",
      "12          どの会社でも言えることだけど、希望退職募ると有能から抜けてくからなぁ。。。\\n\\n     0   59  11\n",
      "13  コロナと言うよりは、時代の流れ。\\n時代の流れに乗れない企業は、特にコロナと言う波に耐えられ...     0  105  29\n",
      "14          建物があることに価値がある時代が変わってくるんだろう\\nテレワークの時代で\\n\\n     0    1   0\n",
      "15                 まあ\\nこれまでのツケを\\n返してマイナスになる時が来たな。\\n\\n     0   44   8\n",
      "16             芸能界は飽きられた\\n\\n広告もネットに散った\\n\\n先細りの運命か\\n\\n     0   47   7\n",
      "17                              エイベックスのやり方はもう時代遅れ\\n\\n     0   33   5\n",
      "18                          盛者必衰の理を現す。　もう「時代」じゃ無い\\n\\n     0   34  15\n",
      "19  エイベックスってビル建てて引っ越したばっかりだろ？\\n流石にこれは気の毒としか言いようがない...     0    7  17\n"
     ]
    }
   ],
   "source": [
    "# ニュースコメント\n",
    "\n",
    "import os\n",
    "\n",
    "csv_folder = [csv_path_fine_train , csv_path_fine_test, csv_path_pred_labeling]\n",
    "\n",
    "file_count = 0\n",
    "for p in csv_folder:\n",
    "    file_count += sum((len(f) for _, _, f in os.walk(p + '/comments'))) - 1\n",
    "#print(file_count)\n",
    "\n",
    "for j, p in enumerate(csv_folder):\n",
    "    cols = ['text', 'reply', 'good', 'bad']\n",
    "    df_temp = pd.DataFrame(columns=cols)\n",
    "    \n",
    "    for i in range(file_count):\n",
    "        n_file = str(i+1).zfill(3)\n",
    "        file_name = \"comment_dataset_\" + n_file + \".csv\"\n",
    "        file_path = (p + \"/comments/\" + file_name)\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "        #print(str(i+1).zfill(3))#あとで消す\n",
    "        df_cmt = pd.read_csv(file_path, index_col=0)\n",
    "        #df_temp = pd.concat([df_temp, df_cmt], ignore_index=True)\n",
    "              \n",
    "        #代入\n",
    "        if j <= 1:\n",
    "            df_temp = pd.concat([df_temp, df_cmt], ignore_index=True)\n",
    "        else:\n",
    "            df_cmt.columns = [\"feature\", \"reply\", \"good\", \"bad\"]\n",
    "            df_cmt[[\"feature\", \"good\", \"bad\"]].to_csv(\"./datasets/pred_labeling/features_\" + n_file + \".csv\", index=False)\n",
    "            #df_pred_comments = df_temp.copy()\n",
    "        \n",
    "    #代入\n",
    "    if j == 0:\n",
    "        df_fine_train_comments = df_temp.copy()\n",
    "    elif j == 1:\n",
    "        df_fine_test_comments = df_temp.copy()\n",
    "        \n",
    "print(\"finetuning train:\\n\", df_fine_train_comments)\n",
    "print(\"finetuning test:\\n\", df_fine_test_comments)\n",
    "#print(\"pred labeling:\\n\", df_pred_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels train:\n",
      "         label\n",
      "0    positive\n",
      "1    positive\n",
      "2    positive\n",
      "3    negative\n",
      "4    positive\n",
      "..        ...\n",
      "490  positive\n",
      "491  negative\n",
      "492  negative\n",
      "493  positive\n",
      "494  positive\n",
      "\n",
      "[495 rows x 1 columns]\n",
      "labels test:\n",
      "        label\n",
      "0   negative\n",
      "1   negative\n",
      "2   negative\n",
      "3   negative\n",
      "4   positive\n",
      "5   negative\n",
      "6   negative\n",
      "7   negative\n",
      "8   negative\n",
      "9   positive\n",
      "10  negative\n",
      "11  negative\n",
      "12  negative\n",
      "13  negative\n",
      "14  positive\n",
      "15  negative\n",
      "16  negative\n",
      "17  negative\n",
      "18  negative\n",
      "19  positive\n"
     ]
    }
   ],
   "source": [
    "#ラベル\n",
    "import os\n",
    "\n",
    "csv_folder = [csv_path_fine_train , csv_path_fine_test]\n",
    "\n",
    "#file_count = 0\n",
    "#for p in csv_folder:\n",
    "#    file_count += sum((len(f) for _, _, f in os.walk(p + '/comments'))) - 1\n",
    "#print(file_count)\n",
    "\n",
    "for j, p in enumerate(csv_folder):\n",
    "    cols = ['label']\n",
    "    df_temp = pd.DataFrame(columns=cols)\n",
    "\n",
    "    for i in range(file_count):\n",
    "        n_file = str(i+1).zfill(3)\n",
    "        file_name = \"comment_labels_\" + n_file + \".csv\"\n",
    "        file_path = (p + \"/labels/\" + file_name)\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "        #print(str(i+1).zfill(3))#あとで消す\n",
    "        df_labels = pd.read_csv(file_path)#, index_col=0\n",
    "        # concat\n",
    "        #print(df_labels)\n",
    "        df_temp = pd.concat([df_temp, df_labels], ignore_index=True)\n",
    "\n",
    "    #代入\n",
    "    if j == 0:\n",
    "        df_fine_train_labels = df_temp.copy()\n",
    "    elif j == 1:\n",
    "        df_fine_test_labels = df_temp.copy()\n",
    "        \n",
    "print(\"labels train:\\n\", df_fine_train_labels)\n",
    "print(\"labels test:\\n\", df_fine_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの作成と保存\n",
    "\n",
    "datasets_folder =  (\"./datasets/finetuning\")\n",
    "\n",
    "\n",
    "#print(\"finetuning train:\\n\", df_fine_train_comments)\n",
    "#print(\"finetuning test:\\n\", df_fine_test_comments)\n",
    "#print(\"pred labeling:\\n\", df_pred_comments)\n",
    "\n",
    "df_fine_train_comments.columns = [\"feature\", \"reply\", \"good\", \"bad\"]\n",
    "df_fine_train_comments[\"feature\"].to_csv(datasets_folder + \"/train/features.csv\", index=False)\n",
    "\n",
    "df_fine_test_comments.columns = [\"feature\", \"reply\", \"good\", \"bad\"]\n",
    "df_fine_test_comments[\"feature\"].to_csv(datasets_folder + \"/test/features.csv\", index=False)\n",
    "\n",
    "#print(\"labels train:\\n\", df_fine_train_labels)\n",
    "#print(\"labels test:\\n\", df_fine_test_labels)\n",
    "\n",
    "df_fine_train_labels[\"label\"].to_csv(datasets_folder + \"/train/labels.csv\", index=False)\n",
    "df_fine_test_labels[\"label\"].to_csv(datasets_folder + \"/test/labels.csv\", index=False)\n",
    "\n",
    "# 上に持っていく\n",
    "#df_pred_comments.columns = [\"feature\", \"reply\", \"good\", \"bad\"]\n",
    "#df_pred_comments[\"feature\"].to_csv(\"./datasets/pred_labeling/features.csv\", index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語分割する関数を定義\n",
    "import MeCab\n",
    "import re\n",
    "import string\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    '''\n",
    "    前処理\n",
    "    '''\n",
    "    # 改行コードを消去\n",
    "    text = re.sub('<br />', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "\n",
    "    # カンマ、ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        if (p == \".\") or (p == \",\"):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "            \n",
    "    for p in string.punctuation:\n",
    "        if (p == \"。\") or (p == \"、\"):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, \"　\")\n",
    "            \n",
    "    # ピリオドなどの前後にはスペースを入れておく\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    text = text.replace(\"。\", \" 。 \")\n",
    "    text = text.replace(\"、\", \" 、 \")\n",
    "    \n",
    "    return text\n",
    "\n",
    "#m_t = MeCab.Tagger('-Owakati -d /usr/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "#m_t = MeCab.Tagger('-Ochasen -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "wakati = MeCab.Tagger(\"-Owakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "def tokenizer_mecab(text):\n",
    "    '''\n",
    "    分かち書き\n",
    "    '''\n",
    "    words = wakati.parse(text).split()\n",
    "    \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized:\n",
      " ['機械学習', 'が', '好き', 'です', '。', '＋', '＋', '？？？', '：', '：', '機械学習', 'が', '好き', 'です', '。']\n"
     ]
    }
   ],
   "source": [
    "text = '機械学習が好きです。\\n＋＋？？？：：機械学習が好きです。'\n",
    "\n",
    "pre_text = preprocessing_text(text)\n",
    "tokenized_text = tokenizer_mecab(pre_text)\n",
    "print(\"tokenized:\\n\", tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre:\n",
      " 機械学習が好きです 。 ＋＋？？？：：機械学習が好きです 。 \n",
      "tokenized:\n",
      " ['機械学習', 'が', '好き', 'です', '。', '＋', '＋', '？？？', '：', '：', '機械学習', 'が', '好き', 'です', '。']\n",
      "text tokenized:\n",
      " 15\n"
     ]
    }
   ],
   "source": [
    "text = '機械学習が好きです。\\n＋＋？？？：：機械学習が好きです。'\n",
    "\n",
    "pre_text = preprocessing_text(text)\n",
    "print(\"pre:\\n\", pre_text)\n",
    "tokenized_text = tokenizer_mecab(pre_text)\n",
    "print(\"tokenized:\\n\", tokenized_text)\n",
    "print(\"text tokenized:\\n\", len(tokenized_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bert-tensorflow\n",
    "#!pip install keras-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tech-blog.cloud-config.jp/2020-02-06-category-classification-using-bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone --recurse-submodules https://github.com/yoheikikuta/bert-japanese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定ファイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features_df:\n",
      "                                                feature\n",
      "0    ハロプロの事務所も昨年末に移転しましたよね\\n都内から都内だけど、複数の拠点を1拠点にまとめ...\n",
      "1    日本の芸能プロダクションも\\nアメリカのエージェントシステムに\\n移行していますね！\\nタレ...\n",
      "2    エイベックスの件はともかくこれから都心の地価は下がるのではないか。これだけリモートワークが推...\n",
      "3    その昔大学が都心から郊外や県外に脱出しました（中央大学や筑波大学など）。その結果として、人気...\n",
      "4    河口湖は東京都心からやや遠いけれど、都心回帰で空きが増えた八王子の大学のキャンパスなんか移転...\n",
      "..                                                 ...\n",
      "490    アニプレックス強いよね。単なる円盤屋だったのにいつの間にか市場の先頭にたっていた感じ。\\n\\n\n",
      "491  イメージセンサーの分野でも、ソニーは負けてしまうのでしょうか？\\n誰か詳しい人、教えて下さい...\n",
      "492  ソニーも調子の良いうちにイメセン事業を上場もしくは売却するべき。\\n今なら５兆円前後で売却可...\n",
      "493  プレステ5でさらに勢いに乗りますね\\n世界に通用する日本ブランドはSONYか任天堂だけになっ...\n",
      "494                やはり不景気になっても強いのは子供と女性向けの商品なのだろう。\\n\\n\n",
      "\n",
      "[495 rows x 1 columns]\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ハロプロ', 'の', '事務所', 'も', '昨年末', 'に', '移転', 'し', 'まし', 'た', 'よ', 'ね', '都内', 'から', '都内', 'だ', 'けど', '、', '複数', 'の', '拠点', 'を', '1', '拠点', 'に', 'まとめ', 'た', 'みたい', '経営', '立て直し', 'する', '上', 'で', '、', '固定費', 'の', '中', 'で', 'も', 'まず', '着手', 'する', 'の', 'は', '家賃', 'です', 'から', 'ね', '。', 'しんどい', '状況', 'です', 'が', '頑張っ', 'て', '欲しい', 'です', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '日本', 'の', '芸能プロダクション', 'も', 'アメリカ', 'の', 'エージェント', 'システム', 'に', '移行', 'し', 'て', 'い', 'ます', 'ね', '！', 'タレント', 'が', 'エージェント', 'を', '雇う', '！', '立場', 'が', '逆転', 'する', 'わけ', 'です', 'が', '1世紀', 'かけ', 'て', '本来', 'の', 'マネジメント', 'の', '姿', 'に', '戻る', '訳', 'です', 'ね', '！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エイベックス', 'の', '件', 'は', 'ともかく', 'これから', '都心', 'の', '地価', 'は', '下がる', 'の', 'で', 'は', 'ない', 'か', '。', 'これだけ', 'リモートワーク', 'が', '推奨', 'さ', 'れ', 'かつ', '円滑', 'に', '仕事', 'が', '進む', 'よう', 'なら', '都心', 'に', '事務所', 'を', '構え', 'て', 'いる', '会社', 'の', '多く', 'は', '売却', 'や', '移転', 'を', 'する', 'なら', '考える', 'の', 'は', '普通', 'の', '流れ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'その昔', '大学', 'が', '都心', 'から', '郊外', 'や', '県外', 'に', '脱出', 'し', 'まし', 'た', '（', '中央大学', 'や', '筑波大学', 'など', '）', '。', 'その', '結果', 'として', '、', '人気', 'が', '出', 'た', '大学', 'は', 'ほとんど', 'なく', '、', '結果的', 'に', '大学', 'ランキング', 'で', 'も', '上位', 'を', '占める', 'こと', 'が', 'だんだん', 'と', '難しく', 'なっ', 'た', '過去', 'が', 'あり', 'ます', '。', '今', 'は', 'その', '反省', 'な', 'の', 'か', '少子化対策', 'なのか', '、', '学生', '集め', 'に', '必死', 'な', '大学', 'は', '東京駅', 'の', 'すぐ', '近く', 'に', 'キャンパス', 'と', 'は', '呼べ', 'ない', 'ビル', 'の', 'フロア', 'を', '借り', 'てる', 'ぐらい', 'です', '。', '芸能事務所', 'が', '同じ', 'と', 'は', '思い', 'ませ', 'ん', 'が', '、', '単に', '田舎', 'に', '引っ越す', 'の', 'で', 'あれ', 'ば', '、', 'それ', 'は', 'コスト削減', '以外', 'に', 'は', '意味', 'が', 'ない', 'でしょ', 'う', '。', '空気', 'が', '綺麗', 'な', 'の', 'と', '、', 'タレント', 'が', '仕事', 'を', 'し', 'やすかっ', 'たり', '、', '事務所', 'が', '反映', 'する', 'こと', 'は', '関係', 'が', 'ない', 'です', 'から', '。', '結局', '、', '支社', 'という', 'こと', 'で', '東京', 'に', '小さな', 'オフィス', 'は', '借りる', 'ん', 'だ', 'と', '思い', 'ます', '。', '間', 'を', 'とっ', 'て', '浅草', 'という', 'の', 'は', 'アイデア', 'の', '一つ', 'だ', 'と', '思い', 'ます', '。', '移動', 'に', 'は', '便利', 'で', '不動産', '価格', 'も', 'ビジネス街', 'より', 'は', '高く', 'は', 'ない', 'です', 'から', '。', 'あと', 'は', 'その', '街', 'を', '好き', 'か', 'どう', 'か', 'だけ', 'な', 'ので', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '河口湖', 'は', '東京都', '心', 'から', 'やや', '遠い', 'けれど', '、', '都心回帰', 'で', '空き', 'が', '増え', 'た', '八王子', 'の', '大学', 'の', 'キャンパス', 'なんか', '移転先', 'の', '候補', '地', 'に', '上がら', 'ない', 'の', 'か', 'な', '？', '所属', 'の', 'タレント', 'が', '日常的', 'に', '使う', 'レッスン', '場', 'や', 'プロモーションビデオ', 'の', '撮影', 'など', 'に', 'もってこい', 'だ', 'と', '思う', 'けれど', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '会長', 'と', '本社', 'が', '河口湖', 'へ', '移転', 'し', 'て', '実務', 'の', '社員', 'は', 'TV', '局', 'や', '都心', 'に', '近い', '雑居ビル', 'に', '残る', 'の', 'で', 'は', '。', '取引先', 'が', 'そんな', '感じ', 'で', '青山', 'から', '地方', 'と', '郊外', 'に', '分散', '移転', 'し', 'た', 'よ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '昨年', 'は', '、', '大手', 'の', '芸能事務所', 'から', 'の', '独立', '・', '退社', 'が', '相次ぎ', 'まし', 'た', '。', '今年', 'は', '、', 'さらに', '増える', 'でしょ', 'う', '。', '大手', 'の', '事務所', 'も', '、', '看板', 'と', '言わ', 'れる', '人', 'を', '月給', '制', 'で', '維持', 'し', 'て', 'ゆく', 'に', 'は', '環境', 'が', '厳しい', '。', 'と', '言っ', 'て', '、', 'いきなり', '歩合', '制', 'に', 'も', 'でき', 'ない', '。', 'なら', 'ば', '、', '赤坂', 'や', '青山', 'や', '表参道', 'に', '事務所', 'を', '置く', '理由', 'も', 'ない', 'し', '、', '今', 'の', '次期', 'です', 'から', '、', '事務所', 'も', 'リモートワーク', 'に', '徹すれ', 'ば', '良い', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '吉本興業', '東京', '本部', 'が', '2008年', 'から', '使っ', 'て', 'いる', 'の', 'は', '「', '旧', '新宿', '区立', '四谷', '第五小学校', '」', '。', '1934年', '竣工', 'の', '昭和', 'モダニズム建築', 'に', 'そっくり', 'そのまま', '入っ', 'て', 'いる', '。', '歌舞伎町', '、', '新宿ゴールデン街', '、', '花園神社', 'の', 'すぐ', '近く', '。', 'これ', 'は', '良い', '建物', '良い', '場所', 'を', '見つけ', 'た', 'な', 'と', '思っ', 'た', 'な', '。', '地代', 'や', '賃料', 'が', '高い', 'と', 'は', 'いえ', '、', '東京都内', 'に', 'は', 'まだまだ', '知ら', 'れ', 'て', 'い', 'ない', '穴場', '的', '場所', 'が', 'ある', '。', '空き', '物件', 'も', '増え', 'て', 'いる', 'し', '、', '意外', 'な', 'ところ', 'に', '意外', 'な', '事務所', 'が', 'ある', 'って', '言う', 'の', 'も', '、', '今後', 'は', '売り', 'に', 'なる', 'ん', 'じゃ', 'ない', 'か', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '海外', 'ファンド', 'から', 'の', '買い', 'が', '多く', 'て', '東京都心', '部', 'の', '100', '単位', 'の', '不動産', 'は', '上がっ', 'て', 'い', 'ます', '。', 'エイベックス', 'は', '290億円', 'も', 'の', '売却', '益', 'を', '得', 'て', '、', 'イベント', 'の', '売上', '減少', 'に', '備え', 'て', 'しっかり', 'キャッシュ', 'を', '確保', 'でき', 'た', 'ので', '良い', 'とき', 'に', '売っ', 'た', 'の', 'で', 'は', '。', 'また', 'イベント', '売上', '減', 'で', 'トータル', 'の', '利益', 'は', 'かなり', '減っ', 'て', 'いる', '今', 'なら', '売却', '益', 'による', '納税額', 'も', '圧縮', '出来る', 'でしょ', 'う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '新た', 'な', 'コロナ', 'の', '出現', '、', '地球温暖化', 'による', '異常気象', 'が', 'もたらす', '台風', 'の', '大型', '化', '、', '集中豪雨', 'に', '伴う', '都心', 'の', '河川', 'の', '氾濫', '、', '南極', '氷河', 'が', '溶け', 'て', '20年後', 'に', 'は', '海面', 'が', '1m', '上昇', 'する', 'と', '言わ', 'れ', 'て', 'いる', '。', '21年', 'は', '今', 'まで', 'の', '価値観', 'が', '大きく', '変わる', '年', '。', '都会', '、', '東京', 'に', '住む', 'こと', 'が', 'ステイタス', 'だっ', 'た', 'こと', 'が', 'これから', 'は', 'リスク', 'に', 'なる', '時代', 'な', 'ん', 'でしょ', 'う', '。', 'ここに', '既に', '気がつい', 'た', '人達', 'が', '避難', 'し', 'て', 'いる', 'の', 'でしょ', 'う', '。', '繁栄', 'を', '謳歌', 'し', '過ぎ', 'た', '人間', 'へ', 'の', 'しっぺ返し', '以外', 'の', '何もの', 'で', 'も', 'ない', 'でしょ', 'う', '。', '仕事', 'も', '学校', 'も', 'リモート', '化', 'で', '都心', 'に', '住む', '必要', 'も', '無い', 'こと', 'に', '気づい', 'た', '人達', '、', '言わば', '変化', 'に', '対応', '出来る', '人', 'だけ', 'が', '生き延びる', '。', 'そんな', '世界', 'が', 'き', 'て', 'い', 'ます', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '東京', 'において', '、', '浅草', '無くし', 'て', '芸事', 'は', '無い', '。', 'です', 'ので', '、', '浅草', 'に', '事務所', 'を', '移す', 'なんて', '、', '下', 'に', '見', 'た', '言い方', 'は', 'まずい', 'か', 'と', '思い', 'ます', 'が', '…', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '河口湖', 'から', '1', '5時間', 'で', '東京', '…', '。', '新宿', 'なら', 'いい', 'けど', 'そこ', 'から', '先', 'だ', 'と', '速度', 'オーバー', 'に', 'なる', 'と', '思う', 'けど', 'な', '。', 'ただ', '中央道', 'の', '渋滞', 'を', '知ら', 'ない', 'の', 'か', 'なぁ', '。', '平日', 'の', '日中', 'で', 'も', '上り線', 'は', '混ん', 'で', 'いる', 'し', '土日', 'も', '朝一番', 'は', '上り線', 'は', '混ん', 'で', 'いる', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ある日', '青山通り', 'を', '歩い', 'て', 'い', 'たら', '、', 'なん', 'か', '特別', 'な', '存在', '感', 'の', 'ある', 'ビル', 'の', '前', 'で', '立ち止まっ', 'た', '。', 'エイベックス', '？', '？', 'あの', 'エイベックス', 'の', 'ビル', '？', 'と', '視線', 'を', '上', 'に', 'し', 'て', 'ビル', 'を', '見上げ', 'た', '時', 'の', '感覚', 'が', '忘れ', 'られ', 'ない', '。', '広く', '取っ', 'た', '敷地', 'と', 'いい', '圧巻', 'でし', 'た', '。', 'なんとか', 'の', '夢', 'の', 'あと', '？', 'って', '感じ', 'です', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'お', '店', 'で', '言う', '薄利多売', 'で', 'は', 'ない', 'です', 'が', 'オフィスビル', '、', 'マンション', '、', '店舗', 'の', 'オーナー', 'さん', 'も', '長期', '機関', '借り', 'て', 'くれ', 'た', '店子', 'さん', '含め', '今後', 'は', '空い', 'たら', '特に', '都心', 'の', 'ビル', '等', 'は', '厳しい', 'でしょ', 'う', 'から', '家賃', 'を', '下げ', 'て', 'でも', 'そのまま', '契約', 'を', '更新', 'し', 'た', '方', 'が', '家賃', '収入', '0', 'より', 'は', '良い', 'の', 'で', 'は', '！', '売れ残り', 'を', '残す', 'より', 'セール', 'で', '売りさばく', 'の', 'と', '同じ', 'では', '？', '特に', '家賃', '収入', 'は', '商品', 'で', '言え', 'ば', '1', '個', 'だけ', '仕入れ', 'て', '複数', 'の', '人', 'に', '販売', 'する', 'の', 'と', '同じ', 'です', 'よ', 'ね', '！', 'これから', 'は', '大家さん', 'も', '欲', 'は', '欠か', 'ない', '！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '浅草', 'は', '江戸時代', 'から', '歌舞伎', 'や', '大道芸人', 'を', '隔離', 'し', 'た', '猿若町', 'も', 'ある', 'し', '。', 'DEEP', 'で', '面白い', '歴史', 'の', '下町', '。', 'たけし', 'の', '足立区', 'や', '墨田区', 'も', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '大手', '事務所', 'も', '「', 'コロナ禍', '」', 'で', 'は', '、', '「', '観客動員', '」', 'の', 'イベント', 'が', '出来', 'ない', '。', '経営', 'の', '負担', 'と', 'なる', '都心', 'の', 'びる', 'の', '固定資産税', 'を', '考える', 'と', '売却', 'も', '正解', 'だろ', 'う', '。', 'ネット配信', 'で', '有れ', 'ば', '「', '撮影', '資材', 'と', '場所', '」', 'は', '日本中', 'に', '有る', '。', '今後', 'は', '各社', 'ネット', 'による', '営業', '展開', 'に', 'も', '力', 'が', '入る', 'だろ', 'う', '。', '総務省', 'も', 'ＮＨＫ', 'が', '使用', 'し', 'て', 'いる', '電波', 'を', 'この', '様', 'な', '業態', 'に', '入札', 'さ', 'せ', 'て', 'は', 'どう', 'だろ', 'う', 'か', '？', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エイベックス', 'は', '完全', 'に', '音楽配信サービス', 'を', '軽視', 'し', 'て', '、', 'その', '波', 'に', '乗り遅れ', 'た', 'ツケ', 'が', '回っ', 'て', '来', 'たって', '感じ', 'か', 'な', '？', '何処', 'の', 'レコード会社', 'も', 'ＣＤ', 'や', 'ＤＶＤ', 'を', '売っ', 'て', 'は', '、', 'それ', 'を', '資金', 'に', 'ライブ', 'で', '儲ける', '手法', 'は', '、', 'この', 'コロナ禍', 'で', '完璧', 'な', 'まで', 'に', '壊れ', 'た', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '都内', 'から', '河口湖', '付近', 'に', '通勤', 'する', 'の', 'は', '無理', 'と', '思う', '社員', 'が', '多い', 'でしょ', 'う', '。', '出勤', 'し', 'て', 'から', '都内', 'に', '出張', 'する', 'なんて', '・', '・', '・', '・', '。', '発案', '者', 'は', '１', '週間', '、', '都内', 'から', '河口湖', '近辺', 'に', '通勤', '体験', 'し', 'たら', '良い', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '青山', 'あたり', 'に', '本社', 'を', '構える', '会社', 'は', '今後', 'は', '蒲田', 'や', '赤', '羽', 'や', '錦糸町', 'や', '小岩', 'に', '移転', 'し', 'て', 'いき', 'そう', 'で', 'ある', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '事務所', 'の', 'コスト', 'カット', 'による', '移転', 'や', '事業', '縮小', 'による', 'タレント', 'や', 'スタッフ', 'の', 'リストラ', 'が', '発生', 'し', 'て', 'しまっ', 'た', '時点', 'で', '結局', '耐え', 'られ', 'なかっ', 'た', 'こと', 'に', 'なる', 'から', '仕方', 'ない', 'で', 'は', '済まさ', 'れ', 'ない', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '河口湖', 'いい', 'やん', '！', '昼休み', 'に', 'は', '釣り', 'が', '出来る', 'タレント', 'も', 'マネージャー', 'も', 'オフィス', 'に', '出勤', 'する', '意味', 'は', 'ない', 'から', 'いいんじゃない', 'まぁ', '河口湖', 'は', '極端', 'だ', 'けど', '千葉', '・', '神奈川', '・', '埼玉', 'なんて', '安い', '土地', 'や', '不動産', 'いくら', 'で', 'も', 'ある', 'から', 'その', '辺', 'で', 'も', 'いいんじゃない', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '完全', '所有', '権', 'から', '区分所有権', 'に', 'し', 'て', '区分', 'を', '売却', '、', '持分', '割合', 'に', 'し', 'たら', 'コツコツ', '売却', '資金', '手', 'に', '出来', 'た', 'のに', 'な', '。', 'まぁ', '金融機関', 'が', 'ウン', 'と', 'は', '言わ', 'なかっ', 'た', 'か', '。', 'アイデア', 'が', 'なかっ', 'た', 'か', '。', '。', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '同じ', '移転', 'なら', '、', '三島', 'の', '方', 'が', 'よかっ', 'た', 'の', 'で', 'は', '。', '三島', 'なら', '、', '東京駅', 'まで', 'こだま', 'で', 'も', '１', '時間', 'あれ', 'ば', '着く', '近', 'さ', 'な', 'のに', '・', '・', '・', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '芸能事務所', 'が', '一般', 'に', '目', 'に', '付き', 'やすい', 'から', '記事', 'に', 'さ', 'れ', 'てる', 'だけ', 'で', '、', '普通に', 'BtoB', '企業', 'で', 'も', '脱', '都心', 'の', '案件', 'は', '数多い', 'だろ', 'う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '青山', 'に', '本社', 'を', '構える', 'エイベックス', '、', '今後', 'は', '都内', 'の', '山手線', 'の', '外側', 'の', 'エリア', 'な', 'の', 'か', '、', '埼玉県', 'の', '和光市', 'や', '朝霞', '、', '志木', '、', '川越', '、', '森林公園', '、', '小川町', '、', '寄居', '、', '児玉', '、', 'そして', '群馬県', 'の', '高崎', 'に', '本社', 'を', '移転', 'し', 'て', 'き', 'そう', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '浅草', 'も', '土地', '代', 'は', '安く', 'ない', 'が', '・', '・', '・', '朝日新聞出版', 'も', '、', '郊外', 'に', '移転', 'すれ', 'ば', 'いい', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '芸能人', 'にとって', 'は', '、', '演劇', 'や', 'ライブ', 'は', '明日', 'の', '食い扶持', 'を', '得る', 'ため', 'の', '生きる', '為', 'の', '必要', 'な', '行動', 'だ', '。', '決して', '不要不急', 'なんか', 'で', 'は', 'ない', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '昔', '、', 'この', 'ビル', 'の', '1階', 'で', '無料', 'で', 'インターネット', 'を', 'さ', 'せ', 'て', 'いただき', 'まし', 'た', '。', '感謝', 'し', 'て', 'い', 'ます', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'レプロ', 'も', '目黒', 'から', 'お茶の水', 'に', '引っ越す', 'よ', 'ね', '。', 'お茶の水', 'は', '出版社', 'が', '多い', '神保町', 'に', 'も', '近い', 'し', '、', '局', 'も', 'お台場', '以外', 'は', '意外と', '近い', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '世田谷区', 'や', '大田区', 'は', '空港', 'も', '近い', 'し', '東名', 'も', '近い', 'ぜ', '大田区', 'は', '高速', '乗っ', 'たら', '都内', 'の', '東側', 'で', 'も', '早く', '着く', 'オススメ', 'だ', 'ぜ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '真相', 'は', '実態', 'を', '伴わ', 'ない', '株', '高', 'と', '近い', '将来', 'の', '不動産', '価格', 'の', '下落', 'を', '見越し', 'て', 'だろ', 'う', '。', 'お金持ち', 'の', '嗅覚', 'は', '鋭い', 'よ', '。', '[SEP]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキストのデータ :\n",
      " ['[CLS]', '「', 'ザ・ドリフターズ', '、', 'ビートたけし', '、', '渥美清', '、', '萩本欽一', '、', '美空ひばり', '、', '浅香光代', '、', '淡谷のり子', '、', '村田英雄', '、', '伊東四朗', '、', '水の江瀧子', '、', '沢村貞子', '、', '永六輔', 'ら', '・', '・', '・', '」', 'ビートたけし', '→', 'ツー・ビート', '萩本欽一', '→', 'コント55号', '伊東四朗', '→', 'てんぷくトリオ', 'と', '書い', 'て', '欲しかっ', 'た', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '芸能事務所', 'だけ', 'の', '問題', 'じゃ', 'なく', '、', '事務所', 'そのもの', 'の', '需要', 'が', 'コロナ', '後', '考える', '会社', '多く', '出', 'て', 'くる', '。', 'コロナ', 'の', '感染', 'どこ', 'で', '終息', 'なるか', 'だ', 'な', '。', '1', '極', '集中', 'が', '悪', 'だ', 'わ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '浅草', 'に', '来', 'ない', 'と', '歴史', 'が', '変わる', '！', '葛飾郡', 'の', '歴史', '埋立地', 'は', 'ない', '時代', 'を', '話せ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '相撲部屋', 'も', '今', 'じゃ', '両国', '辺り', 'だけ', 'じゃ', 'なく', '西新井', '、', '松戸', 'に', 'も', 'ある', 'から', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'テレビ', 'に', '出す', 'より', 'Youtube', 'を', '頑張っ', 'た', 'ほう', 'が', '初期', '投資', 'を', '安く', '抑え', 'れる', 'もの', 'な', '。', '撮影', 'も', '映画', 'も', 'テレビ', 'で', 'わざわざ', '流す', '必要', 'が', 'ない', 'YouTube', 'の', '方', 'が', '遥か', 'に', '優れ', 'て', 'いる', '媒体', 'だ', 'と', '思う', '。', '今', 'の', 'テレビ局', 'って', '人', 'による', '技術', 'とか', 'が', '優位', '性', 'を', '保て', 'て', 'いる', 'だけ', 'で', '、', '白紙', 'から', '何', 'か', 'を', '作る', 'こと', 'に', 'は', '全く', '長け', 'て', 'い', 'ない', '。', 'YouTube', 'から', '新しい', 'チャンネル', 'が', '出来', 'て', 'いく', 'こと', 'を', '期待', 'し', 'て', 'い', 'ます', '。', '先行', '例', 'は', 'いくらでも', 'あり', 'ます', 'し', '芸人', 'さん', 'にとって', 'は', '本当に', '0', 'では', 'なく', '1', 'から', 'の', 'チャンス', 'だ', 'と', '思い', 'ます', 'よ', 'ね', '。', '撮影', 'できれ', 'ば', 'なん', 'でも', '出来る', 'わけ', 'です', 'から', '。', '賢', 'い人', 'は', 'それ', 'を', '規制', 'しよ', 'う', 'と', '考える', 'でしょ', 'う', 'が', '、', 'そもそも', '危ない', 'もの', 'は', 'YouTube', 'で', '自動', '削除', 'かかり', 'ます', 'から', 'ご', '安心', 'を', '。', 'テレビ', 'の', '既存', 'の', '考え方', 'は', '終わり', 'に', '向かう', 'という', 'こと', 'は', 'もはや', '既定路線', 'です', 'ので', '、', '今', 'まで', '他人', 'の', '不幸', 'で', '食っ', 'て', 'き', 'た', 'マスコミ', 'の', '罪深', 'さ', '、', 'これ', 'は', '発信', '者', 'が', 'とことん', '自家中毒', 'として', '味わっ', 'て', '頂き', 'たい', 'と', '思い', 'ます', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '河口湖', 'なんて', '有り得', 'ない', '。', 'また', '、', '一等地', 'から', '浅草', 'へ', 'って', '、', '浅草', 'も', '安く', '見', 'られ', 'た', 'もん', 'だ', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ファン', 'が', 'ささえ', 'て', 'くれる', 'アーティスト', 'は', 'がん', 'が', 'れる', 'と', '思い', 'ます', '。', 'JUJU', '長渕', 'さだまさし', 'ゆず', '中島みゆき', 'サザン', '矢沢', '長渕', 'ゴスペラーズ', 'ユーミン', 'いきものがかり', 'miyavi', 'オレンジ', 'グリーン', '佐野', 'とか', '思いつく', '。', 'みんな', 'avex', 'じゃ', 'ない', 'の', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '移転', 'し', 'て', 'も', 'エイベックス', 'は', '持た', 'ない', 'だろ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '芸能人', 'も', '水商売', 'だ', 'から', 'ね', '。', '元々', '価値', 'の', '無い', 'もの', 'に', '人為', '的', 'に', '価値', 'を', 'つけ', 'て', 'いる', 'から', '落ちる', '時', 'は', '早い', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'レプロ', 'も', '目黒', 'から', '御茶ノ水', 'へ', '移転', 'する', 'し', 'ね', '。', 'まあ', '事務所', 'は', '山手線', 'の', '西側', 'が', 'おおい', 'よ', 'ね', '〜', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'まあ', '港区', 'とか', '目黒区', 'とか', 'じゃなくて', 'も', '、', '都内', 'で', '家賃', 'が', '安い', 'ところ', 'なら', 'いいんじゃない', '。', '河口湖', 'って', '…', '研修', 'なら', 'いい', 'が', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'TBS', 'の', '緑山', 'や', 'カンテレ', 'の', 'レモンスタジオ', '＆', '東京メディアシティ', 'に', '近く', '、', '比較的', '撮影', 'の', '多い', '京王線', 'に', 'も', '近い', '狛江', 'か', '多摩センター', '、', '町田', 'あたり', 'が', '良', 'さ', 'そう', 'だ', 'な', '。', '後', '、', '東京', 'に', 'も', '近い', '新横浜', 'か', '浦安', '、', '幕張', 'あたり', 'も', '。', '千葉', 'なら', 'ディズニーランド', 'や', '成田空港', 'が', 'ある', 'し', '、', 'この', 'ところ', '北総線', 'や', '東葉高速', 'で', 'の', '撮影', 'も', '多く', 'なっ', 'て', 'いる', 'から', '最適', 'かも', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'avex', 'って', 'レコード会社', 'でしょ', '？', '事務所', 'で', 'も', 'ある', 'ん', 'だろ', 'う', 'けど', '‥', 'まぁ', '会長', 'が', '薬物疑惑', 'ある', '会社', 'なんて', '、', 'あぁ', 'なる', 'よ', 'ね', '。', '笑える', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'うち', 'の', '会社', 'も', '移転', 'し', 'まし', 'た', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '逮捕', 'は', 'いつ', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '薄っぺら', 'い記', '事', 'だ', 'なあ', '。', '「', '聞き', 'まし', 'た', '」', 'とか', '「', '某', '事務所', '」', 'とか', '内容', '皆無', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '色々', 'もう', '終わり', 'だ', 'な', '、', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '不要不急', 'だ', 'から', 'な', '（笑）', '…', '都心', '一', '等地', 'なんて', '会社', 'が', 'あっ', 'て', 'いい', '気', 'に', 'なっ', 'てる', 'けど', 'それで', 'ケチケチ', 'し', 'て', '利益', 'だけ', '伸ばし', 'てる', 'から', 'な', '…', 'バカバカしい', '！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '河口湖', '？', 'ホンマ', 'かいな', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '調布', 'あたり', 'が', 'いいんじゃない', '？', '石原軍団', 'の', '事務所', 'って', '調布', 'だ', 'よ', 'ね', '映画', 'の', '撮影', '所', 'とか', 'スタジオ', 'とか', '色々', '近い', 'し', '芸能人', '結構', '住ん', 'でる', 'よ', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '大', '歓', '迎', '！', 'どんどん', '地方', 'へ', '移転', 'を', 'お願い', 'いたし', 'ます', '。', '企業', 'も', '地方', '出身', '者', 'も', '都内', 'で', 'は', 'なく', '生まれ', 'た', '地域', 'で', '学び', '就職', 'し', '起業', 'し', '事業', '活動', 'し', 'て', 'ください', '！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '2010年', 'ごろ', '、', 'この', 'ビル', 'に', '入っ', 'てる', '企業', 'に', '行く', '為', 'に', 'エレベーター', 'に', '乗っ', 'た', '時', 'に', 'avex', '社員', 'らしき', '女', 'の', '人', 'が', '薄手', 'の', '服', 'に', '巨乳', 'を', '露出', 'し', 'まくり', 'で', '入っ', 'て', 'き', 'た', '事', 'を', '思い出し', 'ます', '。', '仕事', '終わり', 'に', '、', 'そのまま', 'クラブ', 'に', '行く', 'つもり', 'だっ', 'た', 'ん', 'だろ', 'う', 'な', 'と', '。', 'イケイケ', 'の', '時代', 'の', '名残', 'でし', 'た', 'が', '、', 'あの', '時', 'は', '良かっ', 'た', 'という', '事', 'でしょ', 'う', 'か', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '浅草', 'は', '都心', 'の', '一等地', 'じゃ', 'ない', 'の', 'か', 'ｗ', '年寄り', 'に', 'は', '人気', 'ある', 'けど', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '仕事', '上', 'は', 'そんな', '一等地', 'じゃなくて', 'も', '全然', '支障', 'ない', 'から', 'ね', '。', '当然', 'だろ', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '都心', 'の', '一等地', 'に', '事務所', 'なんて', '見栄', '以外', 'の', '何者', 'で', 'も', 'ない', 'から', 'な', '。', '結果', 'エーベックス', 'みたい', 'に', 'なる', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '本当', 'ただ', 'の', '妄想', 'コント', 'で', 'しか', 'ない', '記事', 'だ', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '浅草', 'を', 'バカ', 'に', 'し', 'てる', 'の', 'か', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '日本', 'の', 'ショービジネス', 'の', '人間', 'は', 'アマチュア', '。', '辞め', 'た', '方', 'が', '良い', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '足立区', 'で', 'よい', 'の', 'で', 'は', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '八王子', 'あたり', 'で', 'よく', 'ない', '？', '笑', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '大手', 'も', 'リストラ', '状態', 'で', '飲食', 'も', 'エンタメ', 'も', '厳しい', 'し', '、', '税収', '下がっ', 'たら', '生活保護', 'も', '手一杯', 'で', '無理', '。', '若者', 'は', '納税', '増え', 'て', '年金', 'なし', '。', 'これ', 'で', '死ぬ', 'な', 'なんて', '無理ゲー', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エイベックス', 'に', '限ら', 'ず', '早期退職', '希望退職', 'の', 'ニュース', 'が', '多く', '出る', 'が', '、', '応募', 'し', 'て', '辞め', 'て', 'いく', '人たち', 'は', 'どこ', 'へ', '行く', 'の', 'だろ', 'う', 'か', '。', '今', 'の', 'この', '時代', '、', 'よほど', 'の', 'キャリア', '・', '経験', 'と', '人脈', 'が', 'あっ', 'て', 'も', '超', '厳しい', 'の', 'に', '不思議', 'で', 'なら', 'ない', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ここ', '近年', 'の', 'avex', '自体', '、', '衰退', 'の', 'イメージ', 'しか', 'あり', 'ませ', 'ん', 'でし', 'た', 'し', 'ね', '。', '小室', '哲也', 'の', '時代', 'や', '、', '濱崎', 'あゆみ', '頃', 'まで', 'は', '勢い', 'を', '感じ', 'まし', 'た', 'が', '、', '以降', '、', 'AAA', 'など', '看板', 'アーティスト', 'も', 'いる', '中', '、', 'ここ', '最近', 'は', 'avex', '自体', '、', '昔', 'の', '様', 'に', '名前', 'を', '聞く', '事', 'も', '減り', 'まし', 'た', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '儲け', 'た', '感', 'が', '半端ない', '会社', 'な', 'のに', '早期退職', 'の', '募集', 'って', 'やっぱり', '入る', 'の', 'も', '大きい', 'けど', '出', 'て', 'いく', 'の', 'も', '大きい', 'ん', 'だろ', 'う', 'ね', '一般人', 'に', 'は', 'わから', 'ない', '見栄', 'とか', 'も', '必要', 'そうだ', 'し', '儲け', 'てる', 'よう', 'に', '見せ', 'ない', 'と', 'ダメ', 'な', 'よう', 'な', '気', 'が', 'する', '会社', '正直', '驚い', 'た', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '希望退職', 'って', '、', 'だいたい', '40歳', '以上', 'って', 'なる', 'けど', '、', '確か', 'に', '給料', 'は', '高い', 'ん', 'だろ', 'う', 'けど', '、', 'それなり', 'に', 'ノウハウ', 'と', '労働意欲', 'が', '一番', 'ある', '年齢', 'だ', 'から', 'ね', '～', '。', '果たして', '、', '若い', '（', '将来性', '）', 'だけ', 'で', '会社', 'は', '機能', 'する', 'の', 'か', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ライブ', 'イベント', 'CD', '等', '、', 'エイベックス', 'を', '支え', 'た', '事業', 'が', '次', 'から', '次', 'へ', 'と', '低迷', 'し', 'た', '結果', '。', '当然', 'っ', 'ちゃ', '当然', 'です', 'ね', '。', '去年', 'くらい', 'から', '売り出し', '中', 'の', '安斉かれん', 'も', '鳴かず飛ばず', '・', '・', '・', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ', 'も', 'ある', 'が', '、', 'もう', '世の中', 'は', '、', 'YouTube', 'の', '時代', 'に', 'なっ', 'た', 'の', 'かも', '知れ', 'ない', '誰', 'も', 'が', '、', 'スター', 'に', 'なれる', '時代', 'だ', 'わざわざ', '興行', '会社', 'に', '、', '所属', 'する', '必要', '等', '何', 'に', 'も', '無い', '自分', 'で', '、', '音楽', 'を', '作り', '、', '自分', 'で', '配信', '出来る', '何', 'で', '、', 'わざわざ', '興行', '会社', 'に', '入る', '理由', 'が', 'あろう', '今', 'は', '、', 'プロダクション', '等', '、', 'すっ', '飛ばし', 'て', '、', '世の中', 'へ', 'デビュー', '出来る', 'の', 'だ', 'よ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'JASRAC', 'の', 'せい', 'だろ', '。', '街', 'から', '音楽', 'が', '消え', 'たら', 'その', '時代', 'の', '音楽', 'も', '印象', 'に', '残る', 'こと', 'も', 'ない', 'し', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'アーティスト', 'は', '切ら', 'ない', 'の', 'か', 'な', '？', '高給', '取り', 'が', 'いる', 'と', '思う', 'けど', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'まずは', '、', '役員', 'が', '先陣', 'を', '切っ', 'て', '報酬', 'カット', 'し', 'ましょ', 'う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '飲食業', '、', 'イベント', '業', '、', '観光業', 'など', 'これから', 'まだまだ', '失業者', 'が', '出', 'て', 'き', 'ます', '。', '裏', 'を', '返せ', 'ば', '転職', '希望者', 'は', '早め', 'に', '転職', '先', 'を', '決め', 'ない', 'と', '職', 'すら', '選べ', 'なく', 'なり', 'ます', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ＡＮＡ', '、', 'ＪＡＬ', '、', '日立金属', '、', 'ロイヤル', '、', '三菱ケミカル', '、', '三菱自動車', '、', '武田薬品', '、', 'ワールド', '、', 'シチズン', '、', 'ＬＩＸＩＬ', '、', 'ワタベウェディング', '、', 'アツギ', '、', 'ファミマ', '…', '…', '希望退職', 'の', '嵐', 'が', '吹き荒れ', 'て', 'い', 'ます', 'が', '、', 'エイベックス', 'も', 'です', 'か', '。', 'コロナ', 'で', 'イベント', '開け', 'ませ', 'ん', 'し', '…', '代表取締役会長', 'さん', 'が', '３月', 'に', '「', 'コロナ', 'で', '会社', 'が', '何', '個', 'か', 'つぶれ', 'ます', 'よ', '」', 'と', '話し', 'て', 'い', 'ます', 'が', '…', '…', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'avex', 'が', 'どう', 'こうっ', 'て', 'より', 'も', '、', 'あらゆる', '業種', 'が', '追い込ま', 'れ', 'て', 'て', '、', 'コロナ', 'の', '影響', 'が', '前代未聞', 'レベル', 'で', 'すごい', 'こと', 'に', 'なっ', 'てる', 'ん', 'だ', 'なぁ', 'と', '。', '自分', 'も', 'もし', '何', 'か', 'あっ', 'た', '時', 'の', '為', 'に', '、', '収入', '源', 'を', '一つ', 'の', '場', 'に', '頼る', 'の', 'で', 'は', 'なく', '、', '複数', '持っ', 'て', 'おき', 'たい', 'なぁ', 'と', '考える', 'よう', 'に', 'なっ', 'た', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '松浦', '会長', '一択', 'でしょ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エイベックス', 'は', '上場企業', 'だ', 'から', '目立つ', 'が', '、', '他', 'の', '音楽', '・', 'エンタメ', '、', 'メディア', '企業', 'は', '壊滅', '的', '。', 'エイベックス', 'は', '先行', 'し', 'て', '、', 'ライヴ', 'や', '物販', 'など', 'の', 'マネジメント', '事業', '、', '配信', '他', '、', 'そして', 'ジャンル', 'も', '多岐', 'に', '展開', 'し', 'て', 'いる', 'が', '、', 'それでも', '厳しい', '状況', '。', 'CD', '販売', 'に', '依存', 'し', 'て', 'いる', 'レコード会社', 'や', 'CDショップ', 'は', '、', '残念', 'ながら', 'さらに', '厳しく', '閉店', 'など', 'も', '相次い', 'で', 'いる', 'の', 'が', '現実', '。', '先', 'が', '不透明', 'な', '中', '、', '出来る', '人材', 'ほど', '、', '斜陽産業', 'に', 'しがみつか', 'ない', 'で', '転職', 'す', 'べき', 'だろ', 'う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'また', '、', 'こんな', '時代', 'が', '来', 'て', 'しまっ', 'た', 'という', '感じ', 'だ', '。', 'バブル景気', '崩壊', '後', 'に', '私', 'が', '勤める', '会社', 'で', 'も', '希望退職', 'が', 'あっ', 'た', '。', '内々', 'に', 'は', '各', '部署', 'から', '何人', 'だ', 'せとか', 'の', '指示', 'が', 'あり', '、', 'まぁ', '出来', 'が', '悪い', 'とか', '上司', 'が', '気に入ら', 'ない', '者', 'に', 'は', '希望退職', 'に', '応募', 'する', 'よう', 'に', '面接', 'が', 'あっ', 'た', 'よう', 'だっ', 'た', '。', 'そして', '応募', '結果', 'は', '会社', 'の', '思惑', 'と', '異なり', '、', '本来', '留まっ', 'っ', 'て', '欲しい', '優秀', 'な', '方々', 'の', '応募', 'が', '圧倒的', 'に', '多く', '辞め', 'て', 'いっ', 'た', '。', '辞め', 'た', '優秀', 'な', '方々', 'が', '仕事', 'の', 'キーマン', 'だっ', 'た', 'ので', '、', '結局', '、', '会社', 'は', 'その', '方々', 'へ', '特別', '退職', '手当', 'も', '払い', '、', 'その', '方々', 'が', '作っ', 'た', '会社', '、', 'または', '派遣社員', 'として', '仕事', 'を', '依頼', 'し', '高い', '金', 'を', '払っ', 'た', 'よう', 'だ', '。', 'エイベックス', 'も', '優秀', 'な', '方', 'が', '辞め', 'て', '独立', 'する', 'か', '、', '同業他社', 'に', '行っ', 'て', 'しまう', 'かも', 'ね', '。', 'やはり', '会社', 'は', '社員', 'を', '大切', 'に', 'せん', 'と', 'いかん', 'という', 'こと', 'だ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '希望退職', '募集', 'は', '終わり', 'の', '始まり', '社内', 'に', '蓄積', 'さ', 'れ', 'た', '数値', '化', 'し', 'づらい', 'ノウハウ', 'や', 'リレーション', 'は', '失わ', 'れ', 'て', 'しまう', 'よく', '筋肉質', 'に', 'なる', 'とかいう', 'が', '、', 'ガン', 'で', '内蔵', '摘出', 'し', 'た', '後', 'と', '言っ', 'た', 'ほう', 'が', '正しい', 'まぁ', '潰れる', 'こと', 'は', 'ない', 'に', 'せよ', '待っ', 'て', 'いる', 'の', 'は', '長期', 'の', '低迷', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'avex', 'に', '限ら', 'ず', '、', 'クラシック', 'の', 'ミュージシャン', 'から', '太鼓', '奏者', 'まで', '、', 'エンタメ', 'そのもの', 'が', '危機的状況', '。', '逆', 'に', '食品', 'や', '生活用品', 'の', '業界', 'は', '順調', 'じゃ', 'ない', 'か', 'な', '。', '国', 'を', 'あげ', 'て', '自殺対策', 'を', 'し', 'ない', 'と', '、', '大変', 'な', 'こと', 'に', 'なり', 'そう', 'です', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'それ', '以前', 'に', '経営', '人', 'の', '給与', 'を', '減らす', 'という', '方法', 'は', 'でき', 'なかっ', 'た', 'ん', 'だろ', 'う', 'か', '、', 'それ', 'だけ', 'でも', 'かなり', '削減', 'できる', 'と', '思う', 'けど', '。', '正直', '、', 'コロナ', 'で', '本当に', '困っ', 'てる', '人', 'も', '多い', 'と', '思う', 'けど', '、', 'コロナ', 'に', '便乗', 'し', 'てる', '連中', 'も', '多い', '、', '特に', '給与', 'じゃ', 'なく', '人', 'を', '減らそ', 'う', 'っていう', '考え', 'の', '連中', 'は', '便乗', 'も', 'いる', '。', 'エンタメ', 'に関して', '言え', 'ば', '確か', 'に', 'コンサート', 'など', 'は', '収入', 'に', 'なら', 'なかっ', 'た', 'が', '、', '音楽配信', 'など', 'の', '再生回数', 'など', 'は', 'テレワーク', 'も', 'ありか', 'なり', '増加', 'し', 'た', 'と', '聞く', '。', 'この', '企業', 'に', '至っ', 'て', 'は', '今', 'まで', 'が', 'どんぶり勘定', 'で', '単純', 'に', '経営', '努力', 'が', '足り', 'なかっ', 'た', 'ん', 'じゃ', 'ない', 'の', '、', 'と', '思う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '良い', 'とき', 'も', 'あれ', 'ば', '、', '悪い', '時', 'が', 'ある', 'の', 'は', '必然', '。', '悪く', 'なっ', 'た', 'とき', '助け', 'て', 'くれる', '存在', 'が', 'ある', 'か', '？', 'それ', 'は', '良い', 'とき', 'の', '行い', '次第', 'でしょ', 'う', 'ね', '。', '企業', 'も', '個人', 'も', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '音楽業界', 'は', '、', 'CD', 'が', '売れ', 'なく', 'なっ', 'た', '時代', 'に', '、', '何処', 'で', '稼ぐ', 'か', '？', 'と', '言っ', 'たら', 'コンサート', 'の', '集客', 'と', 'グッズ販売', 'だろ', 'う', 'から', 'コロナ', 'で', '会場', '抑え', 'た', 'けど', 'コンサート', '開催', '出来', 'ない', 'って', '事', 'で', 'チケット', '払い戻し', 'と', 'グッズ', 'が', '売れ', 'ない', 'って', '最悪', 'な', '状況', 'だっ', 'た', 'から', '、', 'この', '決断', 'は', '仕方', 'ない', 'の', 'かも', 'ね', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '中小企業', 'に', '勤務', 'し', 'て', 'いる', 'もの', 'です', 'が', '、', 'エイベックス', 'に', '限ら', 'ず', '、', '取引先', 'の', '大企業', '・', '中小零細企業', 'の', 'リストラ', 'の', '話', 'も', 'よく', '聞き', 'ます', '。', 'YAHoo', 'の', '記事', 'に', '掲載', 'さ', 'れ', 'て', 'いる', '以上', 'に', '世間', 'で', 'は', 'もっと', '深刻', 'な', 'ん', 'でしょ', 'う', 'ね', '。', '私', 'の', '勤め先', 'は', '１２月', 'の', '歳暮', '商戦', 'の', '動向', '次第', 'で', 'は', 'リストラ', 'する', '可能性', 'も', '高い', 'です', '。', 'もう', '他人ごと', 'で', 'は', 'あり', 'ませ', 'ん', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '昔', 'の', 'カラオケ', 'ブーム', 'の', '時', 'の', 'シングルCD', 'バカ売れ', '期', 'が', '嘘', 'みたい', 'です', 'ね', '。', '今', 'は', 'メイン', 'の', '購買', '層', 'だっ', 'た', '中高生', 'や', 'OL', 'さん', 'の', '限ら', 'れ', 'た', '小遣い', 'を', '、', '暴利', 'を', '貪る', '携帯', 'に', '食わ', 'れ', '、', '配信', '系', 'や', 'サブスク', 'の', '急激', 'な', '進化', 'で', 'CD', 'の', '売上', 'に', '見切り', 'を', '付け', '、', '売上', 'の', '柱', 'を', 'ライブ', 'に', 'し', 'て', '行く', 'はず', 'だっ', 'た', '時代', 'の', 'コロナ', '騒ぎ', '。', '。', '。', 'ホログラム', 'で', '密', 'に', 'なら', 'ない', '集まり', '方', 'で', 'ライブ', 'を', '演', 'る', 'ニュース', 'も', '流れ', 'て', 'い', 'まし', 'た', 'が', '、', 'まだまだ', '実証', '段階', 'という', '感じ', 'で', '即', '対応', 'できる', '策', 'で', 'も', '無', 'さ', 'そう', 'です', '。', '今', 'は', 'とにかく', '早く', 'ワクチン', 'と', '特効薬', 'が', '望ま', 'れ', 'ます', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '少し', '前', 'に', 'は', 'めちゃくちゃ', '残業', 'が', 'ある', 'ブラック企業', '扱い', 'の', '報道', 'に対し', '、', '経営', 'サイド', 'が', '「', 'エンタメ', 'が', '好き', 'で', '入っ', 'て', 'きた', '人', 'ばかり', '、', '嫌々', '無理やり', 'やらさ', 'れ', 'て', 'いる', '残業', 'で', 'は', 'ない', '」', 'という', 'ニュアンス', 'の', 'コメント', 'を', '出し', 'て', 'い', 'た', '。', 'そんなに', '仕事', 'が', 'あっ', 'た', '会社', 'が', '追い込ま', 'れ', 'て', 'いる', '。', '人件費', 'が', '高い', '方', 'から', '退場', '頂き', 'た', 'いって', '事', 'や', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '政府', 'の', '無策', 'ぶり', 'に', 'は', '呆れる', 'ばかり', '。', '。', '。', 'こんなに', '経済', 'が', '瀕死状態', 'に', 'も', '関わら', 'ず', '、', 'やる', '事', 'は', 'と', '言え', 'ば', '、', '携帯', '会社', 'に', '料金', 'の', '値下げ', '要請', 'や', '事務', '処理', 'で', 'の', 'ハンコ', '廃止', '、', '学術', '学会員', '6', '名', 'の', '任命', '拒否', '等', '等', '。', 'どれ', 'も', '、', '小さい', '事', 'ばかり', '。', 'この', '未曾有', 'の', 'コロナ', 'による', '大恐慌', 'に', '、', '政府', 'の', 'やる', 'べき', '事', 'は', '、', '思い切っ', 'た', '財政', '拡大', 'で', 'は', '。', '。', '。', 'どの', '外国', 'で', 'も', '、', 'しっかり', '財政', '拡大', 'し', 'て', 'いる', 'でしょ', 'う', '？', '日本', 'は', '、', 'このまま', 'で', 'は', '、', 'コロナ', 'が', '収束', 'し', 'た', '後', 'も', '、', '供給', '能力', 'が', '失わ', 'れ', 'て', 'しまっ', 'て', 'いる', '状態', 'で', 'は', '、', '元', 'に', '戻る', 'まで', '10年', '以上', 'も', 'かかっ', 'て', 'しまう', '。', '竹中平蔵', 'や', '後金', '損', '（', 'アトキンソン', '）', '、', '高橋洋一', '、', '三浦', '瑠璃', '等', 'の', '言う', '事', 'を', '聞い', 'てる', '菅総理', '。', 'どうにか', 'なら', 'ない', 'もの', 'か', '。', '。', '。', '日本', 'は', 'ディープステート', '、', 'CSIS', 'の', 'コントロール', '下', 'に', 'あり', '、', 'もう', '破滅', 'の', '道', 'に', '突き進む', 'しか', 'ない', 'の', 'か', '。', '。', '。', '一刻', 'も', '早く', '一人', 'で', 'も', '多く', 'の', '国民', 'が', '真実', 'を', '知り', '、', '声', 'を', '上げ', 'なけれ', 'ば', 'なら', 'ない', 'と', '強く', '思う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ', 'が', 'たとえ', '終焉', 'し', 'た', 'として', 'も', '、', '今後', 'も', '世界経済', 'に', '打撃', 'を', '与える', 'よう', 'な', 'アクシデント', 'は', '起きる', 'と', '思う', '。', '上場企業', 'に', '入社', 'できれ', 'ば', '安泰', 'という', '時代', 'は', 'もう', '終わり', 'な', 'の', 'かも', 'しれ', 'ない', '。', 'これから', 'は', '、', '会社', 'の', '肩書', 'に', 'とらわれる', 'の', 'で', 'は', 'なく', '、', 'こういう', '事態', 'が', '起こっ', 'た', '時', 'に', '、', '冷静', 'に', '自分', 'を', '分析', 'し', 'て', '、', '周囲', 'を', '俯瞰', 'し', 'て', '最良', 'の', '選択', 'を', '獲', 'れる', 'よう', 'な', '判断', '力', 'と', '新しい', '環境', 'でも', 'やっ', 'て', 'いこ', 'う', 'という', 'タフネス', 'さ', '、', 'チャレンジ', 'しよ', 'う', 'という', '開拓', '心', '、', '行動力', 'が', '大切', 'な', '時代', 'な', 'の', 'かも', '。', 'そういう', 'の', 'が', '、', '本質', '的', 'な', '人間力', 'な', 'の', 'かも', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'いろいろ', 'な', '業界', 'に', 'しのびよる', 'コロナ', 'いまさら', 'ながら', 'コロナ', '発祥', 'の', '地武', '漢', 'で', '違う', '対応', 'を', 'し', 'て', 'いれ', 'ば', 'これだけ', '世界', 'の', '人', 'は', '苦しま', 'なかっ', 'た', 'だろ', 'う', '中国', 'の', '責任', 'は', '非常', 'に', '重い', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'CD', 'じゃなくて', '、', 'ライブ', 'で', '音楽', 'は', '稼ぐ', '時代', '。', 'ライブ', 'を', 'やら', 'なきゃ', 'チケット', 'は', '売れ', 'ない', '。', 'ライブ', 'が', 'ない', 'なら', '、', 'グッズ', 'も', '売れ', 'ない', 'し', '、', 'ファンクラブ', 'も', '辞める', '。', 'ライブ', 'が', 'ない', 'なら', '、', '新しい', 'DVD', 'も', '作れ', 'ない', '。', 'これ', 'は', '、', '来年', 'も', 'たぶん', '続く', 'と', '思う', '。', 'やれ', 'て', 'も', '、', '満員', 'で', 'は', '難しい', '。', 'ライブハウス', 'が', 'コロナ', 'の', '初め', 'に', '問題', 'に', 'なっ', 'て', 'た', 'が', '、', 'いよいよ', '大手', 'も', '苦しく', 'なっ', 'て', 'き', 'た', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'カタログ', 'に', '厚み', 'が', 'ない', '。', 'ヒット曲', 'は', 'あっ', 'て', 'も', '経年劣化', 'の', '激しい', '作品', 'ばかり', '。', 'コロナ禍', 'の', '状況', 'で', 'も', 'ロングセラー', 'が', 'あれ', 'ば', '下支え', 'と', 'なる', 'が', '、', '新譜', 'の', '売り上げ', '頼み', 'の', '経営', 'で', 'は', '旧作', 'から', 'の', '売り上げ', 'も', '見込め', 'ない', '。', 'ライヴ', 'も', '音', 'を', 'いじっ', 'て', '誤魔化し', 'て', 'いる', 'し', '、', '動員', '数', 'も', '誤魔化し', '。', '売り上げ', 'に', '至っ', 'て', 'は', '社員', 'が', 'ＣＤ', 'の', '発売日', 'に', '買い', 'まわる', 'の', 'も', '日常', 'で', '、', '売り上げ', '枚数', 'に', 'も', '厚化粧', 'を', 'する', '。', 'それでも', '、', '２', '０', '２', '０', '年', 'の', '今', 'に', '至る', 'まで', '希望退職', '者', 'ゼロ', 'だっ', 'た', 'の', 'だ', 'から', '、', 'むしろ', '、', 'この', 'やり方', 'で', '、', 'よく', 'ぞ', 'ここ', 'まで', '耐え', 'た', 'と', '驚嘆', 'する', '。', 'ただし', '、', '今後', 'も', '第', '２', '、', '第', '３', '弾', 'の', '早期退職', 'の', '募集', 'が', '出る', 'と', '予想', 'する', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'まだまだ', 'これから', 'です', 'ね', '。', '今', 'は', 'しがみつい', 'た', '方', 'が', '正解', 'かも', 'しれ', 'ませ', 'ん', 'ね', '。', '一時的', 'に', 'でも', '消費税', 'を', '無くし', 'て', '、', '消費', 'を', '促さ', 'ない', 'と', '企業', '持た', 'なく', 'なり', 'ます', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'また', '希望退職', 'か', '。', '辞め', 'たく', 'ない', 'と', 'ハッキリ', '言っ', 'て', 'いる', 'のに', '、', '退職届', 'に', 'ハンコ', '突く', 'まで', '何度', 'も', '執拗', 'に', '呼び出さ', 'れ', '、', '今', 'まで', 'の', '仕事', 'は', '取り上げ', 'られ', 'て', '、', 'あなた', 'の', '仕事', 'は', 'あり', 'ませ', 'ん', 'と', '言わ', 'れる', '。', 'どこ', 'が', '希望退職', 'な', 'の', 'だろ', 'う', 'か', '？', '本人', 'が', '希望', 'し', 'て', 'い', 'ない', 'のに', '退職', 'を', '強要', 'する', 'の', 'は', '、', '「', '希望', 'さ', 'せ', 'られ', '退職', '」', 'か', '「', '退職', '脅迫', '」', 'だろ', 'う', '。', 'マスコミ', 'は', 'どうして', 'それ', 'を', 'いつまでも', '「', '希望退職', '」', 'と', '報道', 'する', 'の', 'だろ', 'う', 'か', '？', '広告', '料', 'の', '絡み', 'とか', '何', 'か', '圧力', 'が', 'かかっ', 'て', 'いる', 'の', 'だろ', 'う', 'か', '。', '[SEP]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキストのデータ :\n",
      " ['[CLS]', 'もう', '時代', '的', 'に', '終身雇用', 'は', '無理', 'だろ', 'う', '。', 'グローバル', '、', 'ダイバーシティ', '、', '競争力', 'も', '求め', 'られる', '時代', '。', '終身雇用', 'が', '無理', 'なら', 'もっと', '働き', '方', 'が', '色々', 'あっ', 'て', 'も', 'いい', 'と', '思う', '。', '完全', '副業', '解禁', 'や', '８', '時間', 'の', '縛りつけ', 'じゃ', 'なく', '個人', '個人', 'で', '時間', '等', 'の', '契約', 'を', '結ん', 'で', '柔軟', 'な', '働き', '方', 'が', 'できる', 'よう', 'に', 'する', 'と', 'か', '。', '終身雇用', 'が', '難しい', 'って', '言う', 'だけ', 'で', 'は', '経営者', '失格', '。', '何かしら', '新しい', '働き', '方', 'を', '提案', 'し', 'ない', 'と', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '此処', 'も', '、', '松浦', 'さん', 'の', 'Yes', 'man', 'しか', '居', 'ない', 'の', 'が', '、', '滑落', 'の', '要因', 'の', '一つ', 'で', 'は', '。', '配信', 'LIVE', 'とか', '、', '苦しい', '中', 'でも', 'アイディア', '絞り', '出し', 'て', 'やっ', 'てる', 'とこ', 'は', 'やっ', 'て', 'ます', 'よ', 'ね', '。', '不測', 'の', '事態', 'と', 'は', '言え', '、', '先見の明', 'が', 'な', 'さ', '過ぎる', '気', 'が', 'し', 'ます', '。', '音楽', 'バブル期', 'を', '忘れ', 'られ', 'ず', 'に', 'いる', 'と', 'いう', 'か', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ソニー', 'は', '音楽', '事業', '（', 'ソシャゲ', '）', 'が', '好調', 'で', '増収増益', 'の', 'よう', 'だ', 'から', '、', 'これから', 'は', '多角化', '、', 'グローバル化', 'し', 'て', 'ない', '企業', 'は', '淘汰', 'さ', 'れる', 'だろ', 'う', 'ね', '。', '個人的', 'に', 'は', '、', 'エイベックス', 'は', '体力', 'も', 'ない', 'のに', '訳', 'の', 'わから', 'ない', 'ベンチャー', '事業', 'に', '投資', 'し', '過ぎ', 'だ', 'と', '思い', 'ます', 'が', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '私', 'が', '勤める', '会社', 'も', '斜陽産業', '。', '過去', '何', '度', 'か', 'あっ', 'た', '男性', '総合', '職', 'へ', 'の', '希望退職', 'の', '状況', 'を', '見', 'て', 'き', 'まし', 'た', 'が', '、', '結局', '手', 'を', '上げ', 'て', '退職', 'できる', '人', 'は', '次', 'の', '見込み', 'の', 'ある', 'よく', 'できる', '人', 'ばかり', 'でし', 'た', '。', 'ここ', '5年', 'くらい', '希望退職', '制度', 'は', 'なくなり', '、', 'でき', 'ない', '方', 'の', '定年', 'を', 'ジリジリ', '待っ', 'てる', '感じ', 'です', '。', 'それ', 'まで', '会社', 'の', '体力', 'が', '持つ', 'か', 'どう', 'か', '。', '根比べ', 'です', '。', '一度', '採用', 'し', 'て', 'しまっ', 'たら', '中', '々', '首', 'を', '切れ', 'ない', '日本', 'の', '雇用', '制度', 'は', '問題', 'あり', 'です', '。', 'だから', '企業', 'も', '正社員', '採用', 'を', '躊躇', 'し', '、', '派遣社員', 'も', '増え', 'て', '行く', 'の', 'だ', 'と', '思い', 'ます', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '解雇', '対象者', 'として', '非正規労働者', 'を', '思い浮かべる', '人', 'は', '多い', 'と', '思い', 'ます', 'が', '、', 'それ', 'を', '心配', 'する', 'の', 'は', '主', 'に', '非正規社員', 'が', '多い', '職場', 'です', 'よ', '。', '多く', 'の', '経営者', 'は', 'バブル期', 'に', '大量', 'に', '採用', 'し', 'た', '正社員', 'を', '減らし', 'たい', 'ん', 'です', '。', '優秀', 'な', '人', 'は', '別', 'として', '、', '男女', '問わ', 'ず', '働か', 'ない', 'アラフォー', 'が', '多い', 'から', 'です', '。', '契約期間', 'が', '決まっ', 'て', 'いる', '非正規', 'で', 'は', 'なく', '、', 'この', '世代', 'の', '正社員', 'を', '切り', 'たい', 'の', 'です', '。', 'いい', '機会', 'が', '来', 'た', 'と', 'ニヤリ', 'と', 'し', 'て', 'いる', '企業', 'は', '多い', 'と', '思い', 'ます', 'よ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '大手', 'まで', '希望退職', '出', 'てる', '状況', 'は', '恐怖', 'を', '覚える', '。', '特に', '就職活動', 'や', '転職', '目指し', 'てる', '人', 'に', 'は', '悲報', 'で', 'しか', '無い', '。', 'もちろん', 'こんな', '状況', 'に', '陥っ', 'て', 'き', 'てる', 'の', 'は', '日本', 'だけ', 'に', '限っ', 'た', 'こと', 'で', 'は', '無い', 'が', '、', '若者', 'にとって', 'どんどん', '企業', 'が', '縮小', 'し', 'て', 'いく', '様', 'を', '見る', 'の', 'は', '、', '将来', 'が', '潰さ', 'れ', 'て', 'いく', '気持ち', 'に', 'も', 'なる', 'し', '、', '精神的', 'に', '辛い', 'と', '思う', '。', '周り', 'の', '大人', 'の', 'ケア', 'や', 'サポート', 'が', '必要', 'だ', 'と', '思う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '最近', 'は', 'かつて', 'の', '勢い', 'が', 'なくなっ', 'た', '感じ', 'が', 'する', 'が', '40代', '以上', 'の', '人', 'の', '責任', 'な', 'の', 'か', 'ね', '典型', '的', 'な', '上意下達', 'の', '会社', 'みたい', 'に', '見える', 'が', '、', '今', 'まで', 'さんざん', '尽くし', 'て', 'き', 'て', '上層部', 'に', '退職', 'を', '促さ', 'れる', 'ほう', 'は', 'やり切れない', '気持ち', 'だろ', 'う', 'な', '。', 'でも', '上層部', 'が', 'このまま', '残る', 'よう', 'で', 'は', '徐々に', '衰退', 'し', 'て', 'いく', 'よう', 'に', 'も', '見える', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '今', '、', '会社', '資料', '見', 'て', 'み', 'た', 'けど', 'この', '3年', 'くらい', '数字', 'は', '良く', 'ない', 'し', '、', 'そんなに', '利益', 'ある', 'わけ', 'で', 'も', 'ない', 'のに', '役員報酬', 'が', '異常', 'に', '高い', '、', '会長', 'は', 'オーナー', 'だ', 'として', 'も', '、', '社長', 'やもう', '1人', 'の', '取締役', 'は', '何', 'の', '結果', 'を', '出し', 'た', '方', 'なのか', '？', 'この', '経営', '者', 'の', '方々', 'は', 'まず', '希望退職', '出す', '前', 'に', '自分たち', 'の', '報酬', 'カット', 'す', 'べき', 'で', 'は', '？', 'しかも', '報道', 'は', '対象', '社員', 'が', '400人', 'くらい', 'だっ', 'た', 'が', '、', '実は', '全', '社員', 'は', '1500人', 'くらい', 'いる', 'から', '、', '今後', '考える', 'と', 'まだまだ', '経営', 'は', '改善', 'さ', 'れ', 'そう', 'に', 'ない', '気配', '、', 'ヒット', 'コンテンツ', 'を', '自力', 'で', '作れ', 'ない', '会社', 'に', 'なっ', 'て', 'しまっ', 'た', 'ん', 'だ', 'な', '、', '残念', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '音楽業界', 'も', '厳しい', 'コロナ', '前', 'から', 'とっく', 'に', 'CD', 'が', '売れ', 'なく', 'なる', '中', '創業', '30年', 'が', '経過', 'し', 'て', '小室', '時代', 'から', 'あゆ', '時代', 'LDH', '時代', 'と', 'なっ', 'て', 'も', 'アーティスト', 'が', '育た', 'ない', '時代', 'に', 'なっ', 'た', '力', 'ある', 'プロデューサー', 'で', 'も', '同じ', '戦略', 'で', 'は', '飽き', 'られ', 'て', '当然', '30年', 'と', 'なる', 'と', '初期', 'の', '新卒', 'で', '50', 'に', 'なる', 'なる', 'から', 'どうしても', '差し掛かっ', 'て', 'き', 'て', 'いる', 'と', '思う', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '震災', 'や', '豪雨災害', 'は', '国', '全体', 'から', '見れ', 'ば', '、', '特定', 'の', '限ら', 'れ', 'た', 'エリア', 'の', '話', '。', 'しかし', '、', 'コロナ', 'は', '国', '全体', 'に', '影響', 'し', 'て', 'いる', '。', 'さらに', 'ライブ', 'や', '演劇', 'は', '集客', 'の', '見込める', '大都市', '周辺', 'で', '営業', 'でき', 'ない', 'と', 'キツイ', '。', 'Uber', 'eats', 'や', '出前館', 'も', '需要', 'と', '供給', 'が', 'ある', '地域', 'で', 'ない', 'と', '成り立た', 'ない', '。', '駅前', 'で', '客待ち', 'し', 'て', 'いる', 'タクシー', 'の', 'よう', 'に', '注文', '待ち', 'し', 'て', 'いる', '配達員', 'も', '多く', '見かける', 'よう', 'に', 'なっ', 'た', '。', '仕事', 'も', '収入', 'も', '減れ', 'ば', 'デリバリー', 'を', '頼む', 'の', 'も', '減る', '。', '退職', 'し', 'た', 'ひとは', 'どんな', '仕事', 'に', '転職', 'する', 'の', 'か', '？', 'you', 'tubuer', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エンタメ', '業界', 'に', 'は', '潰し', 'が', '効か', 'ない', '人', 'が', 'いっぱい', 'いる', '。', '４', '０', '過ぎ', 'て', '全く', '新しい', '仕事', 'に', '就く', 'なんて', '気の毒', 'と', 'しか', '言え', 'ない', '。', '音楽', 'や', '芝居', 'の', '世界', 'に', 'は', '夜', 'も', '眠れ', 'ない', '人', 'が', 'いっぱい', 'いる', 'ん', 'だろ', 'う', 'な', '。', 'そして', 'エンタメ', 'の', '衰退', 'は', '私たち', 'の', '心', 'を', 'カサカサ', 'に', 'し', 'て', 'しまう', '。', '汗', 'が', '飛ん', 'で', 'くる', 'よう', 'な', '最前列', 'で', 'パフォーマンス', 'に', '熱狂', 'し', 'て', 'い', 'た', '頃', 'が', 'すでに', '懐かしい', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'どこ', 'の', '業界', 'も', 'そう', 'だ', 'けど', '、', '経営者', 'は', '儲かっ', 'て', 'いる', '時', 'に', '、', 'イザ', '景気', '悪く', 'なっ', 'た', '時', 'の', '雇用', '維持', 'の', 'ため', 'とか', '言っ', 'て', '内部留保', 'を', 'し', 'て', 'き', 'た', '。', 'ところが', '、', 'イザ', '景気', '悪く', 'なる', 'と', '、', '下っ端', 'の', '従業員', 'の', 'クビ', 'を', '切る', '。', '全く', '人間', '扱', 'し', 'て', 'ない', 'ね', '。', 'これ', 'じゃ', '従業員', 'の', '力', 'を', '発揮', '出来る', '様', 'な', '環境', 'に', 'は', 'なら', 'ない', '。', '経営者', 'と', '管理職', 'に', 'は', '、', '経営', '責任', 'と', '言う', 'もの', 'を', '考え', 'て', '欲しい', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'どこ', 'の', '会社', 'も', '青息吐息', '、', '固定比率', 'を', '削減', 'し', 'て', '経営資源', 'を', '削ぎ', '落とさ', 'なけれ', 'ば', '体力', 'が', '持た', 'ない', '。', '給料', 'の', '高額', 'な', '高齢', '者', 'から', '人員削減', 'する', 'しか', 'ない', '。', '60歳', '定年', 'の', 'ところ', 'を', '雇用', '延長', 'し', 'て', '65歳', 'まで', '面倒', 'を', '見る', 'こと', 'は', '無理', 'に', 'なっ', 'て', 'き', 'た', '。', 'それどころか', '45歳', 'とか', '早期退職', '優遇', '制度', 'など', 'の', '策', 'を', '講じる', 'の', 'が', '一般的', 'だろ', 'う', '。', '菅', 'の', '国民', 'に', '要求', 'する', '「', '自助', '」', 'が', '追い', '討ち', 'を', 'かけ', 'て', '来る', 'だろ', 'う', '。', '恐ろしい', '時代', 'に', 'なっ', 'た', 'もの', 'だ', '、', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'もう', 'かなり', '前', 'から', '優秀', 'な', '人', 'が', '内部', 'に', 'い', 'ない', '。', '会社', 'って', '、', '実際', 'は', 'ほとんど', '上層部', 'の', '手腕', '次第', 'な', 'ん', 'だ', 'よ', 'ね', '。', '大小', '関係', 'なく', '。', '上層部', 'の', 'パワーゲーム', 'が', 'もう', '通用', 'し', 'なく', 'なっ', 'た', '。', '要は', '、', '時代の流れ', 'に', '化けの皮', 'を', '剥がさ', 'れ', 'た', 'って', '事', 'な', 'ん', 'だろ', 'う', 'ね', '。', 'ある', '種', 'の', '自然の摂理', 'だ', 'ね', '。', '奢', 'れる', 'もの', 'は', '久', 'しから', 'ず', '、', 'ただ', '春', 'の', '世', 'の', '夢', 'の', '如し', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'まあ', 'どこ', 'の', '会社', 'も', '希望退職', '募集', 'し', 'だす', 'と', '会社', 'の', 'ノウハウ', 'や', '技術', '継承', 'が', 'でき', 'なく', 'なっ', 'て', '倒産', 'の', 'パターン', 'に', 'なら', 'ない', 'よう', 'に', '。', '経営者', 'に', 'も', 'よる', 'だろ', 'う', 'けど', 'うち', 'の', '会社', 'は', 'これ', 'が', '始まり', 'で', '倒産', 'し', 'まし', 'た', '。', 'まあ', '先', 'に', '見切り', 'を', 'つけ', 'て', '退職', 'し', 'た', 'けど', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '俗に', '言う', '「', '新しい', '生活様式', '」', 'が', '今後', 'も', '長期間', '続く', '場合', '、', '生活様式', 'の', '変化', 'によって', '生まれる', '需要', 'が', 'あれ', 'ば', '、', '当然', '不要', 'に', 'なる', 'もの', 'も', 'ある', '。', '確か', 'に', 'ライブ', 'など', 'の', 'イベント', '系', 'は', 'コロナ禍', 'の', '影響', 'を', 'モロ', 'に', '受け', 'た', 'と', '思う', 'が', '、', 'それ', 'によって', 'ネット配信', 'の', '需要', 'が', '一気に', '高まっ', 'た', 'の', 'も', '事実', '。', '音楽CD', 'に関して', 'も', '、', 'と', 'いう', 'より', '、', 'もはや', '読み込み', '専用', 'メディア', '全般的', 'に', '、', 'これ', 'も', 'ダウンロード', '配信', 'によって', '過去', 'の', 'もの', 'に', 'なり', 'つつ', 'ある', '。', '音楽', 'や', '映像', 'を', '楽しむ', 'ため', 'の', '機材', 'は', 'これから', 'も', '必要', 'だ', 'が', 'CD', 'の', '様', 'に', '都度', '「', 'モノ', '」', 'を', '買う', '、', 'という', '時代', 'は', 'もう', '終わる', 'の', 'だ', 'と', '思う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ', '事情', 'そのもの', 'に', '拠る', '損害', 'の', '程', 'は', '、', 'ただ', 'ひたすらに', '同情', 'の', '念', 'や', '先行き', 'を', '懸念', 'する', 'に', '尽きる', 'が', '…', '。', 'それ', 'と', 'は', '全く', '別', 'に', 'し', 'て', '。', 'エイベックス', 'は', '以前', 'から', '「', '手', 'を', '広げ', '過ぎ', '」', 'で', '、', 'その', '弊害', 'が', '各所', 'に', '及ん', 'で', 'いる', 'と', '強く', '懸念', 'し', 'て', 'い', 'た', 'ため', 'に', '、', 'この', '縮小', '事情', 'について', '、', '被雇用者', '個人', 'に対して', 'は', '甚だ', '同情', 'しか', 'ない', '一方', '、', '企業側', 'に対して', 'は', 'これ', 'を', '機', 'に', '「', '催事', '分野', 'は', '以前', 'の', 'よう', 'に', '専門', '業', 'へ', '委託', '依拠', '」', 'し', '、', '元々', 'の', 'レーベル', 'プロモーション', '業', 'に', '専念', 'す', 'べき', 'だろ', 'う', '。', '催事', '業', 'は', '全国', '津々浦々', '、', '各種', '事情', '背景', 'の', '異', 'なり', 'を', '基', 'に', '、', '地元', 'で', 'あれ', 'ば', 'こそ', 'の', '市場', '動向', 'を', '肌', '感覚', 'で', '掴み', '、', 'あるいは', '先読み', 'を', 'し', 'ながら', 'プロモーション', '展開', 'を', '図っ', 'て', 'いる', 'の', 'で', 'あっ', 'て', '、', '東京', 'の', '中央', 'に', '拠る', '定形', '手法', 'で', '一律', 'に', '統制', '、', '制御', 'する', 'こと', 'で', 'すべからく', '音楽', '市場', 'が', '健全', 'に', '形成', 'さ', 'れる', 'もの', 'で', 'は', 'ない', '。', 'その', 'こと', 'を', '認識', 'する', 'ため', '、', '逆', 'に', 'コロナ', '事情', 'を', '活用', 'する', 'ぐらい', 'じゃ', 'ない', 'と', 'いけ', 'ない', 'だろ', 'う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '厳しい', 'です', 'ね', '。', '大', 'リストラ', '時代', 'に', '突入', 'し', '、', '来年', 'は', 'もっと', '厳しく', 'なり', 'そう', '…', '嗅覚', 'が', '利く', '者', 'から', '抜け', 'て', '次に', '行く', '。', 'いつ', '何時', '何', 'が', 'あっ', 'て', 'も', 'おかしく', 'ない', '、', 'と', '常に', '心得', 'て', '準備', 'する', 'しか', 'ない', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '対象者', 'の', '2割', '強', 'が', '対象', 'する', 'と', 'は', '思え', 'ない', 'から', 'これ', '、', 'リストラ', 'も', '視野', 'に', '入れ', 'て', 'そう', '。', '安室奈美恵', 'みたい', 'な', 'アーティスト', 'でも', '現れ', 'ない', '限り', 'は', '、', '厳し', 'そう', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '理由', 'は', '3点', 'まず', '日本', 'の', '音楽シーン', 'が', 'CD', 'や', 'ダウンロード販売', 'が', '収益', 'の', '中心', 'と', 'なる', '構造', 'から', '脱却', 'し', '、', 'サブスクリプション', 'で', 'の', '収益', '構造', 'に', '移行', 'でき', 'て', 'い', 'ない', '。', 'これ', 'は', 'エイベックス', 'に', '限っ', 'た', '事', 'で', 'は', 'ない', '。', '2点', '目', '。', 'LIVE', 'の', '収益', 'の', '減少', '。', 'コロナ', 'が', '原因', 'で', 'LIVE', 'の', '売上', 'が', '100億', '以上', 'マイナス', 'と', '言わ', 'れ', 'て', 'いる', '。', '3点', '目', '。', 'エイベックス', 'は', '、', 'ソニー', 'や', 'ユニバーサル', 'と', '言っ', 'た', '会社', 'と', '違い', '欧米', 'の', 'メジャー', 'アーティスト', 'を', '抱え', 'て', 'い', 'ない', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ禍', 'で', '真っ先', 'に', 'ダメージ', 'を', '受ける', 'の', 'が', 'エンタメ', '業界', '。', '震災', 'の', '時', 'も', 'そう', 'でし', 'た', '。', '音楽', 'は', 'もう', '厳しい', 'かも', 'しれ', 'ない', 'です', 'ね', '。', '元々', 'セールス', '枚数', 'が', '2010年', 'ごろ', 'から', 'どん底', '続き', 'で', 'しんどい', '状況', 'でし', 'た', 'から', '。', '当方', '、', 'フジ', '系', 'の', '音楽業界', 'で', '作曲', '家', 'として', '働い', 'て', 'い', 'まし', 'た', 'が', '、', 'あまり', 'の', '賃金', 'の', '低', 'さ', 'に', '転職', 'し', '、', 'ゲーム', '系', 'の', '業界', 'で', '当時', 'の', '３', '倍', 'の', '年収', 'に', 'まで', '増え', 'まし', 'た', 'よ', '。', 'やっ', 'てる', 'こと', 'は', '全く', '同じ', 'な', 'のに', '。', 'いくら', '天下', 'の', 'エイベ', '出身', 'と', 'いっ', 'て', 'も', '音楽業界', 'じゃ', 'ろくな', '転職', '先', 'も', 'ない', 'です', 'よ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'もう', '会社', 'の', 'ネームバリュー', 'とか', 'ステータス', 'とか', 'に', 'こだわる', 'の', 'は', '古い', '。', '職種', '別', 'の', 'ジョブ', '型', '雇用', 'に', '本格', '的', 'に', '切り替え', 'ない', 'と', '。', '総合職', 'なんて', '時代遅れ', 'も', '甚だしい', '。', '会社', 'なんて', '永久', 'に', '存在', 'する', 'わけ', 'じゃ', 'ない', 'ん', 'だ', 'から', '、', '自分', 'の', '専門性', 'とか', '得意分野', 'に', 'こだわっ', 'て', '仕事', 'を', '選ん', 'で', 'キャリアップ', 'しない', 'と', '駄目', 'だ', '。', 'そう', 'しない', 'と', 'こういう', '時', 'に', '対応', 'でき', 'なく', 'なる', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エイベックス', 'は', 'かつて', 'の', '勢い', '無くなっ', 'た', 'ね', '。', 'あの国', '系', 'の', '歌手', 'が', '結構', 'い', 'た', 'けど', '。', '全然', 'ヒット', 'し', 'て', 'い', 'ない', '。', 'ソニーミュージック', 'に', '大', '負け', 'し', 'てる', '。', 'ソニー', 'は', '自分たち', 'で', 'シンガー', 'を', '育てる', 'が', '、', 'エイベックス', 'は', '、', '金', 'だして', '持っ', 'て', 'くる', 'だけ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'そう', 'せ', 'ざる', 'を', '得', 'ない', 'ん', 'だろ', 'う', 'と', 'は', '思う', 'ものの', '、', 'コロナ', 'が', 'ずっと', '続く', '訳', 'で', 'も', 'なく', '今後', 'どう', 'する', 'の', 'だろ', 'う', 'と', '思う', '。', 'もともと', '景気', 'が', '凄く', '良い', 'という', '業界', 'で', 'は', 'ない', 'から', 'スタッフ', 'も', 'ギリギリ', 'だっ', 'た', 'の', 'で', 'は', 'ない', 'か', 'と', '思う', '。', 'すで', 'に', 'コロナ', 'の', '影響', 'を', '受け', 'にくい', '業種', 'で', 'は', '採用活動', 'を', '活発', 'に', 'し', 'て', 'い', 'て', '優秀', 'な', '人材', 'を', '広く', '集め', 'て', 'いる', '。', 'コロナ', 'が', '終息', 'し', 'て', '、', 'ライブ', 'や', 'イベント', 'を', 'する', 'ぞ', 'ー', '！', 'って', '時に', '人材', 'が', 'なけれ', 'ば', '呼び戻す', 'の', 'も', '難しい', '。', 'もちろん', 'それ', 'も', '分かっ', 'て', 'い', 'て', '会社', 'を', '守る', 'ため', 'の', 'こと', 'な', 'ん', 'だろ', 'う', 'けれど', '。', '切ない', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '有名', 'な', '人', 'を', 'たくさん', 'かかえ', '、', 'いま', 'まで', '、', 'そう', 'と', 'うな', '利益', 'を', '得', 'て', 'き', 'た', 'のに', '赤字', 'は', '今回', 'だけ', '？', 'な', 'の', '。', '昔', 'は', '陽水', 'とか', '、', 'サザン', 'とか', '、', 'ユーミン', 'とか', '、', 'いい', '音楽', 'が', 'たくさん', 'あっ', 'て', '、', '生き', 'てる', 'の', 'が', '楽しかっ', 'た', '。', '今', 'は', '個性', '的', 'な', '歌手', 'で', '一般人', 'に', 'うける', '人', 'が', '出', 'て', 'こ', 'ない', '。', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '経済', 'を', '回す', '為', 'に', 'コロナ', 'と', '共存', 'を', '選ん', 'だ', '世の中', 'な', 'のに', '、', '今年', 'に', '入り', '倒産', 'や', '売却', '、', '社員', '省き', 'が', '増え', 'て', 'いる', 'よう', 'に', '思え', 'て', 'しまう', 'の', 'は', 'わたし', 'だけ', 'でしょ', 'う', 'か', '。', 'コロナ感染者', 'が', '増え', 'て', 'き', 'て', '、', '経済', 'を', '回す', '為', 'コロナ', 'と', '共存', 'など', '言っ', 'て', 'られる', 'の', 'の', '前', 'は', '今', 'だけ', 'な', 'の', 'かも', 'しれ', 'ない', '。', 'この', '考え', 'に', '必ず', '限界', 'が', 'くる', 'と', '思え', 'て', 'なら', 'ない', '。', '経済', 'を', '回し', 'て', '行く', 'に', 'は', '今', 'の', 'やり方', 'で', 'は', '有名', 'な', '会社', 'まで', '消え', 'そう', '。', '手抜き', 'し', 'て', '経済', 'を', '回す', '政府', 'や', '県庁', '、', '都庁', 'に', '思える', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '退職', '後', 'の', '次', 'は', 'どう', 'なる', 'は', 'とりあえず', '置い', 'とい', 'て', '。', '希望退職', 'を', '会社', 'が', '打ち出し', 'て', 'き', 'て', 'も', '、', '会社', 'が', 'その', '人', 'に', '辞め', 'られる', 'と', '困る', '場合', 'は', '、', '「', '会社', 'が', '必要', 'と', '認める', '人材', 'は', '、', '辞める', 'と', '自己都合', '退社', 'に', 'なる', 'よ', '」', 'って', '文言', 'が', '盛り込ま', 'れ', 'て', 'おり', '、', '会社', 'の', '行く末', 'と', '自分', 'の', 'こと', 'を', '考え', 'て', '退職', 'を', '選ん', 'だ', 'として', 'も', '、', '残ら', 'なけれ', 'ば', 'いけ', 'ない', 'こと', 'も', 'あり', 'ます', '。', '自己都合', '退社', 'と', '、', '会社都合', '退社', 'で', 'は', '退職金', 'の', '額', 'が', '全然', '違い', 'ます', '。', 'いい', '会社', 'なら', '、', '残り', 'たい', 'けど', '、', 'ダメ', 'ダメ', 'な', '会社', 'で', '必要', 'な', '人材', 'と', '言わ', 'れ', 'て', 'も', 'なぁ', 'って', '感じ', 'です', 'けど', 'ね', '。', '（', '経験', '者', '）', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '生活', 'を', '豊か', 'に', 'する', 'に', 'は', '音楽', 'など', 'エンターテイメント', 'は', '必要', 'に', '違い', 'ない', 'が', '、', 'コロナ禍', 'で', 'は', '本当に', '生き', 'て', 'いく', 'の', 'に', '必要', 'な', '贅沢', 'で', 'ない', '普通', 'の', '食品', 'や', '医薬品', 'など', 'の', '業界', 'が', '勝ち組', 'と', 'なっ', 'た', '。', 'まさかの', 'こと', 'が', '一年', 'の', '間', 'に', '起こり', '得る', '。', '人員整理', 'や', '減給', 'の', 'ニュース', 'に', '明日', 'は', '我が身', 'かも', 'と', '戦々恐々', 'と', 'する', '日々', 'は', '続い', 'て', 'いく', 'だろ', 'う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '公演', 'が', 'ぜんぜん', '出来', 'なけれ', 'ばこう', 'も', 'なる', 'よ', 'ね', '。', '希望退職', '者', 'って', 'いつも', '40代', '以上', 'だ', 'けど', '正直', 'この', '年齢', 'に', 'なっ', 'て', '転職', 'って', '難しい', 'し', '年齢', '問わ', 'ず', '希望', '募れ', 'ば', 'いい', 'のに', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'イベント', '事業', 'など', '、', '集客', 'し', 'て', '収益', 'を', '得る', 'ビジネス', 'は', '、', 'どこ', 'も', '影響', '大きい', 'だろ', 'う', 'ね', '。', '経営', '視点', 'で', 'は', '、', '早期', 'に', '希望退職', '募集', 'を', '実行', 'する', '方', 'が', '、', '残る', '人たち', 'を', '救う', '施策', 'と', 'なる', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '前', 'から', '言っ', 'て', 'いる', 'けど', '、', 'どんな', '目', 'に', 'あっ', 'て', 'も', '最後', 'まで', '会社', 'に', 'しがみつく', 'べき', '。', '大企業', 'から', 'の', '40代', '転職', 'なんて', '、', 'いい', '事', 'など', '1つ', 'も', '無い', 'から', '。', '中小', 'に', '入っ', 'て', 'みれ', 'ば', 'わかる', '。', '更に', 'そこ', 'から', '非正規', 'に', '落ち', 'たら', '、', '這い', '上がる', 'の', 'は', '非常', 'に', '困難', '。', '厳しい', '現実', 'が', '待っ', 'て', 'いる', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '別に', 'avex', 'に', '限っ', 'た', '事', 'で', 'は', '無い', '。', '今', 'は', '何処', 'の', '会社', 'も', '手一杯', 'でしょ', 'う', '。', 'なら', 'ば', '、', '首切り', 'の', '前', 'に', '役員報酬', 'を', '下げる', 'なり', '、', '何かしら', '方法', 'は', '無い', 'の', 'か', '？', '所詮', 'は', '下々', 'の', '人', '。', 'という', '事', 'の', '表れ', 'で', 'しか', 'ない', 'です', 'ね', '。', 'とりあえず', '松浦氏', 'の', '報酬', 'から', '引け', 'ば', '良い', 'の', 'で', 'は', '？', '下々', '含め', '頑張っ', 'て', '上', 'が', '成り立つ', '。', '理解', '出来', 'て', 'ます', 'か', '？', '出来', 'ない', 'なら', '、', 'やはり', 'クソ', '会社', 'です', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '約', '33億円', 'の', '赤字', 'が', 'これだけ', 'の', '超', '大手企業', 'にとって', 'どれ', '程', 'の', '問題', 'な', 'の', 'か', '分からん', 'が', '、', '素人', 'ながら', 'avex', 'の', '将来', 'は', '暗い', '様', 'に', '思う', '。', '若年層', 'に', '重点', 'を', '置い', 'た', '事', 'で', 'レーベル', 'そのもの', 'も', '知名度', 'が', '上がっ', 'た', 'ものの', 'レコード', '（', 'CD', 'ね', '）', 'を', '買わ', 'なく', 'なっ', 'た', '今日', 'の', '若者', 'と', '前述', 'の', '企業', '方針', 'が', '余りに', 'も', '乖離', 'し', 'て', 'いる', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '我が', '娘', 'は', '、', '毎月', 'の', 'よう', 'に', '、', '遠く', '離れ', 'た', '東京', '迄', '、', '舞台', 'を', '観', 'に', '行き', 'ます', '。', '今月', 'も', '、', '2', '泊', '3日', 'で', '3回', '公演', 'に', '行く', 'よう', 'です', '。', 'チケット', 'に', 'は', '、', 'グッズ', 'や', 'CD', 'そして', 'フェイスシールド', 'まで', '含ま', 'れ', 'て', 'いる', 'ので', '、', 'いつも', 'より', '高価', 'だ', 'そう', 'です', '。', '芸能人', 'も', '、', '大変', 'だ', 'と', '思い', 'ます', 'が', '、', '押し売り', '感', 'が', '否め', 'ない', '？', '早く', '、', '平穏', 'な', '日々', 'が', '、', '訪れ', 'ます', 'よう', 'に', '。', '芸能人', 'の', '方々', 'も', '、', '良い', '生活', 'が', '続き', 'ます', 'よう', 'に', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'イベント', '系', 'は', '、', '運営', '、', '設営', '、', '音響', '、', '照明', '、', '運送', '等', '等', '関わる', '企業', 'が', '多く', '、', '助成金', 'が', '切れれ', 'ば', '、', '大量', 'の', '解雇', '者', 'が', '溢れ', '出', 'て', 'しまう', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '今年', '、', '流行', 'し', 'た', '曲', 'が', '思い', '浮かば', 'ない', 'の', 'は', '俺', 'だけ', '？', '動画', '全盛', 'の', '時代', '、', '欲しい', '映像', 'が', '無料', 'で', '見れ', 'て', 'しまう', '時代', '。', '会員限定', 'オンライン', 'ライブ', 'など', '、', '付加価値', 'を', 'つけ', 'て', 'いか', 'ない', 'と', '、', '生き残れ', 'ない', 'ん', 'じゃ', 'ない', 'か', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '希望退職', 'と', 'は', '聞こえ', 'は', 'いい', 'けど', '、', '俺', 'が', '勤め', 'て', 'いる', 'ところ', 'も', 'だいぶ', '前', 'に', 'あっ', 'た', 'が', '、', '実際', 'は', '指名', '解雇', 'だっ', 'た', '。', '対象外', 'だっ', 'た', 'けど', '、', 'その', '時', 'の', '雰囲気', 'は', '最悪', 'だっ', 'た', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'avex', 'に', '限ら', 'ず', 'です', 'が', '、', '近年', 'は', 'ライブ', 'や', 'それ', 'に', '付随', 'する', 'グッズ', 'の', '収益', 'が', 'アーティスト', 'の', '収入', '源', 'に', 'なっ', 'て', 'い', 'た', 'ため', 'コロナ禍', 'で', '減収', 'に', 'なる', 'の', 'は', '仕方', 'ない', 'と', '思い', 'ます', '。', '日本', 'は', 'まだ', '売れ', 'てる', '方', 'と', 'は', '言え', '、', 'CD', 'や', '配信', '、', 'サブスク', 'など', 'の', '楽曲', '収益', 'も', '、', 'avex', 'は', '近年', 'ずっと', '稼ぎ', '頭', '不在', 'と', '新人', '育成', '、', '発掘', 'が', '思う', 'よう', 'に', 'いっ', 'て', 'ない', 'ので', '致し方', 'あり', 'ませ', 'ん', 'ね', '。', '日本', 'の', '音楽業界', 'に', '置い', 'て', '一', '時代', 'を', '築い', 'た', 'レーベル', 'だけ', 'に', '何とか', '踏ん張っ', 'て', '欲しい', 'です', '。', '窮地', 'を', '救う', 'よう', 'な', '劇的', 'な', 'アーティスト', 'を', '見つけ', '育て', 'て', '欲しい', 'です', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '音楽業界', 'は', '八方塞がり', 'で', 'アルバム', '単位', 'の', '単価', 'から', 'シングル', '配信', 'へ', 'の', '単価', 'へ', 'と', '移り', 'コンサート', 'も', 'コロナ', '自粛', 'で', 'カラオケ', '配信', 'で', 'も', 'カラオケBOX', 'が', 'クラスター', 'の', '原因', 'に', 'なる', 'と', 'さ', 'れ', 'て', '人々', 'が', '遠のき', '収入', '媒体', 'そのもの', 'から', '収入', 'を', '得', 'られ', 'ない', '危機的状況', '下', 'に', '陥っ', 'て', 'いる', '。', 'コロナ', 'が', '終息', 'し', 'ない', '限り', '、', 'また', 'ワクチン', 'が', '出来', 'ない', '限り', '、', 'この', '状態', 'から', '抜け出せ', 'ない', '感じ', 'が', 'する', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'CCCD', 'とか', '自分', 'の', '利益', 'しか', '考え', 'ない', 'こと', 'を', 'やっ', 'て', 'た', '過去', 'も', 'ある', 'し', '、', '所属', 'アーティスト', 'の', '音楽', 'の', '方向性', 'を', '過度', 'に', '絞っ', 'たり', 'し', 'て', 'た', '過去', 'も', 'ある', 'し', '、', '果たして', '今', 'は', 'どう', 'な', 'ん', 'だろ', 'う', 'か', '。', 'あんまり', 'いい', '印象', 'は', 'ない', 'けど', 'ね', '[SEP]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキストのデータ :\n",
      " ['[CLS]', 'みんな', '農業', 'に', '転身', 'し', 'たら', 'いい', 'のに', '。', '食費', 'は', '自家栽培', 'の', '野菜', 'に', '200', '坪', 'の', '古民家', 'が', '500万円', 'で', '購入可能', '、', '派遣会社', 'は', '20時間', '以上', '勤務', 'だ', 'と', '厚生年金', '制度', 'に', '加入', 'に', '社会保険', 'あり', '、', '大阪', 'から', '岡山県', 'に', '移住', 'を', '７', '年', '前', 'に', 'し', 'まし', 'た', '。', '高齢者', 'に', 'は', '暮らし', 'やすい', '。', '高額', 'な', '住宅ローン', 'の', '支払い', 'が', 'ある', 'の', 'に', 'リストラ', 'だ', 'なんて', '？', '生活水準', '下げる', 'べき', '。', '古民家', 'が', '6', 'DK', 'で', '小学校', '近い', 'のに', '家賃', '1万円', 'とか', '空き家バンク', '検索', 'し', 'たら', 'あり', 'ます', 'よ', 'ね', '退職金', 'で', 'ハウス', 'トマト', '栽培', '始め', 'た', '脱サラ', '組', 'も', '居', 'ます', '。', '農業', 'は', '休憩', '4時間', 'あり', 'ます', 'よ', '。', '工場', 'ライン', '作業', 'より', 'は', '全然', 'ラク', 'で', '人間関係', 'モメ', 'ないし', '。', '終わりの時', '間', 'も', '自分', '決めて', 'いい', 'し', '。', '農業', 'は', '自営業', '、', 'お米', 'は', 'その', '年', 'から', 'カネ', 'に', 'なり', 'ます', 'トラクター', 'が', '200万', 'とか', 'JA', 'で', '無料', '貸し出し', 'し', 'て', 'ます', 'し', '耕作放棄地', 'も', '余っ', 'てる', '。', '農業', 'も', '自営業', 'です', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '売上高', 'は', '前年同期', '比', '44', '0', '％', '減', 'の', '342', '億', '7900万円', '最終', '利益', 'は', '32億', '8900万円', 'の', '赤字', '売上高', 'は', '前年同期', '比', '44', '0', '％', '減', '前年', '売上高', 'が', '500億円', 'だっ', 'た', 'として', '、', 'その', '4', '掛け', 'で', '、', '200億円', 'も', '売上高', 'が', '減っ', 'た', 'の', 'かぁ', '。', '支出', '変動費', '？', 'も', '減っ', 'た', 'と', '思わ', 'れる', 'が', '、', '誰か', '法人', '・', '個人', 'が', 'その', '支出', 'を', '受け取っ', 'て', 'い', 'た', '、', 'なに', 'が', '、', 'どの', 'よう', 'に', '減っ', 'た', '・', '影響', 'が', '出', 'た', 'の', 'だろ', 'う', '。', '200億円', 'も', '売上高', 'が', '減る', 'の', 'は', '、', 'たいへん', 'だ', '。', '44', '0', '％', '減', 'は', '、', '大変', 'だ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '今', 'の', 'テレビ', '界', 'は', '印象操作', 'ばかり', '、', '中韓', 'の', '為', 'の', '媒体', 'として', '放送', 'さ', 'れ', 'てる', '気', 'が', 'する', '。', '何にも', '魅力', 'が', 'ない', '！', 'テレビ番組', 'を', 'つける', '気', 'すら', '、', '無くなっ', 'た', 'よ', '。', '真実', 'が', '放送', 'さ', 'れ', 'ない', 'から', '見る', 'だけ', '気分', 'が', '悪い', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '時代', 'は', 'デジタル', 'や', 'ネット', 'で', 'アーティスト', 'も', 'VR', 'の', '世界', 'に', '移行', 'し', 'て', 'いく', 'の', 'か', 'な', '。', 'そのうち', 'ディスプレイ', 'に', 'は', '目', 'の', '前', 'に', 'リアル', 'な', 'アーティスト', 'が', 'い', 'て', '自分', 'だけ', 'の', '為', 'に', 'LIVE', 'し', 'て', 'いる', 'よう', 'に', '観', 'れる', '事', 'に', 'なる', 'だろ', 'う', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '次', 'は', 'コミケ', '産業', 'が', '全滅', 'か', '．', 'イベント', 'は', '五輪', 'が', 'ない', 'と', '厳し', 'そう', '．', 'その', '五輪', 'も', 'バイデン', 'が', '対中', '戦略', 'の', '一環', 'として', '東京', 'を', 'スキップ', 'さ', 'せ', 'て', '北京冬季五輪', 'から', '再開', 'とか', 'やり', 'そう', '．', 'アメリカ', 'は', '五輪', 'の', 'スポンサー', '．', 'エイベックス', 'も', '五輪', 'の', 'お仕事', 'が', '多い', 'ので', 'ちょっと', '心配', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'どの', '会社', 'も', '形', 'だけ', 'の', '仕事', 'し', 'ない', '窓際族', 'リストラ', '散々', '儲け', 'て', '稼ぎ', 'まくっ', 'て', '麻痺', 'し', 'た', '高額', 'な', '役員報酬', 'を', '限り無く', '減らす', '事', 'から', '始め', 'ない', 'と', '一生懸命', '会社', 'に', '貢献', 'し', 'て', '来', 'た', '社員', 'も', '報わ', 'れ', 'ない', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '「', '音楽', '事業', 'の', '一部', 'と', '間接', '部門', 'に', '在籍', 'する', '40歳', '以上', '」', 'の', '言い回し', 'から', '、', '間接', '部門', 'を', '主', 'に', 'リストラ', 'する', 'と', '思う', '。', '技術', '職', 'は', '残し', '、', '技術', '手', 'に', '職', 'の', 'ない', '総務', '経理', 'や', '人事', '、', '営業', 'を', 'リストラ', 'する', 'の', 'だろ', 'う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '40', '以上', 'って', '、', '働き盛り', 'だ', 'し', '、', 'ローン', 'だって', 'ある', 'だろ', 'う', 'に', '。', '。', '。', 'キツイ', 'なぁ', '。', '私ごと', 'だ', 'が', '、', '会社', 'の', '人事', 'やっ', 'てる', 'と', '、', '面接', 'で', '、', 'なぜ', '会社', 'を', '辞め', 'た', 'の', 'か', '？', 'って', '質問', 'は', '必ず', 'する', 'ん', 'だ', 'が', '、', 'コロナ', '理由', 'は', '8割', 'を', '占める', 'よ', '。', '世の中', 'の', '厳しさ', 'が', 'よく', 'わかる', '。', 'シーン', 'です', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '日本', 'の', '企業', 'は', 'どの', '業界', 'も', '赤字', '、', '赤字', 'が', '続け', 'ば', '税収', 'も', '下がる', '。', '税収', 'が', '下がれ', 'ば', '、', '国民', 'サービス', 'も', '低下', 'する', '。', '失業保険', '、', '年金', '、', '介護', '、', '福祉', '、', '健康保険', '個人', '負担', 'が', '増え', 'たり', '等', 'など', '。', '今', '、', '雇用保険', 'を', '掛け', 'てる', '人', 'も', '失業給付', '金', 'が', '減らさ', 'れ', 'たり', '、', '年金', 'も', '支給', '年齢', 'を', '遅らさ', 'れ', 'たり', '、', '支給', '額', 'が', '減らさ', 'れ', 'たり', '。', '日本', 'は', 'すでに', '、', '国債', '発行', '残高', '897', '兆', '円', '、', '国民', '1人', 'あたり', '713万円', 'の', '借金', '！', 'さらに', '「', 'GoTO', 'キャンペーン', '」', 'など', 'に', '1', '兆', '8482', '億', '円', 'を', '国債', '発行', 'し', 'て', 'ます', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '青山', 'の', '一等地', 'の', '本社', 'を', '売れ', 'ば', 'いい', 'だけ', 'な', '気', 'が', 'する', 'が', '…', '。', '希望退職', 'まで', 'し', 'て', '、', 'リストラ', 'を', '図る', '意味', 'が', '分から', 'ない', '」', '。', 'オンライン', 'に', '転換', 'し', 'て', '、', '利益', 'を', '出せれ', 'ば', 'いい', 'の', 'だ', 'から', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'むやみ', 'に', '早期退職', '者', 'を', '募集', 'する', 'と', '優秀', 'な', '人材', 'から', 'どんどん', '退職', 'し', '使え', 'ない', '人材', 'が', '残る', '。', 'これ', 'が', 'お決まり', 'の', 'パターン', 'です', '！', '早期退職', '者', 'を', '募っ', 'た', '時点', 'で', 'その', '会社', 'は', '衰退', 'し', 'て', 'いく', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ禍', 'で', 'の', '不景気', '風', 'が', '蔓延', 'し', 'て', '楽しい', '筈', 'の', '年末', '正月', 'が', '、', 'どこ', 'も', '暗い', 'もの', 'に', '成り', 'そう', 'です', '。', '早く', 'ワクチン', 'を', '完成', 'さ', 'せ', 'て', '終息宣言', 'が', '出る', 'で', 'まで', 'は', '経済', 'が', '良く', 'なら', 'ない', 'でしょ', 'う', '。', '東京オリンピック', 'も', '残念', 'だ', 'が', '無理', 'が', '有る', '気', 'が', 'し', 'ます', '。', 'とりあえず', 'は', '来年', 'いっぱい', 'は', '、', '自他', 'を', '守る', '為', 'の', '自粛', 'に', '徹する', 'しか', 'ない', 'と', '思い', 'ます', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '上場企業', 'で', 'さえ', 'この', '有り様', 'だ', '。', '中小企業', 'は', '更に', '酷い', 'はず', '。', '日本の経済', 'で', '生き延びる', 'の', 'は', 'かなり', '厳しい', '現実', '。', 'avex', 'の', '40歳', '以上', 'なんて', '、', '過去', 'に', '美味しい', '思い', 'を', '散々', 'し', 'て', 'き', 'た', '人達', 'でしょ', '？', 'この', '世の中', 'で', '生き残れる', 'の', 'か', 'ね', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'Gackt', 'が', '、', 'CD', 'が', '売れ', 'ない', '時代', 'に', 'き', 'た', 'から', '別に', 'ビジネス', '始め', 'なきゃ', 'いけ', 'ない', 'と', '話し', 'て', 'た', 'の', 'を', '思い出し', 'た', 'コロナ', 'で', 'ガラリ', 'と', 'すべて', '良い', 'ほう', 'に', '変われ', 'ば', 'いいね', '。', 'ブラック企業', 'は', '、', 'コロナ', 'で', '潰れ', 'たら', 'いい', 'し', '、', '問題', 'の', 'ある', '詐欺', '集団', 'の', '企業', 'も', '、', 'さっさと', 'この', '際', '、', '潰れ', 'たら', 'いい', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '40歳', '以上', 'という', 'の', 'が', 'ね', '。', 'やはり', 'エンタメ', '業', 'らしい', '。', 'キャリア', '重ね', 'て', 'も', '変化', 'に', 'ついて行く', 'の', 'が', 'キツイ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '40歳', '以上', 'って', 'の', 'が', 'キモ', 'ね', '。', 'これ', 'が', '35', 'とか', '30', '以上', 'だっ', 'たら', '下', 'から', 'どんどん', '辞め', 'て', 'いく', 'だろ', 'う', 'な', '。', 'この', '業界', 'の', '賃金', 'は', '上がる', 'こと', 'は', 'ない', 'し', '、', '同じ', '時間', '働く', 'なら', '他', 'に', '行っ', 'た', '方', 'が', '賢明', 'だろ', 'う', '。', '配信', 'リリース', 'メイン', 'と', 'なる', 'と', '、', '人手', 'も', '削れる', 'し', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '私', 'も', '某', '大手', 'レコ', '－', 'ド', '会社', 'を', '14年前', 'に', '57歳', 'で', '希望退職', 'で', '退職', 'し', 'まし', 'た', '。', 'その', '時', 'の', '総', '退職金', 'は', '、', '約', '5300万円', 'で', '破格', 'の', '金額', 'で', '他社', 'の', '注目', 'の', '的', 'に', 'なり', 'まし', 'た', '（', '勿論', '正確', 'な', '金額', 'など', 'は', '教え', 'て', 'い', 'ませ', 'ん', 'です', '）', '。', '今回', 'の', 'エイベックス', 'の', '金額', 'は', '解り', 'ませ', 'ん', 'が', 'それ', '以上', 'で', 'は', '無い', 'と', '思い', 'ます', '・', '・', '兎に角', 'ライバル', 'は', 'エイベックス', 'で', '浜崎', 'を', '始め', 'として', '物凄い', '勢い', 'で', '業界', 'の', 'リーダーシップ', 'に', 'のし上がり', 'まし', 'た', 'が', 'その', '時', 'に', 'もう少し', '先', 'を', '見通し', 'た', '対策', 'を', '講じ', 'て', 'おけ', 'ば', '良かっ', 'た', 'の', 'か', 'な', 'と', '思い', 'ます', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'うち', 'の', '近所', 'に', '昔', 'CD', '、', 'ビデオ', '、', '漫画', '、', 'ゲーム', 'を', '扱う', '大きな', 'お', '店', 'が', 'あっ', 'た', 'けど', 'その', '時', 'デジタル化', 'が', '進ん', 'で', 'Amazon', 'の', '発展', 'で', '影響', 'を', 'うけて', '潰れ', 'た', 'な', '音楽業界', 'は', 'グッズ', 'に', 'CD', '特典', 'で', 'やり過ごし', 'て', 'た', 'イメージ', 'しか', 'わか', 'ない', 'そこ', 'で', 'コロナ', 'の', '影響', 'と', '変わる', '時代', 'の', '変化', 'に', '飲み込ま', 'れ', 'つつ', 'ある', 'の', 'か', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '80年代', '90年代', 'に', '比べる', 'と', '明らか', 'に', 'ヒット作', 'が', '出', 'なく', 'なっ', 'た', '。', '円盤', 'ビジネス', 'も', 'ネット', 'に', '取っ', 'て', '代わら', 'れ', '成り立た', 'ない', '。', 'エンタメ', '業界', 'も', '新た', 'な', 'ビジネスモデル', 'Vtuber', 'など', 'の', 'バーチャルアイドル', 'に', '対応', '出来', 'なけれ', 'ば', '沈ん', 'で', 'いく', '一方', 'な', 'の', 'かも', '知れ', 'ない', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'これ', 'も', '時代', 'です', 'ね', '。', '過去の栄光', 'は', '凄い', 'と', '思い', 'ます', '。', '現在', 'を', '考える', 'と', '…', '苦しく', 'なっ', 'て', 'き', 'てる', 'の', 'でしょ', 'う', '。', 'どの', '企業', 'でも', '雇わ', 'れ', 'てる', '身', 'として', '！', '他人事', 'として', '聞く', 'の', 'で', 'は', 'なく', '自分自身', 'も', '考え', 'なく', 'て', 'は', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '数', 'ヶ月', '前', 'に', 'うち', 'の', '会社', 'に', 'エイベックス', 'の', 'ダンサー', 'だっ', 'た', 'という', '人', 'が', '入社', 'し', 'た', '。', '全く', '畑違い', 'の', '職種', 'な', 'ので', 'コロナ', '渦', 'なら', 'では', '現象', 'と', '思っ', 'た', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'JASRAC', 'に', '受け入れ', '要請', 'すれ', 'ば', 'いい', 'のに', '。', 'コロナ', 'の', 'せい', 'じゃ', 'ない', 'の', 'は', 'わかっ', 'て', 'ん', 'でしょ', '。', 'JASRAC', 'の', '錬金術', 'に', '全面', '協力', 'し', 'た', '結果', 'で', '自業自得', '。', 'リスナー', 'が', '気がつい', 'た', 'ん', 'だ', 'よ', '、', '搾取', 'さ', 'れ', 'て', 'い', 'た', 'こと', 'と', '、', '魅力', 'ある', 'アート', 'は', '版権', 'フリー', 'の', '中', 'に', 'も', 'ある', 'こと', 'を', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '40歳', '過ぎ', 'て', '同じ', '業界', 'へ', 'の', '転職', 'が', '難しい', '環境', 'と', '考えれ', 'ば', '、', '自身', 'の', 'これ', 'まで', 'の', 'キャリア', 'を', '捨て', 'て', 'しまう', 'こと', 'に', 'なり', '、', '希望退職', 'に', '応募', 'する', 'の', 'も', 'かなり', 'ハードル', 'が', '高い', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '戦前', 'の', '日本', 'の', '会社', 'は', '、', '転職', '当たり前', 'だっ', 'た', 'らしい', '。', '戦後', '、', 'アメリカ', 'の', '自動車産業', 'の', 'コミュニティー', 'を', '参考', 'に', 'し', 'て', '、', '終身雇用', 'とか', '。', 'それ', 'を', '日本', 'は', '農耕', '社会', 'だ', 'とか', '言っ', 'て', '、', '。', '元', 'に', '戻っ', 'た', 'だけ', 'かも', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '日本', 'の', '企業', 'の', '内部留保', 'が', '過去最大', 'という', 'ん', 'だ', 'けど', 'な', '。', '景気', 'いい', '時', 'は', '利益', 'を', '積み上げ', 'て', '景気', 'が', '悪い', '時', 'は', 'ここ', 'ぞ', 'とばかり', '従業員', 'の', '入れ替え', 'を', '行う', 'ん', 'だ', 'から', '困っ', 'た', 'もん', 'だ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ', 'の', 'せい', 'で', 'コンサート', 'や', '関連', 'イベント', 'が', '軒並み', '中止', 'だ', 'から', 'な', '。', 'おまけ', 'に', '、', 'コロナ', 'が', '鎮静', 'し', 'て', 'も', '配信', 'など', 'の', 'イベント', 'が', '定着', 'し', 'つつ', 'ある', 'から', '以前', 'の', 'よう', 'に', '集客', 'イベント', 'は', '復活', 'し', 'な', 'さ', 'そう', '。', 'エイベックス', 'も', '、', 'エンタメ', 'を', '配信', 'など', 'の', 'オンライン', '化', '進め', 'ない', 'と', '生き残れ', 'ない', 'かも', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'いや', 'はや', 'とんでも', 'ない', '時代', 'に', '突入', 'の', 'よう', 'です', 'ね', '。', 'コロナ', '特需', 'な', '企業', '除け', 'ば', 'どこ', 'も', '全滅', 'に', '近い', 'ね', '。', 'その', 'コロナ', '特需', 'の', '企業', 'なんか', 'で', 'は', 'パイ', 'は', 'たかが', '知れ', 'てる', 'し', 'この', '先', '、', '国', 'を', '引っ張っ', 'て', 'いける', 'と', 'は', '思え', 'ない', '。', '中国', 'に', '支配', 'さ', 'れる', 'の', 'も', 'そう', '長く', 'ない', '気', 'も', 'する', 'し', 'どう', 'なる', 'ニッポン', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '事業', '継続', '給付', 'で', 'は', 'なく', '、', '創業', 'や', '起業', 'の', '給付', 'を', 'し', 'た', '方', 'が', 'いい', 'と', '思う', '。', '新しい', '人', '、', '新しい', '技術', '、', '新しい', '考え方', 'で', '新た', 'に', '組成', 'さ', 'れる', '組織', 'が', 'でき', 'た', 'ほう', 'が', 'いい', 'の', 'で', 'は', 'ない', 'か', 'な', '〜', '。', '不公平', 'な', 'リストラ', 'に', '怯える', 'の', 'で', 'は', 'なく', '、', '新しく', '雇用', 'を', '生み出し', 'て', '、', 'そこ', 'に', '人', 'が', '移動', 'し', 'た', '方', 'が', '社会', '、', '経済', '、', '心理的', 'に', 'プラス', 'だ', 'と', 'おもう', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ', 'が', 'なく', 'て', 'も', '終わっ', 'てる', '会社', '。', '一番', 'の', 'お偉いさん', 'と', 'エイベックス', 'を', '大きく', 'し', 'た', '人物', 'が', 'あれ', '。', '。', 'じゃ', 'ね', '。', '。', 'CD', 'が', 'バカ', 'みたい', 'に', 'ミリオン', 'ヒット', 'する', 'ほど', '売れ', 'た', '時代', 'が', '懐かしく', '輝かしい', 'ね', '。', 'まぶし', '過ぎ', 'て', '目', 'を', '覆い', 'たく', 'なる', 'ほど', 'の', '転落', '状況', 'に', 'なっ', 'て', 'しまっ', 'た', '。', 'ミリオン', 'ヒット', '。', '。', 'この', '言葉', 'は', 'もう', '死語', 'に', 'なっ', 'て', 'しまっ', 'た', 'の', 'かも', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ウイルス', 'へ', 'の', 'コジツケ', 'っぽい', 'ね', '！', 'あれだけ', '自由', 'に', 'アーティスト', '無名', 'まで', 'も', '入れ', 'て', 'たら', 'さー', '無名', 'な', 'アーティスト', 'は', '言っ', 'て', 'い', 'た', '過保護', '利用', 'し', 'やすい', 'な', '環境', 'に', '感謝', 'みたい', 'な', '話し', 'を', '普通', 'に', '自宅', '、', '通勤時間', 'で', 'の', '娯楽', '業界', 'なら', '儲かる', 'よ', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '来年', 'は', '2021年', '以降', 'は', 'バイデン', 'に', 'なろ', 'う', 'と', 'トランプ', 'に', 'なろう', 'と', '米中戦争', '開戦', 'だ', 'から', '就職氷河期', 'が', 'どう', 'とか', '言っ', 'てる', '場合', 'で', 'は', '無い', 'サバイバル', '生活', 'が', '始まる', 'かも', 'しれ', 'ない', '台湾', '海域', 'で', '始まり', 'そう', 'だ', 'し', '米中', 'は', '貿易', 'で', 'も', '戦争', 'する', 'から', '米中', '両方', 'いい', '顔', 'し', 'て', 'た', '日本', 'の', '産業', 'は', '殆ど', '破産', '潰れる', 'だろ', 'う', '来年以降', 'は', '分断', 'サバイバル', 'が', 'キーワード', 'だ', 'avex', 'も', 'アジア', 'で', '活動', 'し', 'て', 'た', 'が', 'もう', 'ヤバい', 'かも', '中国', 'で', 'ビジネス', 'やっ', 'てる', 'ところ', 'は', 'TikTok', 'も', 'そう', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'GO', 'TO', 'エンタメ', 'みたい', 'な', 'の', 'が', '始まっ', 'て', 'も', '席', 'の', '間隔', '空け', 'ない', 'と', 'いけ', 'ない', 'し', '、', '収益性', 'は', '落ち', 'ます', 'よ', 'ね', '。', 'か', 'と', 'いっ', 'て', '現状', '密', 'に', 'なり', 'やすい', '状況', 'で', 'ライブ', 'に', '行く', 'の', 'は', '憚', 'れ', 'ます', '。', 'ライブ', 'に', '行っ', 'て', 'コロナ感染', 'し', 'た', 'って', '会社', 'に', '報告', 'でき', 'ませ', 'ん', 'もん', '。', '来年', 'の', 'オリンピック', 'を', '含め', 'て', 'スポーツ', '、', 'エンタメ', 'は', '見通し', 'が', '立た', 'ない', 'です', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'これ', 'って', '、', 'もし', '40歳', '以上', 'で', '退職', 'を', '希望', 'し', 'なかっ', 'た', '人', 'は', 'どう', 'なっ', 'て', 'しまう', 'の', 'だろ', 'う', '？', 'エンタメ', '業界', 'は', 'なに', 'も', 'avex', 'に', '限ら', 'ず', 'かなり', '厳しい', 'だろ', 'う', 'から', '、', '辞め', 'た', '後', 'に', 'どんな', '仕事', 'に', 'つける', 'かも', '先行き', '厳し', 'そう', 'です', 'よ', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'avex', 'の', '日本', 'に', 'もたらし', 'た', '功績', 'は', 'かなり', 'デカ', 'かっ', 'た', '。', 'しかし', '時代', 'も', '変わり', '、', 'いつ', '会社', 'を', '畳ん', 'で', 'も', '誰', 'も', '文句', 'は', '言わ', 'ない', 'と', '思う', '。', 'むしろ', 'ありがとう', 'ぐらい', 'に', '思う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ソニー', 'の', '独り', '勝ち', '。', '今年', '最後', 'に', 'NiziU', 'ちゃん', '、', 'BTS', 'の', 'album', 'を', '欧米', 'と', '同じく', '日本市場', 'も', 'ソニー', 'が', 'やる', 'なら', '、', '更に', '儲かる', '。', '嫌韓派', 'が', 'NiziU', 'や', 'BTS', 'を', '批判', 'する', 'けど', '、', '彼ら', 'が', '売れれ', 'ば', 'ソニー', 'が', '儲かる', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '補償', 'を', 'しろ', '、', '自粛', 'を', 'しろ', '、', '雇用', 'を', '守れ', '。', '命', 'だ', '、', '人', 'の', '迷惑', 'だ', '、', 'みんな', '我慢', 'し', 'て', 'いる', '。', 'お金', 'は', '降っ', 'て', '湧い', 'て', 'くる', 'と', 'でも', '思っ', 'て', 'いる', 'の', 'だろ', 'う', 'か', '。', 'そういう', '事', 'を', '声高', 'に', '叫ん', 'で', 'い', 'た', '輩', 'は', '。', '最初', 'は', '飲食店', 'など', 'が', '苦境', 'に', '立たさ', 'れ', 'て', 'いる', '程度', 'の', '報道', 'しか', 'さ', 'れ', 'て', 'い', 'なかっ', 'た', 'が', '、', 'この', 'よう', 'に', '大資本', 'の', '企業', 'の', '悪化', 'し', 'た', '業績', 'が', '報じ', 'られ', 'だし', '、', '危機感', 'を', 'やっと', '抱き', '始める', '人', 'が', '出', 'て', 'くる', 'の', 'か', 'と', '。', 'それでも', 'わから', 'ない', '人', 'に', 'は', 'わから', 'ない', 'の', 'だろ', 'う', 'が', '。', 'この', '状況', 'が', '続く', 'と', 'コロナ', 'で', '死ぬ', 'より', '、', '別', 'の', '理由', 'で', '死ぬ', '人々', 'が', '出', 'て', 'くる', 'という', '事', 'を', '。', 'さらに', '言う', 'と', '、', '自分', 'の', '命', 'も', '含め', 'そう', 'な', 'の', 'だ', 'が', '、', 'そんなに', '命', 'に', '価値', 'は', 'ある', 'の', 'だろ', 'う', 'か', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '以前', 'に', 'エイベックス', '取引', '業者', 'で', '働い', 'て', 'い', 'た', 'けど', '、', 'そこ', 'も', '従業員', '3分の2', 'まで', '減っ', 'てる', 'これから', 'は', '所属', 'アーティスト', 'の', '大量', 'リストラ', 'だ', 'な', 'その', '前', 'に', '自社', 'の', '窮状', 'を', 'ある程度', '知ら', 'しめ', 'たい', 'の', 'だろ', 'う', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '状況', 'の', '改善', '時期', 'の', '予想', 'も', '出来', 'ない', 'の', 'だ', 'から', '、', '会社', 'を', '守る', '為', 'に', 'は', '仕方', 'ない', '選択', 'でしょ', 'う', '。', '退職金', 'の', '加算', '額', 'が', '多い', 'こと', 'を', '祈り', 'ます', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '大手', 'や', '名', 'の', 'ある', '企業', 'の', '希望退職', '募集', 'など', 'が', '、', '日に日に', '増え', 'て', 'き', 'た', '。', '年末', 'や', '来春', '、', 'どう', 'なっ', 'て', 'いる', 'の', 'だろ', 'う', 'か', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '経済', 'より', '命', 'と', '言っ', 'て', 'いる', '人たち', 'って', '失業', 'し', 'て', 'い', 'ない', 'の', 'でしょ', 'う', 'ね', '。', '公務員', 'か', '医療', '者', 'でしょ', 'う', '、', 'この', '思想', 'は', '。', '一周', '回っ', 'て', '、', '特に', '医療', '者', 'さん', '達', 'の', '収入', 'は', '減り', 'ます', '。', 'だって', '税収', 'が', 'ない', 'もん', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ', 'の', '影響', 'は', '強い', 'の', 'か', 'と', '…', 'もともと', '会社経営', 'は', '借金', 'から', '始まる', 'ので', '…', '一昔', '前', 'は', '借金', 'ゼロ', 'の', '大手', '会社', 'あり', 'まし', 'た', 'よ', 'ねぇ', '。', 'キヤノン', 'でし', 'た', 'っけ', '。', 'コロナ', '影響', 'と', 'いう', 'より', 'は', 'もともと', '基盤', 'が', '弱かっ', 'た', 'とか', '阿呆臭い', '事', '書く', '人', 'いる', 'けど', '…', '日本', 'の', 'トップクラス', 'の', 'トヨタ', 'で', '20', '兆', 'でし', 'た', 'か', '？', 'ソフトバンク', 'は', '3', '兆', '？', 'これ', '倒産', 'し', 'て', 'も', 'コロナ', 'ない', 'です', 'か', 'ね', '…', 'コロナ', '影響', 'です', 'よ', '。', '奮起', 'し', 'て', '下さい', '。', 'エイベックス', 'ガンバ', '！', '！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'いい', 'こと', '思いつい', 'た', '。', '「', 'オレ', 'の', '趣味', 'は', '盆栽', 'だ', '」', 'と', '自己暗示', 'を', 'かけよ', 'う', '。', 'そしたら', '多少', 'エンタメ', 'が', '縮小', 'し', 'て', 'も', '心', 'の', '渇き', 'は', '抑制', 'できる', 'はず', 'だ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'パーティーピーポー', 'どころ', 'で', 'ない', 'もん', 'な', '、', 'なん', 'か', '急', 'に', '冷め', 'て', 'しまっ', 'た', 'から', '音楽イベント', 'は', '成り立た', 'ない', '。', 'コロナ', 'が', '終息', 'し', 'て', '元', 'の', '賑わい', 'に', 'なれ', 'ば', '良い', 'けど', '、', '今後', 'は', 'オンライン', 'パーティー', 'に', 'なる', 'かも', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エイベックス', 'は', 'コロナ', '前', 'より', '新しい', '売れっ子', 'を', '世に', '送りだせ', 'ず', '衰退', '傾向', 'に', 'あっ', 'た', 'から', '仕方', 'ない', '。', 'コロナ', 'による', '興行', 'の', '中止', 'と', 'AAA', 'の', '解散', '、', 'EXILE', 'の', 'アツシ', '脱退', 'といった', '主力', 'グループ', 'の', '離脱', 'で', '縮小', 'は', '避け', 'られ', 'ない', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'リストラ', '、', '希望退職', '者', '募集', '、', '別', '業種', 'へ', 'の', '社員', '数', '千人', '出向', 'など', 'コロナ', '不況', 'の', 'ニュース', 'だらけ', '。', 'Goto', 'の', 'お陰', 'で', 'コロナ感染', 'が', '拡大', '中', '。', '日銀', 'も', 'GPIF', 'も', '、', '実体経済', 'は', 'ガン無視', '状態', '。', 'そんな', '中', 'で', '年金', '資金', 'と', '、', '日銀', '資金', 'が', '派手', 'に', '注入', 'さ', 'れ', 'まくっ', 'た', '官製', '株価', 'だけ', 'が', '虚しく', '上昇', '、', 'この', 'イ', 'かれ', 'た', '現実', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ここ', 'から', '大きい', '収入', 'を', '上げ', 'て', 'きた', '人', 'が', '出資', 'し', 'て', '働い', 'て', 'いる', '人達', 'の', '生活', 'を', '支える', 'お金', 'に', '会社', 'が', '使う', '。', '音楽', 'が', '好きな人', '達', 'の', '会社', 'で', 'あり', 'コミュニティ', 'なら', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ここ', 'の', '会社', 'の', '会長', 'の', 'SNS', '見', 'てる', 'と', 'いつも', '豪勢', 'に', '遊ん', 'で', 'いる', '画像', 'や', '動画', 'ばかり', 'で', 'リストラ', 'と', 'は', '無縁', 'な', 'ん', 'だ', 'なー', 'と', '思っ', 'て', 'い', 'まし', 'た', 'が', '、', '上', 'が', '生き延びる', 'ため', 'に', '社員', 'が', '犠牲', 'に', 'なる', 'ん', 'です', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '役員報酬', 'は', '松浦', 'さん', 'で', '4億', '、', '林', 'さん', 'で', '1億', '、', '黒岩', 'さん', 'で', '1億円', 'を', 'もらっ', 'て', 'い', 'ながら', '、', 'その', '減額', 'の', 'アナウンス', 'が', 'ない', 'のに', '、', '社員', 'は', 'やめろ', 'なんて', '、', '人事部', '筆頭', 'に', 'あまりに', 'ひどい', '会社', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'こんな', 'コロナ', '渦', 'の', '中', 'で', 'デリカシー', 'の', 'ない', '事', '言う', 'けど', '、', '音楽業界', '始め', 'エンタメ', 'の', '業界', 'は', '本当', 'の', '実力者', 'が', '生き残る', 'の', 'かも', '知れ', 'ない', 'ね', 'AKB', '系', 'は', 'G', 'みたい', 'に', 'しぶとく', '生き残り', 'そう', 'だ', 'けど', '、', '空恐ろしい', '草', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'avex', 'は', '、', 'かつて', 'は', '音楽業界', 'に', '一大', '勢力', 'を', '築い', 'て', 'た', 'けど', '、', '最近', 'は', '泣かず飛ばず', 'の', 'タレント', 'ばかり', 'を', '抱え', 'て', 'いる', 'イメージ', 'でし', 'た', 'ね', '。', 'しまいに', 'は', '社長', 'と', '浜崎あゆみ', 'の', '不倫', 'すら', 'も', '売り物', 'に', 'し', 'なく', 'ちゃ', 'いけ', 'ない', 'くらい', '落ちぶれ', 'て', 'き', 'て', 'い', 'た', 'し', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'a', 'nation', 'は', '10年', '位', '行っ', 'て', 'た', 'けど', '、', 'k', 'nation', 'に', 'なっ', 'て', 'から', '行か', 'なく', 'なり', 'まし', 'た', '。', '昔', 'は', '楽しかっ', 'た', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'CD', '売れ', 'ない', '、', 'ライブ', '回れ', 'ない', '。', '今', 'の', '状態', 'が', '続け', 'ば', '有名', 'アーティスト', 'で', 'も', 'ただ', '曲', '作っ', 'てる', 'だけ', 'じゃ', '厳しく', 'なる', 'かも', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'イベント', '開催', 'が', '出来', 'なかっ', 'た', 'の', 'が', '影響', 'し', 'てる', 'と', 'いう', 'が', '、', 'ＣＤ', 'の', '売上', 'が', '下がっ', 'てる', 'の', 'も', '影響', 'し', 'てる', 'と', '思う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '給料', '下げ', 'て', 'でも', '残す', 'という', '選択肢', 'は', '無い', 'の', 'か', 'な', '？', '今年', 'は', '特別', 'な', '年', 'な', 'のに', '、', '、', '貴重', 'な', '人材', 'は', '無くす', 'と', '取り戻せ', 'ない', 'よう', 'な', '気', 'が', 'する', 'が', '、', '、', '、', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'いくら', '年金', 'や', '外貨準備', '金', 'を', '使っ', 'て', '株価', 'を', 'あげ', 'て', 'も', '現実', 'は', '飲食店', 'や', 'タクシー', 'や', '電車', 'そして', '宿泊', 'その他', 'コロナウイルス', 'で', '大きな', '被害', 'を', '受け', 'て', 'いる', '小手先', 'の', '誤魔化し', 'で', '世の中', '良く', 'なら', 'ない', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '不況', 'の', '業界', 'から', '好調', 'の', '業界', 'へ', '雇用', 'が', 'シフト', 'し', 'て', 'いく', 'ない', 'いい', 'の', 'だ', 'が', '、', 'この', 'コロナ禍', 'で', 'は', '厳しい', 'だろ', 'う', '。', 'あと', '2', '〜', '3年', 'くらい', '、', '皆', 'が', '何とか', '生き延びる', 'こと', 'が', 'できれ', 'ば', 'いい', 'が', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '昔', 'の', 'avex', 'は', 'ドル箱', 'と', 'なる', '人', 'を', '沢山', '抱え', 'て', '、', 'CD', '販売', 'など', 'も', '絶好調', 'でし', 'た', '。', 'それ', 'を', '踏まえる', 'と', '、', 'コロナ', 'の', '影響', 'だけ', 'で', 'は', 'ない', 'よう', 'に', '思い', 'ます', 'が', '・', '・', '・', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'わたし', 'の', '会社', 'も', '希望退職', '者', '募集', 'し', 'て', 'た', 'けど', '40代', '以上', 'が', '対象', 'でし', 'た', '。', '私', 'が', '40代', 'だっ', 'たら', '間違い', 'なく', '手', 'を', 'あげ', 'てる', '。', '残る', '側', 'も', 'この', '会社', 'やばいっ', 'て', '感じる', 'の', 'です', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '社員', 'が', '希望退職', 'なら', 'エイベックス', '所属', 'の', 'アーティスト', 'や', 'タレント', 'は', 'もっと', '契約', '切ら', 'れる', '人', 'が', '増える', 'だろ', 'う', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '基本', '配信', '、', 'リアル', 'イベント', '出来', 'ず', 'だ', 'と', '先々', '危ない', 'と', 'なっ', 'た', 'の', 'か', 'な', '。', '株価', 'は', '堅調', 'でも', '安心', 'でき', 'ない', '世の中', 'に', 'なっ', 'て', 'き', 'た', '。', '40代', '以上', 'なら', 'おいしい', '時代', 'も', '知っ', 'てる', '社員', 'たち', 'だろ', 'う', 'から', '辞め', 'たら', '厳しい', '現実', 'を', '知る', 'こと', 'に', 'なる', '。', 'おそらく', '年金', 'も', '今', 'の', '数字', 'を', '信用', 'し', 'てる', 'と', '大変', 'な', 'こと', 'に', 'なり', 'そう', 'だ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ネット配信', 'によって', '楽曲', '販売', 'が', '儲から', 'なく', 'なっ', 'た', 'ため', 'に', '、', 'ライブ', 'で', '観客', '動員', 'し', 'て', '儲けよ', 'う', 'と', 'し', 'たら', 'コロナ', 'だ', 'もん', 'なあ', 'ライブ', 'や', 'イベント', 'も', 'ネット', 'という', 'バーチャル', 'が', 'メイン', 'に', 'なっ', 'たら', '、', 'いよいよ', 'レコード会社', 'や', 'イベント', '会社', 'って', '無くなる', 'か', '、', 'さらに', '減少', 'する', 'ん', 'だろ', 'う', 'なあ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '芸能人', 'の', '人達', 'も', 'これから', 'は', '他', 'の', '道', 'を', '選択', 'し', 'なく', 'て', 'は', '成ら', 'なく', 'なる', 'の', 'だろ', 'う', 'か', '？', 'コロナ', 'は', '核兵器', 'より', '怖い', '気', 'が', 'する', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '4人', 'に', '1人', 'か', '、', '戦々恐々', 'と', 'する', 'なぁ', '。', 'エイベックス', 'に', 'い', 'た', 'よう', 'な', '方', 'なら', '優秀', 'だ', 'し', '、', '数少ない', '好況', 'を', '維持', 'し', 'て', 'いる', '企業', 'に', '転職', 'できる', 'ん', 'じゃ', 'ない', 'か', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '希望退職', 'に', '応募', 'する', '人', 'は', '他社', 'でも', '欲し', 'がる', '優秀', 'な', '人', 'が', '多い', 'だろ', 'う', 'し', '、', '確実', 'に', '会社', 'の', '力', 'は', '落ちる', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '芸能', 'という', '水商売', 'は', '厳しい', 'と', '思う', 'よ', '。', 'イベント', '軒並み', '中止', 'だ', 'から', 'な', '。', 'タレント', 'は', '芸', 'を', '磨き', '、', 'スタッフ', 'は', '仕掛け', 'を', '考える', '時期', 'だ', '。', '分野', 'は', '違う', 'が', '俺', 'も', '頑張る', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'クビ', 'を', '切る', '必要', 'の', 'ない', '財務', '体質', 'の', '会社', 'に', '於い', 'て', 'も', '、', '最近', 'の', '潮流', 'に', '乗っ', 'て', 'この', 'よう', 'な', '対応', 'を', '取っ', 'て', 'いる', '会社', 'も', 'ある', 'でしょ', 'う', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ここ', 'は', 'やっぱり', 'ダンス', 'と', 'ディスコ', '（', 'クラブ', '）', 'の', '会社', '今', 'は', '物販', 'で', '頑張っ', 'てる', 'みたい', 'だ', 'けど', '、', 'ウルトラジャパン', 'で', '外国人', 'DJ', 'を', 'a', 'nation', 'で', 'EXILE', 'を', '呼ん', 'で', '20代', 'の', '若者', 'が', '熱狂', 'する', 'よう', 'な', 'ムーブメント', 'を', '作り出す', '会社', '大人', 'に', 'なっ', 'た', '今', 'は', '行か', 'ない', 'けど', '、', '若者', 'に', 'は', '必要', 'だ', 'よ', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エンタメ', '系', 'は', 'コロナ', 'によって', '時期', 'が', '早まっ', 'た', 'だけ', 'で', '、', '将来的', 'に', 'は', '厳しい', '業界', 'です', 'よ', 'ね', '。', 'ＶＲ', 'が', '進化', 'し', 'たら', '更に', '巣籠', 'が', '進み', 'そう', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '韓国', 'の', '音楽業界', 'は', '、', '逆', 'に', 'YouTube', 'を', '上手く', '利用', 'し', 'て', 'い', 'ます', '。', '一つ', 'の', 'グループ', 'の', '動画', 'が', '１', '日', 'に', '50本', '以上', '出', 'て', 'ます', '。', '一般', 'の', '方', 'の', 'アップ', 'し', 'た', '動画', 'が', 'ほとんど', 'です', 'が', '。', '日本', 'の', '場合', '、', 'JASRAC', 'が', '音楽業界', 'を', '衰退', 'さ', 'せ', 'て', 'い', 'ます', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'どんな', '会社', 'で', 'も', '終身雇用', 'は', '無理', 'だ', 'よ', '。', '不可能', 'で', 'は', 'ない', 'と', '言い', 'きれる', '会社', 'は', '新しい', 'こと', 'に', '取り組める', 'ところ', 'だ', 'な', '。', '時代', 'に', 'ついていけ', 'ない', '会社', 'は', '運', '任せ', '。', '[SEP]']\n",
      "テキストのデータ :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ['[CLS]', 'あの', '贅沢三昧', 'の', '社長', '？', '違っ', 'た', 'か', 'な', '？', 'いい', '時', 'って', 'いつまでも', '続か', 'ない', 'ねぇ', '。', '豆柴の大群', 'が', 'エーベックス', 'に', '入っ', 'たって', 'やたら', '喜んで', 'た', 'が', '今', 'は', '大丈夫', 'か', '？', '緊急', 'ミーティング', '始動', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'avex', 'は', '沢尻エリカ', 'が', '薬', 'が', '理由', 'で', '前', 'の', '会社', 'を', '解雇', 'さ', 'れ', 'た', 'に', 'も', 'かかわら', 'ず', 'それ', 'を', '知り', 'ながら', '、', '当時', 'の', '松浦', '社長', 'が', '沢尻エリカ', 'と', '契約', 'し', 'た', '。', 'まぁ', '正直', '、', 'まとも', 'な', '会社', 'で', 'は', 'ない', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'でも', '株価', 'は', 'ドンドン', '上がる', '。', '公的資金', 'が', '入っ', 'て', 'から', '、', '株価', 'は', '経済指標', 'で', 'は', 'なくなっ', 'た', 'なぁ', '。', 'まあ', '下がり', 'さえ', 'し', 'なけれ', 'ば', '、', '誰', 'も', '損', 'は', 'し', 'ない', 'けど', '。', '下がり', 'さえ', 'し', 'なけれ', 'ば', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'やっぱり', '生', 'ライブ', 'し', 'て', 'グッズ', '等', 'で', '収益', 'あげ', 'ない', 'と', '厳しい', 'と', '思い', 'ます', '。', 'CD', 'など', '売り上げ', 'も', '今', 'は', '難しい', 'よ', 'ね', '買っ', 'て', '握手', 'とか', '特権', 'が', 'なか', 'か', 'ゃ', '買わ', 'ない', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '【', '間違っ', 'て', '浜崎あゆみ', 'が', '希望', '退職', 'し', 'たり', 'し', 'て', '・', '・', '・', '】', 'いや', '、', '無い', 'な', '在籍', 'し', 'て', 'いる', '事', 'が', '今', '唯一', 'の', '救い', 'だ', 'から', '歌え', 'なく', 'なっ', 'た', '（', '声', 'が', '出', 'なく', 'なっ', 'た', '）', '元', '歌姫', 'の', '唯一', 'の', '居場所', 'これ', 'も', '現実', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ジョブズ', 'さん', 'の', '居', 'ない', '→', 'アップル', '庵野', '監督', 'の', '居', 'ない', '→', 'ガイナックス', '小室', 'さん', 'の', '居', 'ない', '…', '´', 'Д', 'y', '━･', '踏み止まら', 'ない', '？', 'ん', 'だ', 'ろー', 'なぁ', 'この', '手', 'の', 'ニュース', 'で', '思う', 'ん', 'だ', 'が', '…', '全員', '→', '給料', '半額', '」', 'とか', '？', '上位', '5', '→', '退職', '」', 'とか', '？', '下', 'を', '切る', '」', 'より', '→', '上', 'を', '切っ', 'た', '方', 'が', '…', '会社', 'が', '生き残れる', '」', 'と', '思う', 'ん', 'だ', 'よ', 'ね', 'd', '￣', '￣', '有能', 'な', '人', 'は', '→', '新しい', '会社', 'を', '起こせる', '」', 'の', 'だから', 'さ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'これだけ', 'リストラ', 'し', 'たり', '縮小', 'し', 'てる', 'の', 'に', '過剰', '公務員', 'は', 'リストラ', 'し', 'ない', 'の', 'は', '不思議', 'だ', 'と', '思う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'イベント', '関連', 'は', '全体', '的', 'に', 'ボロボロ', 'です', 'ね', '。', '財務', '体質', 'の', '良い', '会社', 'は', '大丈夫', 'でも', '、', 'いずれ', 'は', '存続', '自体', 'の', '危機', 'が', '来る', 'ん', 'じゃ', 'ない', 'か', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ライブ', 'は', '赤字', '、', 'ファンサービス', 'だ', 'と', 'いつか', '聞き', 'まし', 'た', 'が', 'それ', '、', '大嘘', 'だっ', 'た', 'の', 'です', 'ね', '。', 'まぁ', '赤字', 'な', 'のに', 'やる', 'わけ', 'ない', 'か', '。', 'エンタメ', '界', '、', '外食産業', 'は', '本当に', 'キツ', 'いと', '思い', 'ます', '。', '好き', 'な', 'アーティスト', 'や', 'それ', 'を', '支える', 'スタッフ', 'の', '皆さん', 'いつも', '食べ', 'に', '行っ', 'て', 'い', 'た', 'お', '店', 'など', 'など', '自分', 'の', '好きな人', 'や', 'お', '店', 'は', '支援', 'を', 'し', 'たい', 'もの', 'です', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ', 'は', '収まら', 'ない', 'し', '、', '企業', 'は', '希望退職', '募る', 'し', '、', '来年', 'は', 'もっと', '悪く', 'なり', 'そう', '、', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '勢い', 'が', '凄い', 'と', '衰退', 'も', '凄い', 'ん', 'だ', 'な', '、', 'バブル', 'と', '一緒', 'で', '。', '貧しく', 'も', '安定', 'が', '一番', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '希望', '退職', 'さ', 'せ', 'て', '必要', 'に', 'なっ', 'たら', '派遣', 'で', '補填', 'する', 'やり方', 'なんて', '止め', 'て', '欲しい', '！', 'どうせ', '竹中平蔵', 'の', 'いれ', '知恵', 'な', 'ん', 'だろ', 'けど', '！', 'この国', '、', '悪い', '方向', 'へ', 'しか', '進ま', 'ない', 'ね', '！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '残念', 'ながら', 'これ', 'が', '経済的', '淘汰', '。', 'そして', '新しい', '芽', 'が', '出る', '。', '企業', 'は', '常に', '正しい', '波', 'に', '乗り', '続け', 'なく', 'て', 'は', 'いけ', 'ない', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ここ', '数', '年', '、', 'K', 'POP', '依存', 'が', '凄かっ', 'た', 'から', 'なぁ', '。', 'コロナ', 'で', 'K', 'POP', 'の', 'ライブ', '再開', 'の', '見通し', 'が', '立た', 'ない', '事', 'も', '要因', 'なのかな', '？', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '安斉かれん', 'を', '力', 'を', '入れ', 'て', '売り出す', 'の', 'とか', '見', 'てる', 'と', '、', '未だに', '小室ファミリー', '時代', 'な', 'ん', 'だ', 'なぁ～', 'と', '時代錯誤', 'を', '感じる', '、', 'そんな', '人材', 'だらけ', 'なんだろう', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'こんな', '状態', 'で', 'も', '、', '今日', 'も', '国会', 'で', 'は', 'アングル', '質問', '答弁', 'の', '応酬', 'で', '。', 'しっかり', '歳費', 'は', '貰える', 'ん', 'だ', 'から', '、', 'お', '気楽', 'な', 'もん', 'だ', 'わ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '今', '辞め', 'た', 'ところ', 'で', 'どこ', 'も', '暇', 'だ', 'し', '転職', '先', 'も', '難しい', 'し', '、', '自営', '始め', 'た', 'ところ', 'で', '厳し', 'そうだ', 'し', '、', '田舎', 'で', 'のんびり', '暮らし', 'たい人', 'くらい', 'だろ', 'う', 'な', '手', 'を', 'あげる', 'の', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '韓国', 'の', 'エンターテイメント', '会社', 'は', '調子', 'が', 'とても', '良い', 'です', 'が', '、', '日本', 'は', '大変', 'そう', 'です', 'ね', '。', '結局', 'ビジネス', 'は', 'リスク分散', 'が', '大事', 'だ', 'と', '思い', 'ます', '。', '日本', 'は', '日本', 'マーケット', 'と', 'CD', 'で', 'の', '流通', 'に', '集中', 'し', 'た', '結果', '、', '日本', 'で', 'の', 'ビジネス', 'が', '縮小', 'し', 'た', '場合', '打ち', '手', 'が', 'ない', '。', '韓国', 'は', 'ストリーミング', 'と', 'オンラインー', 'live', 'に', '切り替え', '、', 'ダメージ', 'が', 'ほぼ', '無い', 'に', '等しい', '。', '韓国', 'を', '真似', 'する', '必要', 'は', 'ありません', 'が', 'これ', 'を', '機', 'に', '体質', 'を', '変える', '必要', 'が', 'ある', 'と', '思い', 'ます', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '100', '名', '退職', 'さ', 'せ', 'て', '何', 'か', 'が', '変わる', 'の', 'か', '？', 'そんなに', '人件費', 'が', '圧迫', 'し', 'て', 'いる', 'の', 'なら', '下', 'から', '100人', 'で', 'は', 'なく', '1番', '多く', '利益', 'を', '得', 'て', 'いる', 'トップ', 'から', '100人', '辞めれ', 'ば', 'いい', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ライブ', 'で', 'コロナ', 'の', '影響', 'も', '有る', 'けど', '単純', 'に', '売れ', 'なく', 'なっ', 'た', 'アーティスト', 'が', '多い', 'の', 'も', '事実', 'だ', 'よ', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エイベックス', 'も', 'そう', 'だ', 'けど', '、', '今年', '、', '来年', 'プロ野球', 'で', '戦力外', 'に', 'なっ', 'た', '選手', 'の', '再就職', 'なんて', 'かなり', '厳しい', 'ん', 'だろ', 'う', 'な', '…', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '昔', 'は', 'やたら', 'と', '高額', 'な', 'ボーナス', '出し', 'たり', '本当に', '羽振り', 'が', 'よかっ', 'た', 'のに', 'なあ', '盛者必衰', 'の', '理', 'を', '表す', 'か', '・', '・', '・', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '希望退職', '者', 'を', '募る', '前', 'に', '、', '薬物', '検査', '希望', '者', 'を', '募っ', 'た', '方', 'が', 'いいんじゃない', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '僕', 'は', 'エイベックス', 'の', '面接', '試験', 'に', '行き', 'まし', 'た', 'が', '不採用', 'でし', 'た', '。', '希望退職', 'は', '初めて', '聞き', 'まし', 'た', '。', 'しかしながら', '、', 'リストラ', 'も', 'おかしく', 'ありません', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'この', '会社', 'が', '伸び', 'た', 'の', 'と', '氷河期', 'は', 'ほぼ', 'リンク', 'する', 'けど', '氷河期', 'を', 'リストラ', 'する', 'ほど', '雇っ', 'て', 'た', 'と', 'は', '思え', 'ない', 'の', 'です', 'が', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '感染対策', '、', '感染対策', 'と', 'ばっかり', 'いう', 'けど', '、', 'それ', '以上', 'に', '経済', '対策', 'し', 'ない', 'と', 'ほんとに', 'マズイ', 'よ', '。', '政府', 'は', '自分', '達', 'が', '緊急事態宣言', 'なんて', 'だして', '撒い', 'た', '種', '、', 'ちゃんと', '広い', '集め', 'て', 'ください', '。', '経済', 'を', '軽視', 'し', '、', '国民', 'や', 'マスコミ', 'に', 'いわ', 'れる', 'が', 'まま', 'に', '自粛', 'さ', 'せ', '、', 'インフル', '以下', 'の', '死亡者', '数', 'で', 'ここ', 'まで', '日本の経済', 'を', 'ボロボロ', 'に', 'し', 'た', 'の', 'は', '行政', 'です', 'よ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '世の中', 'で', '不要', 'な', 'もの', 'が', 'ここ', 'まで', '良く', '頑張っ', 'て', 'き', 'た', 'と', '思う', '。', 'この', '業界', 'コロナ禍', 'で', 'ある', 'うち', 'は', '廃れ', 'ます', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'やっぱり', 'こういう', '世界', 'も', 'コロナ禍', 'の', '影響', 'モロ', 'に', '受け', 'てる', 'の', 'か', '。', '別', 'の', '会社', 'で', '舞台', '装置', 'の', 'レンタル', '会社', 'が', '何', 'ヶ月', 'か', '前', '、', 'コンサート', '中止', 'で', '、', '大変', 'だ', 'みたい', 'な', '、', 'ニュース', '見', 'た', '。', '本当に', '最悪', 'だ', '。', 'コロナ', 'で', '滅茶苦茶', 'に', 'さ', 'れ', 'てる', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '政府', 'は', '新型コロナ', 'の', '影響', 'で', '日本企業', 'が', 'ピンチ', 'だ', 'という', 'こと', 'を', '肌', 'で', '感じ', 'て', 'い', 'な', 'あの', 'では', '・', '・', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '参入障壁', 'は', '低い', '業界', 'だ', 'から', 'アイデア', 'と', '人脈', 'が', 'ある', '人', 'は', '独立', 'する', 'の', 'に', 'いい', '機会', 'かも', 'しれ', 'ない', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'そりゃ', 'ライブ', 'ない', 'と', '稼げ', 'ない', 'よ', 'な', '。', 'スタッフ', 'は', 'い', 'て', 'も', '仕事', 'ない', 'だろ', 'う', 'から', '退職', '勧告', 'は', '妥当', 'だ', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ禍', 'を', '利用', 'し', '言い訳', 'に', 'し', 'て', '企業', 'は', '何', 'でも', '出来る', 'よう', 'に', 'なり', 'まし', 'た', 'ね', '浅はか', 'な', '人たち', 'が', '不況', 'だ', 'と', '騒ぎ', '過ぎる', 'から', 'なのに', 'まったく', '分かっ', 'て', 'い', 'ない', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'そもそも', 'エンタメ', 'も', '飲食', 'も', '供給', '過多', 'だ', 'から', 'ねー', '需要', 'を', '上回っ', 'て', 'いる', 'サービス', 'は', 'きつい', 'だろ', 'う', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'Go', 'to', '一部の人', 'だけ', 'に', '恩恵', 'の', 'ある', '政策', 'は', '即', 'やめ', 'て', '賛否', 'ある', 'けど', '給付金', 'が', '平等', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '流行り', 'だ', 'から', '「', 'それ', 'っ', '」', 'と', '不動産', 'を', '買い', 'まくり', '大', '失敗', '流行り', 'だ', 'から', '「', 'それ', 'っ', '」', 'と', '中国', 'へ', '行っ', 'て', '大', '失敗', '流行り', 'だ', 'から', '「', 'それ', 'っ', '」', 'と', '成果主義', '入れ', 'て', '大', '失敗', '流行り', 'だ', 'から', '「', 'それ', 'っ', '」', 'と', '希望退職', '募っ', 'て', 'やっぱり', '大', '失敗', 'に', 'なる', 'と', '思う', 'よ', '。', '流行り', 'だ', 'から', '「', 'それ', 'っ', '」', 'と', '規制', '緩和', 'し', 'て', '大', '失敗', 'し', 'た', 'みたい', 'に', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'レイオフ', 'みたい', 'に', '業績', 'が', '回復', 'し', 'たら', '再雇用', 'できる', '希望退職', '制度', 'なら', 'いい', 'のに', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ', 'から', '一体', '私たち', 'は', '何', 'を', '守っ', 'て', 'いる', 'の', 'だろ', 'う', 'か', '？', 'サッカー界', 'から', 'も', '多数', 'の', 'コロナ', '患者', 'が', 'で', 'た', 'し', '野球界', 'から', 'も', '。', 'で', 'だれか', '死亡', 'し', 'た', 'の', 'か', '？', '重症', '化', 'し', 'た', 'の', 'か', '？', 'そん', 'あ', '話', 'は', 'まったく', '聞か', 'ない', '。', 'インフル', 'と', '何', 'が', '違っ', 'て', 'いる', 'の', 'か', '？', '今年', 'は', '老人', 'の', '死亡者', 'も', '減少', 'し', 'て', 'いる', '。', 'なんで', '経済', 'を', 'これだけ', '犠牲', 'に', 'し', 'て', 'いる', 'の', 'だろ', 'う', 'か', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '株高', 'で', '含み益', 'が', 'ある', 'はず', 'な', 'のに', '希望退職', '？', '欲', 'ど', 'おしい', 'にも', '程', 'が', 'ある', '。', 'もしかして', '株', '投資', 'し', 'て', 'い', 'なかっ', 'た', 'の', 'か', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'そもそも', 'エイベックス', 'に', '就職', 'する', '事', 'に', '、', '抵抗', 'は', '無かっ', 'た', 'の', 'か', 'な', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'もう', 'avex', 'の', '音楽', 'は', '面白く', 'ない', '。', '大量生産', 'が', '売れ', 'ない', 'の', 'は', '当然', 'でしょ', 'う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'avex', 'の', '売り上げ', 'が', '昔', 'に', '比べ', 'て', '落ち', 'て', 'しまっ', 'た', 'の', 'かも', '。', '時代の流れ', 'も', 'あり', 'そう', 'だ', 'よ', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ここ', '最近', 'ずっと', '鳴かず飛ばず', 'だっ', 'た', 'し', '何', 'も', 'コロナ', 'だけ', 'が', '原因', 'じゃ', 'ない', 'と', '思う', 'けど', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '今年', 'より', '来年', 'は', '自殺', '者', '増え', 'そう', 'だ', 'な', '。', 'こんな', 'ご時世', 'で', 'も', '公務員', 'の', '給料', 'を', '大幅', 'カット', 'し', 'ない', 'って', '、', '人事院', 'は', '頭', 'おかしい', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'マサ', 'こと', '松浦', 'は', 'めっちゃ', '羽振り', '良', 'さ', 'そう', 'だ', 'けど', 'まずは', '幹部', 'が', '身', 'を', '切っ', 'て', '節約', 'し', 'たら', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ', 'に', '関係', 'なく', '落ち目', 'の', '企業', 'だ', 'から', 'どのみち', 'リストラ', 'は', '規定', '路線', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'あれ', '？', '社長', 'は', '別', '会社', 'で', '資産運用', '中', 'でしょ', '？', '社員', 'を', '大事', 'に', 'し', 'ましょ', 'う', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '任天堂', '、', 'めっちゃ', '儲かっ', 'てる', 'ぞ', '。', '差', 'が', '激しいっ', 'て', '感じ', 'も', 'する', 'けど', '、', '可処分所得', 'の', '関係', 'で', '負け組', '多数', 'だ', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '日本', 'の', '氷河期世代', 'って', '、', 'つくづく', '影響', '受け', 'て', 'ます', 'よ', 'ねぇ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エンタメ', '業界', 'は', '厳しい', '。', 'でも', 'あらゆる', '業界', 'は', '悲鳴', 'を', 'あげ', 'てる', '。', 'それでも', '通年', 'の', 'インフルエンザ', 'より', 'も', '少ない', '死者数', 'の', 'コロナ', 'を', '怖がり', 'すぎ', 'て', 'いまだに', '非常事態宣言', 'だ', 'と', '言っ', 'てる', '人', 'も', '多い', '。', '正気', 'か', '？', 'もう', 'コロナ', 'より', '経済', '壊れる', '方', 'が', 'ずっと', '怖い', '。', 'いい加減', 'この', '茶番劇', 'は', 'いつ', 'まで', '続ける', 'の', 'か', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '希望退職', 'を', '募集', 'する', 'と', '有能', 'な', '残っ', 'て', 'い', 'て', '欲しい', '人', 'から', '辞め', 'て', 'いく', '退職金', '割り増し', 'さ', 'れ', 'て', 'ヘッドハンティング', 'だ', 'から', 'ウマウマ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '正直', 'コロナ', 'なく', 'て', 'も', '下降', '線', 'を', '辿っ', 'て', 'いる', '事', 'は', '分かっ', 'て', 'い', 'まし', 'た', 'が', '…', 'みんな頑張ってる', 'やん', '…', '。', '頑張ら', 'ない', '方', 'が', '稼げる', 'ん', 'や', '。', '不条理', 'や', 'な', '…', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エイベックス', 'まで', 'か', '。', '日本経済', 'は', 'どん底', '、', 'コロナ', 'の', '影響', 'だけ', 'で', 'は', 'ない', 'と', '思う', 'の', 'だ', 'が', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '年末', '年明け', 'にかけて', 'この', '動き', 'は', '各', '業界', 'に', '加速', 'し', 'そう', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '会社', 'に', '貢献', 'し', 'て', 'も', '、', 'もう', '年寄り', 'は', '要ら', 'ない', 'と', '、', 'そういう', '事', 'か', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ禍', 'の', '影響', 'と', '解っ', 'て', 'い', 'て', 'も', '、', '薬', '関係', 'で', 'ロク', 'な', '印象', 'が', 'ない', '。', '大金', 'が', '舞い込み', '好き', '勝手', 'な', '事', 'を', 'し', 'て', 'き', 'ちゃっ', 'た', 'から', 'なぁ～', '。', '神様', '見', 'て', 'た', 'の', 'か', 'な', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エンタメ', 'も', 'やはり', '大', '打撃', 'か', '色んな', '業界', 'で', '高給', '盛り', 'の', '40代', '以上', 'が', 'しめ出', 'さ', 'れる', '転職', 'も', '待遇', 'かなり', '下がる', 'やろ', 'し', '大変', 'や', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エイベックス', 'の', 'ある意味', 'バブル期', 'を', '知る', '人達', 'が', 'リストラ', '対象', 'に', 'なる', 'の', 'か', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '本当に', '未だに', 'コロナ', 'コロナ', '騒い', 'でる', '人', 'は', '現実', '見', 'て', 'ない', 'な', 'って', '思う', '経済', '力', 'の', 'なくなっ', 'た', '日本', 'の', '方', 'が', 'よっぽど', '恐怖', 'だ', 'よ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '港区', 'に', 'あんな', '立派', 'な', 'ビル', '建て', 'た', '会社', 'が', 'リストラ', 'は', 'ない', 'と', '思う', 'なあ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '元', 'エイベックス', 'って', '肩書', 'は', 'ちょっと', 'かっこいい', 'と', 'は', '思う', 'けど', '、', 'エンタメ', '業界', 'の', '転職', 'って', '大変', 'そう', 'な', 'イメージ', 'しか', 'わか', 'ない', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ユーチューバー', 'の', '様', 'に', 'ネット上', 'の', '投げ銭', 'や', '広告', 'で', '食っ', 'て', 'いく', 'よう', 'な', '業態', 'が', '儲かり', '実際', 'の', 'ライブ', 'は', '儲から', 'なく', 'なる', 'ん', 'だろ', 'う', 'ね', '、', '韓国', 'じゃ', 'ライブ', 'は', '無料', 'が', '当たり前', 'らしい', 'から', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'まぁ', '選ば', 'なけれ', 'ば', 'いくら', 'で', 'も', '仕事', 'は', 'ある', 'し', 'ね', '。', 'やっぱり', '日本', 'は', '恵まれ', 'てる', 'と', '思わ', 'ない', 'と', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ', 'が', '流行り', 'だし', 'た', '2月', '、', '押谷', '教授', 'は', 'これから', '大企業', 'が', 'どんどん', '潰れ', 'て', 'いく', 'と', '言っ', 'た', 'その', '言葉通り', 'の', '世の中', 'が', '始まっ', 'てる', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '切ら', 'ない', 'と', 'いけ', 'ない', '無能', 'は', '、', 'きちんと', '切ら', 'なきゃ', '、', '日本', 'が', '破綻', 'し', 'ます', '。', '人柱', 'に', 'なり', '、', '政府', 'は', '血税', 'を', '恵ま', 'ない', 'で', 'ください', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エンタメ', '系', 'で', '好調', 'な', 'の', 'って', '任天堂', 'と', 'ソニー', 'くらい', 'じゃ', 'ない', 'の', '？', 'あと', 'は', '一部', 'ゲームソフト', 'の', 'メーカー', 'とか', 'ライブ', 'で', '稼い', 'で', 'たよう', 'な', '人', 'は', '考え', 'かた', 'かえ', 'ない', 'と', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ヒカル', 'と', 'コラボ', 'する', 'ん', 'だっ', 'た', 'か', '。', 'YouTuber', 'と', '組ま', 'ない', 'と', 'いかん', 'ほど', 'やばい', 'ん', 'や', 'なー', '。', 'むしろ', 'な', 'んで', '自ら', 'もっと', 'YouTube', '活用', 'せ', 'ん', 'の', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '社員', 'の', '4分', 'の', '一', 'って', '…', 'なかなか', '凄い', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '盛者必衰', 'と', 'は', '正に', 'こういう', '事', 'な', 'の', 'か', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ケツメイシ', 'が', '来年5月', 'に', '全国ツアー', 'を', '開催', 'する', 'と', 'アナウンス', 'が', 'あり', 'まし', 'た', '。', 'ファン', 'な', 'ん', 'です', 'けど', '正気', 'の', '沙汰', 'と', 'は', '思え', 'ない', '。', 'クラスター', '発生', 'さ', 'せ', 'て', 'エイベックス', '共々', '炎上', 'し', 'て', '終わり', 'の', '図式', 'が', '見える', '様', 'な', '…', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '時代', 'が', '一', '回転', 'し', 'た', '感じ', '。', '[SEP]']\n",
      "テキストのデータ :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ['[CLS]', '全盛期', 'の', '浜崎', 'は', '1人', 'で', '200', '億', '以上', '叩き', 'だし', 'て', 'い', 'た', 'から', 'なぁ', '。', 'コロナ', 'の', '影響', 'が', '怖い', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'これから', '、', '益々', '、', '景気', 'は', '、', '悪化', 'の', '一途', 'だろ', 'う', '…', 'どれ', 'だけ', 'の', '企業', 'が', '年', 'を', '越せる', '事', 'やら', '…', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '青山', 'の', '一等地', 'の', '豪華', 'な', '自社ビル', '。', 'あれ', 'は', '必要', 'か', 'なー', '。', '心配', 'です', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '自分', 'は', 'まだ', '安定', '業種', 'な', '方', 'で', '良かっ', 'た', 'か', 'な', '・', '・', 'まぁ', '安', '月給', 'だ', 'けど', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '行く', '宛', 'が', 'ある', '有能', 'な', '人', 'が', '辞め', 'て', '行く', 'よ', 'ね', '。', 'エンタメ', 'なんて', '横', 'の', '繋がり', 'が', '大きい', 'し', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'この', '会社', '位', 'なら', '残業', 'し', 'て', 'も', '、', '残業代', 'は', '出', 'た', 'の', 'で', 'は', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'もう', 'コロナ', '前', 'に', 'は', '戻ら', 'ない', 'から', 'な', '。', 'ライブ', 'も', '採算', 'ベース', 'で', 'やれ', 'ない', 'だろ', '。', 'ワクチン', '次第', 'って', '事', 'だ', 'けど', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '40歳', '以上', '…', 'まとも', 'な', '再就職', 'で', 'きん', 'だろ', 'な', '…', 'Avex', '上がり', 'た', 'なら', 'プライド', 'も', 'あり', 'そうだ', 'し', 'や', 'べ', 'ー', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '大不況', 'です', 'ね', '。', 'サラリーマン', 'は', 'いつ', '首', '切ら', 'れる', 'か', 'わかり', 'ませ', 'ん', 'ね', '。', 'そう', '考える', 'と', 'ライセンス', 'の', '要る', '仕事', 'は', '不況', 'に', '強い', 'です', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'イベント', 'の', '中止', 'と', '今後', 'は', '無観客', 'も', '当たり前', 'に', 'なる', 'だろ', 'う', 'が', '仕方', 'ない', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '昔', 'の', 'よう', 'に', '、', 'Uターン', 'や', 'Ｊ', 'ターン', 'で', '中央', 'の', 'ノウハウ', 'や', 'ビジネス', 'が', '地方', 'に', '還元', 'さ', 'れる', 'と', '全体', 'が', '活性', '化', 'する', 'の', 'です', 'が', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ', 'で', '荒廃', 'し', 'た', '今', '、', 'バイト', 'が', 'ある', 'だけ', 'でも', 'ありがたい', 'の', 'かも', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'どの', '企業', 'も', '希望退職', 'を', '募集', 'し', '始め', 'た', '。', 'で', '、', '国会議員', 'たち', 'の', 'リストラ', 'は', 'いつ', 'に', 'なっ', 'たら', 'やる', 'ん', 'です', 'か', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'リストラ', '合戦', 'が', '始まる', 'ね', '。', '始まり', 'に', '過ぎ', 'ない', 'です', 'ね', '。', 'これから', 'です', '。', 'マネ', 'し', 'ます', 'から', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '年末', 'な', 'のに', '、', 'ガッカリ', 'ばかり', 'だ', 'ね', '。', 'サラリーマン', 'は', 'なに', 'も', '出来', 'ない', 'から', '辛い', 'な', '。', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '新しい', '豪華', 'な', '自社ビル', '造っ', 'て', 'おい', 'て', '希望退職', 'は', 'マジ', '笑える', 'まあ', '、', '昔', 'の', 'ビル', 'の', '時', 'は', '中', '、', '汚く', 'て', 'だら', 'しない', '会社', 'だ', 'な', '、', 'って', '思っ', 'た', 'けど', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '会社員', '100人', '退職', 'さ', 'せる', 'なら', '、', '役員', 'を', '2', '〜', '3人', '辞め', 'させ', 'て', '、', '会社員', 'の', '失業', 'を', '減らす', 'べき', 'じゃ', 'ない', 'か', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '氷山の一角', 'です', 'ね', '。', 'コロナ', 'を', '恐怖', 'の', '病気', 'として', '見', 'て', 'いる', '以上', '、', '景気', 'は', 'これから', 'どんどん', '悪く', 'なる', '一方', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '大規模', 'イベント', 'やれ', 'ば', '良い', 'だけ', 'じゃ', 'ない', 'の', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '40代', '以上', 'の', '音楽', '市場', 'も', 'まだまだ', '稼げ', 'そう', 'だ', 'が', '。', '懐メロ', '市場', 'は', '侮れ', 'ない', 'よ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'バブル期', 'の', '代表', '企業', 'が', 'よく', '頑張っ', 'てる', 'よ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '天下り', 'の', '人間', 'を', '一層', 'し', 'て', '一', 'から', '音楽', '作り', 'を', 'やり直し', 'た', 'ほう', 'が', 'いい', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '年末', 'にかけて', '今年', 'は', '特に', 'こんな', 'ニュース', 'が', '激増', 'し', 'そう', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'こういう', '事案', 'に', '（', '数', 'が', '多い', '数字', 'に', '）', '…', '高水準', 'と', '書く', 'の', 'は', '…', 'おかしい', 'の', 'です', 'けども', 'ねぇ', '〜', '…', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ', 'の', '責任', '追及', 'が', '薄く', 'なっ', 'て', 'き', 'た', 'な', '。', '米', 'が', '頼り', 'だっ', 'た', 'が', '、', '、', '、', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '音楽', '、', '舞台', 'など', 'エンタメ', 'は', '、', 'これから', 'が', '苦境', 'かも', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '浜崎', 'に', 'かける', '経費', '削っ', 'た', '方', 'が', '早く', 'ない', 'か', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'やはり', 'エンタメ', '界', 'も', '相当', '厳しい', 'という', 'こと', 'です', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エイベックス', 'みたい', 'な', '夢', 'を', '売る', 'みたい', 'な', 'ところ', 'が', '希望退職', '募集', 'って', '、', 'どんだけ', 'お先真っ暗', 'な', 'ん', 'だ～っ', 'て', '思っ', 'て', 'しまっ', 'た', 'けど', '笑', '、', 'エイベックス', 'も', '会社', 'な', 'の', 'よ', 'ねー', '致し方', 'なし', '；', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '歌手', 'の', '歌い', 'かた', 'YouTube', '講座', 'とか', '開い', 'た', '方', 'が', '良い', 'の', 'かも', '。', '自社', 'の', '歌', 'なら', '、', '著作権', 'は', 'クリア', 'し', 'て', 'いる', '。', '月謝', 'を', 'しっかり', '取れ', 'ば', '、', 'かなり', 'の', '収益', 'に', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '細かい', 'ん', 'だ', 'けど', '、', '「', '最終', '利益', '」', 'じゃなくて', '「', '最終損益', '」', 'だ', 'よ', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '100', '名', 'って', '数字', 'だけ', 'だ', 'と', '少なく', '聞こえる', 'が', '、', '1', '4', '相当', 'の', '人員', 'だ', 'から', 'けっこう', 'な', '規模', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '国会議員', 'が', 'おじいさん', 'ばっか', 'で', '本気', 'に', 'なっ', 'て', '選挙', 'に', '投票', 'し', 'ない', 'と', '利権', '政治', 'に', 'なっ', 'て', '行っ', 'て', 'しまう', 'なぁ～', '、', '、', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '自社', '買い', 'し', 'て', 'いる', 'イメージ', 'だ', 'けど', 'こんなに', 'なる', 'まで', '厳しい', 'と', 'は', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '初めて', 'が', '、', '来年', '頃', 'に', 'は', '二', '度目', 'に', '成る', 'かも', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'これから', 'も', 'まだまだ', '続く', 'でしょ', 'う', '。', '恐ろしい', '、', 'どう', 'なる', 'ん', 'だろ', 'う', 'か', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エイベックス', 'が', 'こんな', 'こと', 'に', 'なる', 'の', '数年前', 'に', 'は', '考え', 'られ', 'なかっ', 'た', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'じゃー', '、', 'そ', 'ー', 'なる', '状況', 'を', '作っ', 'た', 'って', 'の', 'は', '経営陣', 'の', '能力', '不足', 'も', 'ある', 'ん', 'じゃ', 'ねー', 'の', '？', '○', '麻', 'やっ', 'て', 'たら', 'まとも', 'な', '経営', '、', '判断', '出来', 'ねー', 'だろ', '笑', '。', '日本', 'の', '経営者', 'って', '自分', 'の', '能力', '買いかぶり', '過ぎ', 'な', 'ん', 'だ', 'よ', '。', 'なら', 'テメー', 'も', '何らかの', 'ペナ', 'を', '科せ', 'よ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '時代の流れ', '。', '消える', '産業', 'に', '延命措置', 'は', 'いら', 'ない', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ここ', 'は', '、', '松浦', 'さん', 'が', '辞め', 'ない', '限り', '、', '永遠に', 'ホワイト企業', 'に', 'なら', 'ない', 'よ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '対象', '社員', '443', '名', 'で', '100', '名', 'って', '凄い', '比率', 'だ', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'この', '会社', 'も', 'エラ', 'イ人', 'が', '無駄遣い', 'し', 'てる', '印象', 'しか', 'ない', 'けど', 'なあ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '売れ', 'て', 'ない', 'アーティスト', 'も', 'リストラ', 'し', 'た', 'ほう', 'が', 'いい', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '建て直し', 'し', 'た', 'ばかり', 'の', '所', '？', 'イベント', '系', 'の', '会社', 'は', 'コロナ', 'で', '大', '打撃', 'です', 'ね', 'この先', 'どう', 'なっ', 'て', '行く', 'ん', 'だ', 'か', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '一気に', 'これ', 'で', 'この', '会社', 'の', '未来', 'も', '暗く', 'なり', 'はじめ', 'た', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'CD', '売れ', 'ない', '、', 'ライブ', '開催', 'でき', 'ない', 'から', '致し方', 'ない', '…', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '薬', 'と', '噂', 'の', '会社', 'も', 'ついに', 'その', '時', 'が', '来', 'た', 'ん', 'だ', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ライブ', 'も', '当分', '無理', 'や', 'から', 'な', 'もう', 'どの', '業界', 'も', '厳しい', 'よ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '一年', '前', 'は', 'どこ', 'も', '人手', '不足', 'だっ', 'た', 'のに', '180度', '変わっ', 'て', 'しまっ', 'た', 'なあ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ディストリビューター', 'なんて', 'スタッフ', '30', '名', '40', '名', 'いりゃ', '回せる', '時代', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'この', '会社', 'まで', '？', '？', 'と', '思う', '所', 'も', '、', '続々', 'と', '早期退職', '者', 'を', '募っ', 'て', 'いる', '。', 'コロナ', 'が', '滅びる', 'か', '、', '世界', 'が', '滅びる', 'か', '…', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '乃木坂メンバー', 'で', 'さえ', '契約期間', '終了', 'で', '卒業', 'の', 'メンバー', 'も', '毎年', 'いる', 'ので', '仕方', 'ない', 'です', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '希望退職', 'いくら', '上乗せ', 'し', 'て', 'くれる', 'の', 'か', 'が', '知り', 'たい', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'イベント', 'など', 'も', 'ＧＯＴＯ', '政策', 'が', '必要', 'な', 'の', 'で', 'は', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '沢尻エリカ', 'を', '雇う', 'なら', '、', 'その', '金', 'で', '何人', 'か', 'の', '社員', 'は', '救わ', 'れる', 'はず', 'でしょ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'こういう', '時', 'の', '社内', 'って', 'どんな', '雰囲気', 'な', 'ん', 'だろ', 'う', '。', 'ちょっと', 'こわい', '・', '・', '・', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '近いうちに', '生活保護', 'の', '打ち切り', 'も', '開始', 'さ', 'れる', 'ん', 'じゃ', 'ない', '？', '最低', 'で', 'も', '減額', 'とか', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'どんどん', '広がる', '不況', 'を', '目先', 'の', '数字', 'で', 'しか', '判断', '出来', 'ない', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エイベックス', 'と', '言え', 'ば', '、', 'MAX', '松浦氏', '。', '元気', 'に', 'し', 'てる', 'か', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '今', '、', 'エーベック', '在籍', 'タレント', 'て', '、', '誰', 'が', 'いる', 'の', 'か', 'な', 'と', '見', 'て', '安西', 'かれん', '、', 'ああ', 'ドラマ', 'の', 'ね', 'ＥＬＴ', '、', '大塚愛', '、', '倖田來未', '、', '小室哲哉', 'ああ', '前', '会長', 'の', '付き合い', 'ね', '伍代夏子', '、', '杉良太郎', 'え', '？', 'ちょっと', '笑っ', 'た', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'とうとう', '行き詰まっ', 'て', '来', 'た', 'か', '。', '繁栄', 'は', '永遠', 'じゃ', '無い', 'お手本', 'です', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '安斉かれん', 'を', '推し', 'た', '時点', 'で', 'avex', 'は', '終わっ', 'た', 'な', 'と', '感じ', 'た', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '最近', 'の', 'エイベックス', 'は', '良い', 'イメージ', 'が', '無い', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '会社', 'は', '自分', 'の', '人生', 'を', '預ける', 'とこ', 'じゃ', 'ない', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ちょうど', 'ゲーム', 'の', 'Apex', 'が', 'シーズン', '７', '始まっ', 'て', 'た', 'から', 'Apex', 'と', '読み', '間違え', 'た', 'わ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '希望退職', 'を', '募る', 'こと', 'は', '、', '退職勧奨', 'を', '迫る', 'うち', 'より', '、', 'ずっと', 'いい', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '社長', 'の', '麻薬', '疑惑', 'に', '売上', '低下', '。', 'もう', '会社', 'が', '無くなる', '日', 'が', '案外', '近い', 'かも', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'もう', 'a', 'nation', 'も', '開催', 'でき', 'ない', 'の', 'か', 'な', '？', 'さびしい', 'なあ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'いくら', '上乗せ', 'し', 'て', 'くれる', 'の', 'だろ', 'う', 'か', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '結局', '将来', 'あと', 'を', '追う', 'だけ', 'でしょ', 'う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ', '関係', 'なく', '斜陽', '気味', 'で', '、', 'コロナ禍', 'に', 'なっ', 'て', '、', 'なおさら', '加速', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エイベックス', 'の', '１', '社員', 'も', '浜崎あゆみ', 'も', '同じ', '人間', 'の', 'はず', '。', 'なんだか', 'な', 'ア', '。', '。', '。', '。', '・', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '音楽業界', 'が', '好き', 'で', '入っ', 'た', '人', 'が', '多い', 'だろ', 'う', 'から', '辞め', 'たく', 'ない', 'だろ', 'う', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '結局', '生き残る', 'の', 'は', '政治家', 'の', '天下り', '先', 'と', '癒着', 'し', 'てる', '企業', 'か', '…', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'みんな', '生き', 'てけない', '…', '税', 'だけ', '徴収', 'さ', 'れる', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'こういう', 'の', 'って', '、', '企業', 'が', '公表', 'する', 'ん', 'です', 'か', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エンタメ', 'を', 'どうにか', '守っ', 'て', '欲しい', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '最大', 'の', '稼ぎ', '頭', 'の', 'EXILE', 'も', 'ATSUSHI', '抜ける', 'と', 'ほぼ', '解散', '状態', 'だ', 'し', '、', 'しょうが', 'ない', 'ん', 'じゃ', 'ね', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '簡単', 'に', 'は', '辞め', 'ない', 'と', '思う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '濱崎', 'さん', 'が', '応募', 'し', 'たり', 'し', 'て', '！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ＧｏＴｏ', 'で', '浮かれ', 'て', 'いる', '裏側', 'で', '起こっ', 'て', 'いる', '現実', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '大手', 'の', '芸能', 'プロ', 'と', 'かも', '追従', 'し', 'そう', 'な', '感じ', 'だ', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '議員', 'は', '、', '政党交付金', 'で', '1人', '二', '千', '万', '円', '入り', 'まし', 'た', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'でも', 'コンテンツ', '持っ', 'てる', 'ところ', 'は', '強い', 'よ', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '退職者', '多数', '。', '仕事', 'は', 'ある', 'ん', 'だろ', 'う', 'か', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'とうとう', 'avex', 'まで', 'もか', '…', 'ウィズ', 'コロナ', '、', 'アフター', 'コロナ', 'は', '地獄', 'だ', 'よ', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '芸能界', 'も', '縮小', '来年', 'も', 'コロナ', 'は', '治ら', 'ない', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ここ', 'も', 'やばい', '。', 'の', 'か', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '鮎', 'で', 'いい', 'の', 'では', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '今', 'の', 'この', '状況', 'で', '辞め', 'れる', '人', 'いない', 'って', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '浜崎あゆみ', 'は', '退職', 'に', '応じる', 'ん', 'です', 'か', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'そんな', '時代', 'が', '来る', 'と', 'は', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エイベックス', 'で', 'すら', 'これ', 'か', 'ホンマ', 'この国', '大丈夫', 'か', '公務員', '以外', '全部', 'ヤバイ', 'やん', 'これから', '冬', 'に', 'なる', 'と', 'もっと', '増える', 'ん', 'やろ', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'CCCD', 'を', '売っ', 'た', '罪', 'は', '許さ', 'ない', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '何だか', '温まる', '話', 'だ', 'な', '竹中', 'の', '懐', 'が', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'これ', 'は', '…', 'まずい', 'な', 'ヤバイ', 'なんて', 'レベル', 'で', 'は', 'ない', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'もう', '凋落', 'の', '一途', 'だろ', 'う', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '希望退職', 'って', '有能', 'が', '人', 'が', 'やめ', 'て', '無能', 'な', '人', 'が', '残る', 'と', 'よく', '聞く', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'あゆ', 'は', '妊娠中', 'で', 'も', 'ライブ', 'を', '休め', 'ない', 'わけ', 'だ', 'わ', '…', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '来年', 'に', 'は', '倒産', 'か', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '早く', 'て', 'この', '春', '。', '持っ', 'て', '1年', 'か', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '夢', 'を', '売る', '会社', 'から', 'の', '酷い', '現実', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'バブル期', 'よ', '、', 'もう一度', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'え', '、', '、', '就活', '大丈夫', 'か', 'な', '…', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '希望退職', '後', 'って', 'どう', 'なる', 'の', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ここ', 'は', 'まず', '社長', 'が', '辞める', 'べき', 'や', 'と', '思う', 'わ', '！', 'ww', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ひとつ', 'の', '時代', 'が', '終わっ', 'た', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '松浦', 'さん', 'が', '切ら', 'れる', 'くらい', 'だ', 'から', '相当', 'やばい', 'ん', 'だろ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '松浦', 'さん', '、', 'お金', 'も', 'らいす', 'ぎなんじゃない', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '下請け', 'は', '、', '納入', '業', '社', 'は', '段々', '締め', 'て', 'き', 'て', 'いる', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '＊', '＊', '会', 'ＯＢ', 'とか', '、', '警察官僚', 'ＯＢ', 'と', 'かも', 'リストラ', '？', 'それ', 'は', 'ない', 'よ', 'ね', '笑', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '25', 'って', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'イベント', 'が', '出来', 'ない', 'から', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'まず', '会長', '切れ', 'よ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'それでも', '今日', 'も', '日経平均株価', 'は', '爆上', 'げ', 'し', 'てる', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '小室哲哉', 'に', '金', '使い', '過ぎ', 'た', 'から', 'か', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '名前', 'が', '大き', '過ぎ', 'て', '何だか', '、', '、', '、', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ウーバーイーツ', 'の', 'せい', 'です', 'ね', '。', 'ウーバーイーツ', 'は', '日本', 'から', '追放', 'し', 'ましょ', 'う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '本社', 'ビル', '売却', 'か', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'イベント', 'でき', 'ない', 'ん', 'じゃ', 'そら', 'そう', 'なる', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '辞め', 'てる', '人', '多い', 'みたい', 'です', 'ね', '・', '・', '・', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'リストラ', 'さ', 'れる', 'の', 'は', 'ローバ', 'か', 'オクタン', 'か', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '辞め', 'たる', 'わ！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '無能', 'な', '正社員', '切っ', 'て', '契約社員', 'だけ', 'の', '会社', 'に', 'し', 'て', 'くれ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'お浜', 'さん', '、', '、', '、', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'それじゃダメじゃん', '春風', '亭', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '大麻', 'は', 'どう', 'なっ', 'た', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '残る', 'も', '地獄', '。', '辞める', 'も', '地獄', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '氷河期', 'で', '企業', '滅びる', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'どんな', '状況', 'に', '置か', 'れ', 'て', 'も', '、', '国家資格', 'を', '持っ', 'てる', '人', 'は', '強い', '資格', '、', '資格', 'です', 'よ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'いや', '〜', '明日', 'は', '我が身', 'だ', 'わ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '箱物', 'エンタメ', 'は', '先細り', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ついに', 'き', 'た', 'ね', '。', 'エンタメ', '業界', 'コロナ', 'に', 'まけ', 'た', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '捕まる', 'から', 'だろ', 'う', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '倒産', 'する', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '浜崎あゆみ', 'が', 'リストラ', 'で', '良い', 'の', 'で', 'は', '？', '（笑）', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ', 'に', '負ける', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'キスマイ', 'の', 'ため', 'に', 'がんばっ', 'て', 'くれ', 'てる', 'のに', '・', '・', '・', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'メッキ', 'が', '剥がれ', 'た', '、', 'と', 'いう', 'か', '分かっ', 'て', 'た', 'よ', '、', '「', 'ＡＡ', 'Ａ', '」', 'とか', 'ゴリ押し', 'し', 'て', 'た', '時点', 'で', '。', '」', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'TOP', 'も', 'アレ', '買う', '金', 'を', 'セーブ', 'せ', 'にゃ', 'いか', 'ん', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'いら', 'ない', '歌手', 'とか', '俳優', 'とか', 'は', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'コロナ', '脳', 'の', '皆さん', '！', '明日', 'は', '我が身', 'です', 'よ～', '！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '今年', '紅白', 'でる', '可能性', 'ある', 'の', 'Snowman', 'と', 'BiSH', 'と', 'エグザイル', 'グループ', 'くらい', 'しか', 'い', 'ない', 'もん', 'な', '。', 'ソニー', 'と', 'エライ', '違い', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '身軽', 'な', '企業', 'が', '生き残る', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '色々', 'と', '浮かび上がっ', 'て', 'くる', '現実', 'に', 'ちょっと', '恐怖', 'を', '感じ', 'ます', 'ね', '音楽業界', 'に', '限ら', 'ず', 'お客', 'を', '集め', 'て', '収益', 'を', '上げる', '産業', 'は', '全滅', 'し', 'て', 'いく', 'の', 'か', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '音楽業界', 'は', '本当に', '厳しい', 'よ', 'な', '。', 'コロナ', '前', 'から', '音楽', 'が', 'CD', 'として', '商品', 'で', '売ら', 'れ', 'て', 'い', 'た', '時代', 'から', 'ただ', 'の', 'データ', 'に', 'なっ', 'て', 'しまっ', 'た', '。', '所有', 'する', 'という', '感覚', 'が', 'なくなっ', 'て', 'しまう', 'と', '価値', 'は', '一気に', '落ちる', '。', '音楽', 'を', '配信', 'さ', 'せ', 'た', 'の', 'は', '絶対', 'に', '失敗', 'だっ', 'た', 'と', '思う', '。', 'しかも', 'アルバム', 'で', 'も', '1曲', '単体', 'で', '買える', 'よう', 'に', 'し', 'た', '理由', 'を', '知り', 'たい', 'よ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'みんな', '、', '景気', 'の', '悪い', '話', 'ばかり', 'で', 'お先真っ暗', 'だ', '、', '切り替え', 'て', '上手く', 'やっ', 'てる', '話し', 'を', 'する', '。', '上手く', 'やっ', 'てる', 'と', '言っ', 'て', 'も', '、', '配信', '会社', 'で', 'は', 'ない', '。', 'この', 'コロナ', '状況', 'で', '夏', 'に', 'は', '、', '有', '観客', 'ライブ', 'を', '始め', '、', '夏', 'に', '33', '公演', '、', '秋', 'から', '12月', 'にかけて', '4度', 'の', '武道館', 'を', '含む', '120', '公演', 'の', '有', '観客', 'ライブツアー', 'を', '行っ', 'て', 'いる', 'アイドルグループ', 'が', 'ある', '。', 'モーニング娘', '。', 'など', 'の', 'ハロプロ', 'だ', '。', '確か', 'に', '一', '公演', 'あたり', 'の', '客席', 'を', '半分', 'に', 'し', 'て', 'いる', 'が', '公演', '数', 'を', '増やし', 'て', '対応', '。', '感染対策', 'も', '関連会社', 'の', '人達', 'を', '動員', 'し', 'て', 'い', 'て', '忙し', 'そう', 'だ', '。', 'やろう', 'と', '思え', 'ば', 'やれる', 'の', 'だ', '。', '頭', 'の', 'いい', '人たち', '言い訳', 'する', 'より', '、', '何', 'が', 'できる', 'の', 'か', '考える', 'べき', 'な', 'の', 'で', 'は', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '業界', '最大手', 'も', 'これ', 'か', '、', '来年', 'は', '世界恐慌', 'だ', 'な', '。', 'もちろん', '来年', 'の', '東京オリンピック', 'は', '中止', 'だ', '。', 'コロナ', 'の', '勢い', 'は', '全然', '落ち', 'ない', 'し', '、', '特効薬', 'も', 'ワクチン', 'も', 'まだ', 'でき', 'ない', '。', '経済', 'なんて', '言っ', 'てる', '場合', 'じゃ', 'ない', 'よ', 'な', '。', 'どっか', 'の', '馬鹿', 'が', '、', '新', 'コロナ', 'は', '弱毒', 'で', '心配', 'ないけい', 'ざいをうごかそうなんていってるが', '自分', 'が', '遊び', 'たい', 'だけ', 'じゃ', 'ない', 'か', '。', '全く', 'これだけ', '人', 'が', '死ん', 'でる', 'の', 'に', 'ちょっと', '自分', 'は', '若く', 'て', '元気', 'だ', 'から', 'って', '強', '毒', 'コロナ', 'を', '弱毒', 'だ', 'なんて', '言っ', 'たら', '亡くなっ', 'た', '人', 'に', '失礼', 'だろ', 'う', '、', '地獄', 'に', '落ちる', 'ぞ', '。', '話', 'が', '脱線', 'し', 'て', 'しまっ', 'た', '、', '来年', 'は', '大変', 'な', '年', 'に', 'なる', '。', 'もう', '一つ', 'ぐらい', '大変', 'な', 'こと', 'が', '起き', 'そう', 'な', '気', 'が', 'する', '。', '大地震', 'なんて', 'ある', 'かも', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'さあ', '！', '「', '一', '億', '総', '底辺', '時代', '」', 'の', '到来', 'です', '。', '1', '％', 'の', '富裕層', 'を', '99', '％', 'の', '底辺', 'が', '支え', 'て', '行く', 'の', 'です', '。', '今迄', '、', '底辺', '職業', 'を', '見下し', 'て', 'い', 'た', '層', 'が', '、', 'その', '底辺', 'の', '下', 'で', '仕事', 'を', 'する', '事', 'に', 'なる', 'の', 'です', '。', '移民', 'も', '入っ', 'て', 'くる', 'ので', '底辺', '職業', 'で', 'さえ', '、', '競争率', 'が', '高く', 'なる', 'の', 'です', '。', '勿論', '、', '賃金', 'は', '「', '最低賃金', '」', 'です', 'そして', '物価', 'は', '上がり', '続け', '、', '税金', 'も', '下がり', 'ませ', 'ん', '。', '年金', '制度', 'も', '国民皆保険', '制度', 'も', '破綻', 'し', 'ます', '。', 'お金持ち', 'だけ', 'が', '高い', '保険料', 'を', '払っ', 'て', '、', '任意保険', 'に', '加入', 'し', 'て', '、', '高度', 'な', '医療', 'を', '受け', 'て', '健康', 'を', '維持', 'し', 'ます', '。', 'そんな', '時代', 'に', '突入', 'し', 'て', 'いる', '事', 'も', '理解', '出来', 'ず', 'に', '、', '愚か', 'な', '日本国民', 'は', '「', 'GO', 'TO', '〜', '」', 'に', '踊ら', 'さ', 'れ', 'て', '散財', 'し', 'て', '感染者', '数', 'を', '増やし', '続け', 'て', 'い', 'ます', '。', '人口', '削減', '計画', 'は', '着実', 'に', '進行', 'し', 'て', 'いる', 'の', 'です', '。', 'ちなみに', '日本人', 'は', '削減対象', 'です', '皆さん', 'も', '、', 'この', '事', 'を', '頭', 'の', '片隅', 'に', '置い', 'た', '上', 'で', '、', '世の中', 'の', '出来事', 'を', '観察', 'し', 'て', 'み', 'て', '下さい', '。', 'きっと', '何', 'か', 'が', '解る', '筈', 'です', '！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '新型コロナ', 'って', '、', '働き盛り', 'の', '人達', 'の', '生活', 'を', '壊し', 'まくる', 'よ', 'なぁ', '。', '感染', 'し', 'て', '亡くなる', '人', 'は', 'ほとんど', 'が', 'お', '看取り', '老人', 'や', 'けど', '、', '感染', 'し', 'ない', '若い世代', 'が', '次々', 'に', '殺さ', 'れ', 'てる', '。', '感染', 'し', 'た', '後遺症', 'は', '、', 'コロナ', '差別', 'と', '後ろめた', 'さ', 'から', 'くる', 'ストレス', 'が', '原因', 'の', '不定愁訴', 'ばかり', 'やし', '。', 'ああ', '恐ろしい', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'や', 'べ', '！', 'っ', 'くす', '（', 'Y', 'avex', '）', '状態', 'じゃん', 'か', '。', '100人', '斬っ', 'て', 'それ', 'で', 'だいじょうぶ', 'な', 'わけ', 'ない', 'じゃん', 'か', '。', 'とりあえず', '斬ら', 'れ', 'た', '100人', 'が', '一丸', 'と', 'なっ', 'て', '会社設立', 'してやっ', 'て', 'く', 'しか', 'ない', 'よ', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '最近', '鳴かず飛ばず', 'の', 'アーティスト', 'ばかり', 'だ', 'もの', 'ね', '。', 'せっかく', '軌道', 'に', '乗っ', 'て', 'も', '不祥事', 'で', '解散', 'する', 'グループ', 'も', 'ある', 'し', '。', 'あと', 'は', 'やっぱり', 'CD', 'を', 'はじめ', 'と', 'し', 'た', '楽曲', 'が', '売れ', 'ない', 'こと', 'と', '、', 'コロナ', 'の', '影響', 'で', 'イベント', 'の', '集客', 'が', '出来', 'ない', 'こと', 'が', '大きな', '原因', 'か', 'な', '。', '最近', 'は', 'まだ', 'spotify', 'なら', 'まだ', '良い', 'ほう', '、', '１', '曲', '１', '円', 'が', '支払わ', 'れる', 'ん', 'だ', 'し', '。', 'でも', '多く', 'は', '違法アップロード', 'の', 'ストリーミングアプリ', '使っ', 'てる', 'よ', 'ね', '。', 'お金', 'を', 'かけ', 'なけれ', 'ば', 'コンテンツ', 'は', '育た', 'ない', 'し', '、', '育た', 'なけれ', 'ば', 'ますます', 'コンテンツ', 'の', '質', 'は', '下がる', 'だけ', 'の', '悪循環', '。', 'もっとも', 'コンテンツ', 'を', '利用', 'する', '多く', 'の', '層', 'が', '貧しく', 'なっ', 'て', 'いる', 'の', 'が', '一番', 'の', '要因', '。', 'でも', 'みんな', '高齢者', '優先', 'の', '自民党', 'に', '投票', 'し', '続ける', 'って', '言う', '矛盾', 'ね', '自ら', '飼い殺し', 'を', '望ん', 'で', 'いる', '馬鹿', 'な', '民族', 'な', 'ん', 'です', 'よ', '、', '日本人', 'は', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '小室', 'と', 'あゆ', 'が', '生み出し', 'た', '貯金', '使い果たし', 'た', 'ん', 'だろ', 'う', 'なー', '。', 'CD', 'とか', '配信', '曲', 'すら', '買わ', 'ない', 'なんとか', 'し', 'て', '無料', 'で', '見聞き', 'しよ', 'う', 'と', 'する', '古事記', '多い', 'もん', 'な', '。', 'はっきり', '言っ', 'て', '音楽業界', '潰し', 'た', 'の', 'は', 'AKB', 'みたい', 'な', '取り柄', 'は', 'ない', 'けど', '足', 'は', '開ける', '女', 'と', 'EXILE', 'みたい', 'な', '9割', '何', 'の', 'ため', 'に', '舞台', 'に', 'いる', 'の', 'か', 'わから', 'ない', '集団', 'が', '筆頭', 'だ', 'と', '思う', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '希望退職', 'が', '出', 'てる', '時点', 'で', 'ヤバイ', 'って', '事', 'だ', 'から', '、', '潤っ', 'て', 'いる', 'なら', '必要', 'ない', 'ので', 'ね', '。', '優秀', 'な', '人', '程', '辞め', 'て', 'いく', '、', 'スキル', '、', 'コネ', '等', '何', 'か', 'ある', 'から', 'お金', 'もらっ', 'て', '転職', '、', '渡りに船', '。', 'ダメ', 'な', '人', '程', '残り', 'た', 'がる', '、', '出来る', '人', 'は', '自分', 'で', 'とっく', 'に', '見つけ', 'てる', 'か', '起業', 'か', '投資', 'か', '何', 'か', 'やっ', 'てる', '。', '外資', 'なんか', '数年', 'で', '他', '行く', 'から', 'それ', 'も', '役職', 'クラス', 'が', '。', '日本', 'も', 'もっと', '転職', 'し', 'やすく', 'すれ', 'ば', 'いい', 'のに', '、', '典型的', '社畜', 'の', '古い', '連中', 'が', '相変わらず', '長い', 'の', 'が', '偉い', 'と', '思っ', 'て', 'いる', 'から', '、', '大半', 'パワハラ', '野郎', 'だ', 'が', '。', '日本', 'の', '未来', 'は', '···', '違う', '意味', 'で', 'Wow', 'Wow', 'だ', 'わ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'いい', 'ぞ', 'いい', 'ぞ', 'もっともっと', 'コロナ禍', 'が', '広がっ', 'て', '、', '世の中', 'が', '混乱', 'すれ', 'ば', 'いい', '混乱', 'から', '新た', 'な', '何', 'か', 'が', '生まれ', '、', 'それ', 'が', '今後', 'の', '標準', 'に', 'なる', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '今', 'の', 'うち', 'に', '都市部', 'から', '引っ越し', 'て', '、', '自給自足', 'の', '足固め', 'を', 'する', '人', 'が', '勝者', '。', '東京', 'とか', 'に', 'しがみつい', 'て', 'いる', '人', 'は', '敗者', 'に', 'なり', 'ます', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'イベント', '収益', 'が', '途絶え', 'たら', '母体', '自体', 'が', '耐え', 'きれ', 'ない', 'です', 'もの', 'ね', 'アーティスト', 'から', '切ら', 'ない', 'の', 'は', 'エイ', 'べ', 'の', '良心', 'な', 'ん', 'だろ', 'う', 'か', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '流石に', 'エイベックス', 'も', '１', '０', '０', '名', 'の', '人員削減', 'で', 'すか～', '・', '・', '・', '´', 'Д', '｀', 'コロナウイルス', 'も', '色々', 'な', '業界', 'に', '影響', 'を', '与え', 'ます', 'よ', 'ね', '～', '・', '・', '・', 'エイベックス', 'を', '辞め', 'て', 'から', '、', '何処', 'へ', '転職', 'する', 'の', 'でしょ', 'う', 'か', '？？？', '潰し', 'は', 'きく', 'の', 'でしょ', 'う', 'か', 'ね', '～', '・', '・', '・', '（', '＾', 'ω', '＾', '）', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '売上', 'から', 'すれ', 'ば', '450人', 'くらい', '楽', 'に', '養える', 'はず', 'だ', 'が', 'なあ', '・', '・', '・', '厳しい', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '芸能界', 'も', '、', '淘汰', 'す', 'べき', 'で', 'ある', '。', '下ら', 'ん', '番組', 'は', '、', '廃止', 'す', 'べき', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '夏', 'の', '由比ガ浜', 'に', '行く', 'と', '、', 'わが物顔', 'で', 'ここ', 'の', '社員', 'が', 'いる', 'けど', '、', '見た目', 'は', '完全', 'に', 'チンピラ', '入れ墨', '入っ', 'てる', 'し', '再就職', '出来る', 'ん', 'かい', 'な', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '大名行列', 'の', '人員', 'を', '削除', 'さ', 'れる', 'の', 'でしょ', 'う', 'か', '若しくは', '少数', 'の', '仕事', 'する', 'クリエイター', 'を', '削除', 'さ', 'れる', 'の', 'しょう', 'か', 'ご自愛ください', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '従業員', 'を', '減らす', 'の', 'なら', '、', 'それ', 'に', '合わせ', 'て', '無駄', 'な', '「', '役員', '」', 'って', 'やつ', 'ら', 'も', '減ら', 'せ', 'や', '。', 'クソ', 'の', '役', 'に', 'も', '立た', 'ない', 'の', 'が', '大半', 'な', 'ん', 'だ', 'から', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '上場企業', 'が', 'どんどん', '希望退職', 'という', '名', 'の', 'リストラ', '。', '中小企業', 'は', '雇用調整助成金', 'で', 'しのい', 'で', 'いる', 'が', '、', 'いつ', '飛ん', 'で', 'も', 'おかしく', 'ない', '状況', '。', 'この', 'よう', 'な', '状況下', 'で', '飲食店', 'は', '時間短縮', 'または', '営業', '自粛', '、', '県', 'を', 'またぐ', '移動', 'は', '止めよ', 'う', 'なんて', '言え', 'ます', 'か', '？', '横浜', '球場', 'で', '実験', 'を', 'し', 'たら', 'びっくり', 'する', 'ぐらい', 'の', '誹謗中傷', '。', '対策', 'は', 'し', 'なけれ', 'ば', 'なら', 'ない', 'が', '、', '感染者', '陽性', '者', 'が', '出', 'た', 'だけ', 'で', '騒ぎだす', '。', 'もう', '、', 'やめ', 'ませ', 'ん', 'か', '？', 'その', 'よう', 'な', '報道', '。', '欧米', 'と', '比べる', '気', 'は', 'ない', 'です', 'が', '、', '陽性', '者', 'も', 'わずか', 'で', '、', '死者数', 'も', '少ない', '。', 'このまま', 'で', 'は', '5年後', 'の', '日本', 'は', '失業者', 'で', '治安悪化', 'の', '最悪', 'な', '国', 'に', 'なっ', 'て', 'しまい', 'ます', 'よ', '。', '対策', 'を', 'し', 'つつ', '、', 'しっかり', '普通', 'の', '生活', 'を', 'おくる', 'よう', 'に', 'し', 'たい', 'ね', '。', '指定感染症', 'の', '変更', 'を', '切に', '望み', 'ます', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '夜', 'の', '街', 'で', '羽振り', 'の', '良かっ', 'た', 'ギョーカイ人', 'も', '一', '度', 'ぐらい', 'は', 'リストラ', 'の', '嵐', 'に', 'さらさ', 'れ', 'て', 'みる', 'と', 'いい', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'うらやましい', '。', 'うち', 'の', '会社', 'も', '早く', '実施', 'し', 'て', 'ほしい', 'なぁ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '4人', 'に', '1人', 'ぐらい', 'か', '。', '年度末', 'は', '凄い', 'こと', 'に', 'なる', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '世界中', 'が', '何', '京円', 'も', 'の', '損害賠償', 'を', '請求', 'し', 'た', 'いね', '、', 'あの国', 'に', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ライブ', 'なんて', 'なくなる', 'でしょ', '？', 'コロナ', 'は', '死滅', 'し', 'ない', '。', 'みんな', 'マスク', 'し', 'て', '距離', '保っ', 'て', '生きること', 'に', 'なる', 'から', 'ね', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '潰れる', '企業', 'コロナ', '恨めしや', '政府', 'に', 'お願い', 'コロナ', '潰し', 'て', '、', '自然に', '任せ', 'ない', 'で', '＞＜', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'もう', '日本', '終わっ', 'て', 'いる', 'ん', 'だ', 'から', '良い', 'だろ', 'う', '！', '滅', '流行っ', 'て', 'いる', 'こと', 'だ', 'し', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '酔っ', 'て', '？', '女', '追いかけ', 'て', '？', '？', 'ぶん殴っ', 'て', '？', '逮捕', 'さ', 'れ', 'て', '？', 'その', 'せい', 'で', '元', 'グループ', 'が', '解散', 'する', '事態', 'まで', 'なっ', 'てん', 'のに', 'まだ', '匿っ', 'てる', 'avex', 'は', 'マジ', 'な', 'ん', 'な', 'ん', 'です', 'か', '？', '？', '犯罪者', 'です', 'よ', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'スター', 'が', '1人', 'も', '居', 'なかっ', 'たら', '、', '潰れる', 'ん', 'じゃ', 'ない', 'の', 'w', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '希望', 'という', '名', 'の', '肩たたき', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'max松浦', 'の', '仕事', 'が', '遊び', 'で', '遊び', 'が', '仕事', '！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'AAA', 'が', '終わる', 'から', 'もう', '稼ぎ', '頭', 'が', 'ない', 'もん', 'な', '。', 'ダイス', '？', 'パラダイス', 'やろ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '何故', '？', '年齢', '層', 'な', 'ん', 'だろ', '？', '仕事', 'し', 'ない', '！', '出来', 'ない', '奴ら', 'から', 'ダローーー', '！♪', '39歳', 'は', '大丈夫', 'な', 'ん', 'だ', '！♪', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '40歳', '以上', 'が', '対象', 'かぁ', '！', 'やっぱり', '厳しい', 'な', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'マサ', '！', '見える', 'よ', '・', '・', '。', 'avex', 'の', '恐ろしい', '未来', 'が', '！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '半グレ', 'は', '暴対法', 'の', '範囲', 'だろ', '早く', '潰せ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '入る', 'の', 'に', '大変', 'そう', 'な', 'のに', '自ら', '出ん', 'だろ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '松浦氏', 'は', '疑惑', 'の', '薬物', 'を', '飲ん', 'でる', '場合', 'で', 'は', 'ない', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '薬', '会社', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'トラック運転手', 'や', '警備員', 'に', 'なる', '人', 'が', '多い', 'よう', 'です', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '薬', 'やっ', 'てる', '人', 'から', 'やめ', 'た', 'ほう', 'が', 'いい', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '潮時', 'かも', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '東方神起', 'Live', 'チケット', '返金', 'は', 'よー', 'し', 'て', 'よ', '！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '半グレ', 'あがり', 'の', '社長', 'が', 'まず', '退職', 'しろ', 'ばかたれ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '売れ', 'ない', 'ダンス', 'グループ', 'どんどん', '淘汰', 'さ', 'れ', 'そう', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '平和', '産業', 'は', '淘汰', 'さ', 'れる', 'の', 'か', '、', '、', '、', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'あそこ', 'の', '社長', 'の', '豪遊', 'っ', 'プリ', 'に', '相反', 'し', 'て', '、', '辞め', 'たい', 'やつ', 'は', '早め', 'に', '辞め', 'て', 'って', '。', 'ジャンキー', 'が', '！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '大麻', '吸っ', 'て', 'ラリ', 'っ', 'てる', '社長', 'の', '下', 'じゃま', 'とも', 'に', '仕事', '出来', 'ない', '、', 'って', '事', 'じゃなくて', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'え', 'むが', 'あゆ', 'を', 'もてあそん', 'だ', 'から', 'よー', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '今', 'まで', 'が', 'よかっ', 'た', 'だけ', 'でしょ', '良かっ', 'た', 'ね', '[SEP]']\n",
      "テキストのデータ :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ['[CLS]', '熱', 'も', '出', 'ない', '風邪', 'に', 'ビクビク', 'し', 'て', '自粛', '自粛', 'マスコミ', '、', '馬鹿', '厚労省', 'の', 'お陰', 'で', '今年', 'の', '年末', 'は', '失業者', '続', '失', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '薬', '中', '助け', 'て', '社員', 'を', '切る', '。', 'とんでも', 'ない', '会社', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'あの', '超', '有名', 'な', 'エーベックス', 'が', '！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '勿論', 'ウラ', 'で', 'は', '金食い虫', 'と', 'なっ', 'てる', 'だけ', 'の', '奴', 'を', '辞め', 'させる', 'ため', '、', '肩たたき', 'する', 'ん', 'だろ', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'この', '会社', 'も', 'あっち', 'へ', 'ころり', '、', 'そっち', 'へ', 'ころり', 'だ', 'もん', 'な', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '明るい', 'ニュース', 'ない', 'ね', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'これ', 'は', 'もしや', '…', 'の', 'まね', 'この', '呪い', '…', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '社畜', 'から', '脱皮', 'する', '好機', 'だ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'うわー', 'ﾟ', 'Д', 'ﾟ', 'イケイケ', 'の', 'エイベックス', 'で', 'すら', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'これ', 'でも', '自助', 'か', '。', '日本人', 'って', '鬼', 'だ', 'ねえ', '。', '鬼', '滅', 'の', '世界', 'だ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '松浦', 'やばい', 'し', 'イメージ', '最悪', '企業', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'あっち', 'は', '？', 'あの', 'エグザイル', 'の', '事務所', '、', 'どう', 'な', 'の', '？', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '山本寛', 'twilight', 'yutaka', 'こっち', 'も', 'か', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'あれ', 'ま', 'ー', 'こわ', 'こわ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '浜崎', 'キタ', 'よ', '！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '所詮', '、', '朝鮮', '企業', '、', '潰れ', 'た', '方', 'が', '良い', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'マサ', '…', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'エリカ様', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '足', '臭', '納豆', 'だ', 'よ', '、', 'マツ', 'ウラ', 'は', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '小室哲哉', 'さん', 'が', '代表', '成ら', 'こんな', '事', 'は', '無い', 'のに', 'ね', 'www', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'Kis', 'My', 'F', 't2', 'が', 'お世話', 'に', 'なっ', 'てる', 'から', '胸', 'が', '痛み', 'ます', '。', 'ＣＤ', '、', 'ＤＶＤ', 'を', '買う', 'しか', '出', 'きる', 'こと', 'が', '無い', '・', '・', '・', '大河の一滴', 'に', 'しか', 'なり', 'ませ', 'ん', 'が', '、', '頑張り', 'ます', '！', '！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'わ', 'て', 'の', 'ネジ', '工場', 'こうば', 'の', '職人', 'は', '守り', 'まっせ', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ネットワーク', 'の', '売り上げ', 'も', '絶好調', 'コロナ', 'で', 'も', 'なんの', 'その昔', 'ソニー', 'か', '不調', 'な', '頃', '「', '今', 'の', '状況', 'で', 'ネットワーク', 'へ', 'の', '投資', 'を', '続ける', '意味', 'ある', 'の', 'か', '？', '」', 'と', '質問', 'さ', 'れ', '当時', '社長', 'だっ', 'た', '平井', 'さん', 'は', '「', '今後', 'ネット', 'は', '何', '倍', 'も', 'の', '利益', 'に', 'なる', '」', 'と', '投資', 'を', '続け', 'た', '急', 'に', '好調', 'に', 'なっ', 'た', '訳', 'で', 'は', 'なく', 'そういう', '時', 'から', 'の', '投資', 'が', '重要', 'だ', 'と', '今', '絶好調', 'の', 'ソニー', 'を', '見', 'て', 'い', 'て', '思う', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', '多角経営', 'の', '強', 'さ', 'だ', 'なあ', 'コロナ', 'リスク', 'も', 'そうだ', 'が', 'ファーウェイ', 'の', 'アメリカ', 'と', 'の', '政治', 'リスク', 'という', '問題', 'で', 'の', 'マイナス', 'も', '他', 'で', '埋め', 'れる', '強み', 'が', 'ソニー', 'に', 'は', 'ある', 'そういった', '多方面', 'で', 'の', '強', 'さ', 'を', '研い', 'て', 'き', 'た', 'ソニー', 'に対して', '古い', '価値観', 'の', '人', 'が', '難癖', 'つける', 'こと', '多かっ', 'た', 'が', '時代', 'に', 'あっ', 'て', '形', 'を', '変える', 'の', 'は', '当たり前', 'で', 'ソニー', 'は', 'それ', 'を', 'し', 'て', 'き', 'た', 'から', '今', 'の', '強', 'さ', 'が', 'ある', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'アニプレックス', '強い', 'よ', 'ね', '。', '単なる', '円盤', '屋', 'だっ', 'た', 'のに', 'いつの間にか', '市場', 'の', '先頭', 'に', 'たっ', 'て', 'い', 'た', '感じ', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'イメージセンサー', 'の', '分野', 'で', 'も', '、', 'ソニー', 'は', '負け', 'て', 'しまう', 'の', 'でしょ', 'う', 'か', '？', '誰か', '詳しい', '人', '、', '教え', 'て', '下さい', '！', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'ソニー', 'も', '調子', 'の', '良い', 'うち', 'に', 'イメセン', '事業', 'を', '上場', 'もしくは', '売却', 'する', 'べき', '。', '今', 'なら', '５', '兆', '円', '前後', 'で', '売却', '可能', 'だろ', 'う', '。', 'やはり', '半導体事業', 'は', 'リスク', 'が', '高い', '。', '勝ち逃げ', 'を', 'する', 'の', 'も', '立派', 'な', '経営戦略', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'プレステ5', 'で', 'さらに', '勢い', 'に', '乗り', 'ます', 'ね', '世界', 'に', '通用', 'する', '日本ブランド', 'は', 'SONY', 'か', '任天堂', 'だけ', 'に', 'なっ', 'て', 'しまっ', 'た', '。', '[SEP]']\n",
      "テキストのデータ :\n",
      " ['[CLS]', 'やはり', '不景気', 'に', 'なっ', 'て', 'も', '強い', 'の', 'は', '子供', 'と', '女性向け', 'の', '商品', 'な', 'の', 'だろ', 'う', '。', '[SEP]']\n",
      "[60, 45, 57, 214, 55, 45, 93, 113, 82, 151, 35, 60, 72, 114, 32, 95, 60, 51, 29, 42, 52, 48, 37, 33, 53, 23, 32, 26, 31, 29, 31, 46, 41, 20, 19, 208, 25, 41, 11, 29, 24, 27, 75, 31, 11, 6, 21, 8, 35, 6, 32, 40, 84, 20, 19, 23, 13, 11, 16, 9, 9, 37, 56, 68, 59, 52, 43, 82, 25, 17, 16, 40, 77, 73, 7, 101, 181, 61, 53, 131, 44, 67, 81, 133, 76, 216, 128, 41, 88, 148, 40, 121, 91, 74, 61, 127, 106, 94, 74, 130, 75, 118, 87, 95, 103, 87, 64, 129, 215, 49, 36, 98, 121, 77, 54, 132, 73, 118, 137, 79, 40, 46, 68, 93, 75, 102, 37, 53, 46, 126, 81, 56, 173, 112, 52, 56, 58, 46, 54, 71, 115, 48, 45, 81, 51, 67, 25, 66, 123, 71, 53, 52, 37, 66, 42, 50, 44, 67, 72, 88, 82, 53, 97, 82, 56, 41, 48, 173, 44, 38, 33, 52, 99, 38, 45, 48, 72, 41, 52, 43, 45, 57, 29, 31, 31, 41, 42, 47, 43, 43, 24, 76, 58, 33, 41, 30, 40, 38, 64, 34, 61, 38, 39, 49, 44, 40, 54, 96, 24, 34, 78, 21, 24, 41, 29, 33, 32, 34, 41, 99, 43, 26, 28, 27, 20, 30, 30, 78, 28, 56, 25, 26, 28, 37, 23, 21, 82, 18, 90, 31, 18, 20, 25, 22, 32, 24, 17, 20, 26, 14, 63, 28, 39, 23, 14, 20, 45, 30, 16, 30, 18, 27, 43, 25, 29, 33, 37, 34, 12, 13, 48, 10, 24, 26, 19, 22, 25, 19, 28, 26, 32, 19, 33, 18, 28, 23, 22, 37, 28, 25, 12, 21, 12, 19, 14, 27, 23, 15, 14, 13, 39, 40, 19, 23, 28, 16, 15, 18, 17, 63, 12, 19, 15, 16, 14, 28, 16, 14, 18, 15, 19, 12, 31, 20, 13, 14, 20, 20, 19, 14, 17, 46, 18, 17, 11, 13, 19, 17, 20, 16, 12, 11, 17, 22, 21, 18, 12, 14, 8, 24, 10, 11, 13, 15, 15, 12, 12, 17, 11, 9, 7, 13, 11, 8, 26, 10, 11, 13, 10, 18, 15, 8, 13, 10, 6, 10, 9, 15, 8, 14, 10, 15, 20, 5, 8, 6, 12, 11, 12, 17, 7, 10, 12, 11, 5, 15, 7, 6, 8, 10, 6, 22, 10, 7, 13, 7, 5, 12, 7, 15, 28, 14, 10, 13, 26, 7, 35, 82, 149, 145, 224, 65, 46, 164, 80, 132, 34, 34, 31, 67, 21, 20, 33, 28, 40, 161, 27, 15, 16, 17, 26, 18, 21, 49, 16, 7, 12, 19, 27, 12, 15, 10, 12, 15, 4, 12, 12, 5, 12, 12, 11, 13, 27, 22, 10, 12, 27, 14, 9, 23, 15, 6, 12, 8, 11, 19, 8, 14, 9, 7, 6, 12, 4, 3, 12, 14, 43, 13, 91, 86, 24, 26, 46, 26, 21]\n",
      "max_token_number: 224\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "\n",
    "# 学習データX\n",
    "# feature.csvは上記で用意したファイルのパスを指定してください\n",
    "train_features_df = pd.read_csv('./datasets/finetuning/train/features.csv')\n",
    "print(\"train_features_df:\\n\", train_features_df)\n",
    "\n",
    "\n",
    "# テキストの最初に[CLS]、最後に'[SEP]をつけて単語数を数える関数\n",
    "def _get_indice(feature):\n",
    "    tokens = []\n",
    "    tokens.append('[CLS]')\n",
    "    pre_text = preprocessing_text(feature)#追加\n",
    "    tokenized_text = tokenizer_mecab(pre_text)#追加\n",
    "    tokens.extend(tokenized_text)#追加\n",
    "    #tokens.extend(sp.encode_as_pieces(feature))# sentence piece\n",
    "    tokens.append('[SEP]')\n",
    "    print(\"テキストのデータ :\\n\",tokens)\n",
    "    number = len(tokens)\n",
    "    return number\n",
    "\n",
    "# sentence pieceでは、与えた文書の中で高い頻度で現れるフレーズは、\n",
    "# 多少長くても一つの単位として認識します。\n",
    "# Mecabでは対応する辞書を使って文章を分割します。辞書にはneologdとかがよく使われます。\n",
    "#これでも上手くいくことも多いですが、語彙数が大きくなってしまうことや、\n",
    "# 分割の仕方が分割したいデータセットに適していないこともあり、問題点\n",
    "#sp = spm.SentencePieceProcessor()\n",
    "\n",
    "# ダウンロードした事前学習モデルのパスを指定してください\n",
    "#sp.Load('./downloads/bert-wiki-ja/wiki-ja.model')\n",
    "\n",
    "numbers = []\n",
    "for feature in train_features_df['feature']:\n",
    "    features_number = _get_indice(feature)\n",
    "    numbers.append(features_number)\n",
    "\n",
    "print(numbers)\n",
    "\n",
    "# 最大トークン数\n",
    "max_token_num = max(numbers)\n",
    "print(\"max_token_number: \" + str(max_token_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTの設定ファイル、モデルのロード  \n",
    "学習回数と事前に調べていた最大トークン数、ファイルパスを自分用に書き換えてください。以下に書き換える箇所を示します。  \n",
    "\n",
    "- config_path：設定ファイルのパス  \n",
    "- checkpoint_path：事前学習モデルのファイルパス  \n",
    "    - 拡張子まで書かないでください  \n",
    "- SEQ_LEN：最大トークン数  \n",
    "- EPOCH：学習回数  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        [(None, 224)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      [(None, 224)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 224, 768), ( 24576000    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 224, 768)     1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 224, 768)     0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 224, 768)     172032      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 224, 768)     0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 224, 768)     1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 224, 768)     0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 224, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 224, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 224, 768)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 224, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 224, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 224, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 224, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 224, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 224, 768)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 224, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 224, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 224, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 224, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 224, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 224, 768)     0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 224, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 224, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 224, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 224, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 224, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 224, 768)     0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 224, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 224, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 224, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 224, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 224, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 224, 768)     0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 224, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 224, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 224, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 224, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 224, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 224, 768)     0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 224, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 224, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 224, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 224, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 224, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 224, 768)     0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 224, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 224, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 224, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 224, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 224, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 224, 768)     0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 224, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 224, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 224, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 224, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 224, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 224, 768)     0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 224, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 224, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 224, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 224, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 224, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 224, 768)     0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 224, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 224, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 224, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 224, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 224, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 224, 768)     0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 224, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 224, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 224, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 224, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 224, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 224, 768)     0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 224, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 224, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Dense (Dense)               (None, 224, 768)     590592      Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Norm (LayerNormalization)   (None, 224, 768)     1536        MLM-Dense[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Sim (EmbeddingSimilarity)   (None, 224, 32000)   32000       MLM-Norm[0][0]                   \n",
      "                                                                 Embedding-Token[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Masked (InputLayer)       [(None, 224)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "NSP-Dense (Dense)               (None, 768)          590592      Extract[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "MLM (Masked)                    (None, 224, 32000)   0           MLM-Sim[0][0]                    \n",
      "                                                                 Input-Masked[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "NSP (Dense)                     (None, 2)            1538        NSP-Dense[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 111,021,826\n",
      "Trainable params: 111,021,826\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "\n",
    "#sys.pathに追加（必要なのか調査が必要）\n",
    "sys.path.append('modules')\n",
    "\n",
    "#import pprint\n",
    "#pprint.pprint(sys.path)\n",
    "\n",
    "# BERTのロード\n",
    "config_path = './downloads/bert-wiki-ja_config/bert_finetuning_config_v1.json'\n",
    "# 拡張子まで記載しない（.ckptファイルで保存されている）\n",
    "checkpoint_path = './downloads//bert-wiki-ja/model.ckpt-1400000'\n",
    "\n",
    "# 最大のトークン数\n",
    "SEQ_LEN = max_token_num#上の処理の出力\n",
    "BATCH_SIZE = 16\n",
    "BERT_DIM = 768\n",
    "LR = 1e-4\n",
    "# 学習回数\n",
    "EPOCH = 20\n",
    "\n",
    "# 学習ずみモデルでモデル構築\n",
    "bert = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True,  trainable=True, seq_len=SEQ_LEN)\n",
    "bert.summary()\n",
    "\n",
    "# この後に追加する（転移学習）\n",
    "# 分類問題用にモデルの再構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データのロード関数\n",
    "こちらの関数でもモデルの読み込みを行うので、各自ファイルパスの変更をお願いします。\n",
    "\n",
    "- sp.load(\"ファイルパス\")\n",
    "\n",
    "## 文章のベクトル化\n",
    "_get_indice関数では、SentencePieceとwikipediaモデルを使用し文章のベクトル化を行っています\n",
    "\n",
    "## 学習データ読込\n",
    "_load_labeldata関数は学習データを読込、_get_indice関数を用いて特徴量を抽出しています。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import utils\n",
    "import numpy as np\n",
    "\n",
    "# ここでもsentence piece\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('./downloads/bert-wiki-ja/wiki-ja.model')\n",
    "\n",
    "# 上に同じ名前の関数があるので注意\n",
    "# 最大単語数分のID化された文を返す関数\n",
    "# maxlenがなくてエラーになるので勝手に追加（maxlenは最大単語数か？）\n",
    "def _get_indice(feature, maxlen):\n",
    "#def _get_indice(feature):\n",
    "    # インデックス ０で埋める\n",
    "    indices = np.zeros((maxlen), dtype = np.int32)\n",
    "    # 最初に[CLS]、最後に'[SEP]をつけてトークン作る\n",
    "    tokens = []\n",
    "    tokens.append('[CLS]')\n",
    "    pre_text = preprocessing_text(feature)#追加\n",
    "    tokenized_text = tokenizer_mecab(pre_text)#追加\n",
    "    tokens.extend(tokenized_text)#追加\n",
    "    #tokens.extend(sp.encode_as_pieces(feature))# sentence piece\n",
    "    tokens.append('[SEP]')\n",
    "\n",
    "    for t, token in enumerate(tokens):\n",
    "        # 最大単語数までトークンの単語をindicesに入れていく\n",
    "        if t >= maxlen:\n",
    "            break\n",
    "        try:\n",
    "            indices[t] = sp.piece_to_id(token)# id化してくれる？\n",
    "        except:\n",
    "            logging.warn(f'{token} is unknown.')# コメントしてくれる\n",
    "            indices[t] = sp.piece_to_id('<unk>')# id化してくれる？unknown\n",
    "    \n",
    "    # 最大単語数分のID化された文を返す\n",
    "    return indices\n",
    "\n",
    "\n",
    "#勝手に追加 maxlen=103  \n",
    "# 引数のパスは直接書けばいらないかも\n",
    "def _load_labeldata(train_dir, test_dir, maxlen):\n",
    "    # pandasでcsvの学習データとテストデータを読み込む\n",
    "    train_features_df = pd.read_csv(f'{train_dir}/features.csv')\n",
    "    train_labels_df = pd.read_csv(f'{train_dir}/labels.csv')\n",
    "    test_features_df = pd.read_csv(f'{test_dir}/features.csv')\n",
    "    test_labels_df = pd.read_csv(f'{test_dir}/labels.csv')\n",
    "    \n",
    "    ##### ラベル側の処理 #####\n",
    "    \n",
    "    # ラベルのユニーク値を取り出す（ラベル数）（インデックスとラベル別別に保管）\n",
    "    # ネガポジなら　ポジティブ, ネガティブ と　０、１　を入れてしまえばいいと思われる\n",
    "    #{'スポーツ': 0, '携帯電話': 1},\n",
    "    label2index = {k: i for i, k in enumerate(train_labels_df['label'].unique())}\n",
    "    #{0: 'スポーツ', 1: '携帯電話'}\n",
    "    index2label = {i: k for i, k in enumerate(train_labels_df['label'].unique())}\n",
    "    #　クラス数（何種類に分類するか）ネガポジなら２\n",
    "    class_count = len(label2index)\n",
    "    \n",
    "    # Numpyユーティリティ to_categorical(y, nb_classes=None)\n",
    "    # クラスベクトル（0からnb_classesまでの整数）を categorical_crossentropyとともに用いるためのバイナリのクラス行列に変換します．\n",
    "    # y: 行列に変換するクラスベクトル, nb_classes: 総クラス数\n",
    "    # ↓trainのラベルを文字からインデックスを使用して変換\n",
    "    train_labels = utils.np_utils.to_categorical([label2index[label] for label in train_labels_df['label']], num_classes=class_count)\n",
    "    #　testのインデックスをまず作る\n",
    "    test_label_indices = [label2index[label] for label in test_labels_df['label']]\n",
    "    # ↓testのラベルを文字からインデックスを使用して変換\n",
    "    test_labels = utils.np_utils.to_categorical(test_label_indices, num_classes=class_count)\n",
    "\n",
    "    ##### 特徴量側の処理 #####\n",
    "    \n",
    "    train_features = []\n",
    "    test_features = []\n",
    "    for feature in train_features_df['feature']:\n",
    "        # 上で作った関数 _get_indice  を使ってID化\n",
    "        train_features.append(_get_indice(feature, maxlen))\n",
    "    # shape(len(train_features), maxlen)のゼロの行列作成\n",
    "    train_segments = np.zeros((len(train_features), maxlen), dtype = np.float32)\n",
    "\n",
    "    for feature in test_features_df['feature']:\n",
    "        # 上で作った関数 _get_indice  を使ってID化\n",
    "        test_features.append(_get_indice(feature, maxlen))\n",
    "    # shape(len(test_features), maxlen)のゼロの行列作成\n",
    "    test_segments = np.zeros((len(test_features), maxlen), dtype = np.float32)\n",
    "\n",
    "    print(f'Trainデータ数: {len(train_features_df)}, Testデータ数: {len(test_features_df)}, ラベル数: {class_count}')\n",
    "\n",
    "    return {\n",
    "        'class_count': class_count,\n",
    "        'label2index': label2index,\n",
    "        'index2label': index2label,\n",
    "        'train_labels': train_labels,\n",
    "        'test_labels': test_labels,\n",
    "        'test_label_indices': test_label_indices,\n",
    "        'train_features': np.array(train_features),\n",
    "        'train_segments': np.array(train_segments),\n",
    "        'test_features': np.array(test_features),\n",
    "        'test_segments': np.array(test_segments),\n",
    "        'input_len': maxlen\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4, 828,   5,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_indice(\"制作\", maxlen=SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainデータ数: 495, Testデータ数: 20, ラベル数: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'class_count': 2,\n",
       " 'label2index': {'positive': 0, 'negative': 1},\n",
       " 'index2label': {0: 'positive', 1: 'negative'},\n",
       " 'train_labels': array([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]], dtype=float32),\n",
       " 'test_labels': array([[0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.]], dtype=float32),\n",
       " 'test_label_indices': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0],\n",
       " 'train_features': array([[    4,     0,    10, ...,     0,     0,     0],\n",
       "        [    4,    99,    10, ...,     0,     0,     0],\n",
       "        [    4, 28877,    10, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    4,  5274,    30, ...,     0,     0,     0],\n",
       "        [    4,     0,    19, ...,     0,     0,     0],\n",
       "        [    4,  5770,     0, ...,     0,     0,     0]], dtype=int32),\n",
       " 'train_segments': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " 'test_features': array([[    4,   650,    28, ...,     0,     0,     0],\n",
       "        [    4, 30168,   324, ...,     0,     0,     0],\n",
       "        [    4,  8318,    13, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    4, 28877,    10, ...,     0,     0,     0],\n",
       "        [    4,     0,    10, ...,     0,     0,     0],\n",
       "        [    4, 28877,   341, ...,     0,     0,     0]], dtype=int32),\n",
       " 'test_segments': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " 'input_len': 224}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_load_labeldata('./datasets/finetuning/train', './datasets/finetuning/test', maxlen=SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル作成関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# おそらく、この関数を作った理由は複数分類モデルを自由に作れるようにしたかったからだ。\n",
    "# 単にネガポジにするなら関数にしないで直接書けばいい。\n",
    "\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional, Flatten, GlobalMaxPooling1D\n",
    "from keras_bert.layers import MaskedGlobalMaxPool1D\n",
    "from keras import Input, Model\n",
    "\n",
    "# nadam を選べば使わなくてもいい\n",
    "# https://github.com/CyberZHG/keras-bert\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "\n",
    "def _create_model(input_shape, class_count):\n",
    "    # AdamWarmupをオプティマイザーとして使用するために必要な情報を得る関数\n",
    "    # nadam を選べば使わなくてもいい\n",
    "    decay_steps, warmup_steps = calc_train_steps(\n",
    "        input_shape[0],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCH,\n",
    "    )\n",
    "\n",
    "    # 学習済みモデル 「bert」 の最終出力層のoutputを取り出す\n",
    "    bert_last = bert.get_layer(name='NSP-Dense').output\n",
    "    x1 = bert_last\n",
    "    # 最終出力層のoutputを新規作成した全結合層に入れる\n",
    "    output_tensor = Dense(class_count, activation='softmax')(x1)\n",
    "    \n",
    "    # Trainableの場合は、Input Masked Layerが3番目の入力なりますが、\n",
    "    # FineTuning時には必要無いので1, 2番目の入力だけ使用します。\n",
    "    # Trainableでなければkeras-bertのModel.inputそのままで問題ありません。\n",
    "    model = Model([bert.input[0], bert.input[1]], output_tensor)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=LR),\n",
    "                  #optimizer='nadam',\n",
    "                  metrics=['mae', 'mse', 'acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データのロードとモデルの準備\n",
    "事前準備で作成した学習用データと学習後のモデル名および出力先を指定してください。\n",
    "\n",
    "- trains_dir,tests_dir：学習用データのパス\n",
    "- model_filename：学習後のモデル名、出力先のパス\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras import Input, Model, utils\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainデータ数: 61, Testデータ数: 10, ラベル数: 2\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        [(None, 214)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      [(None, 214)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 214, 768), ( 24576000    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 214, 768)     1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 214, 768)     0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 214, 768)     164352      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 214, 768)     0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 214, 768)     1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 214, 768)     0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 214, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 214, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 214, 768)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 214, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 214, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 214, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 214, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 214, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 214, 768)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 214, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 214, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 214, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 214, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 214, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 214, 768)     0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 214, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 214, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 214, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 214, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 214, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 214, 768)     0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 214, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 214, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 214, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 214, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 214, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 214, 768)     0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 214, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 214, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 214, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 214, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 214, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 214, 768)     0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 214, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 214, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 214, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 214, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 214, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 214, 768)     0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 214, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 214, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 214, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 214, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 214, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 214, 768)     0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 214, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 214, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 214, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 214, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 214, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 214, 768)     0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 214, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 214, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 214, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 214, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 214, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 214, 768)     0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 214, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 214, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 214, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 214, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 214, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 214, 768)     0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 214, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 214, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 214, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 214, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 214, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 214, 768)     0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 214, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 214, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "NSP-Dense (Dense)               (None, 768)          590592      Extract[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            1538        NSP-Dense[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 110,390,018\n",
      "Trainable params: 110,390,018\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# データロードとモデルの準備\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "trains_dir = './datasets/finetuning/train'\n",
    "tests_dir = './datasets/finetuning/test'\n",
    "\n",
    "#上で作った関数\n",
    "data = _load_labeldata(trains_dir, tests_dir, SEQ_LEN)\n",
    "\n",
    "# モデルの読み込み\n",
    "model_filename = './downloads/models/knbc_finetuning.model'\n",
    "\n",
    "# 上で作った関数（関数を使わずに直接書くこともできる）\n",
    "# data['train_features'].shape　は　文の数×最大単語数　＝　特徴量Xのインプットshape\n",
    "# data['class_count']　は　クラスの数\n",
    "model = _create_model(data['train_features'].shape, data['class_count'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の実行\n",
    "いよいよ学習の実行です。以下のプログラムを実行した際に画像のような出力が出ると思います。（tensorflowのバージョンでWarningが出ますが問題ありません）あとはお茶でも飲みながら学習経過を観察してみましょう！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ファインチューニング？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4/4 [==============================] - 683s 155s/step - loss: 0.8308 - mae: 0.5402 - mse: 0.3145 - acc: 0.3660 - val_loss: 0.6466 - val_mae: 0.4584 - val_mse: 0.2277 - val_acc: 0.6000\n",
      "INFO:tensorflow:Assets written to: ./downloads/models/knbc_finetuning.model/assets\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 435s 110s/step - loss: 0.4363 - mae: 0.3271 - mse: 0.1339 - acc: 0.8871 - val_loss: 0.6272 - val_mae: 0.3890 - val_mse: 0.2299 - val_acc: 0.6000\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 364s 96s/step - loss: 0.1853 - mae: 0.1532 - mse: 0.0432 - acc: 0.9705 - val_loss: 0.7420 - val_mae: 0.4041 - val_mse: 0.2704 - val_acc: 0.6000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([data['train_features'], data['train_segments']],\n",
    "          data['train_labels'],\n",
    "          epochs = 3,\n",
    "          #epochs = EPOCH,\n",
    "          batch_size = BATCH_SIZE,\n",
    "          validation_data=([data['test_features'], data['test_segments']], data['test_labels']),\n",
    "          shuffle=False,\n",
    "          verbose = 1,\n",
    "          callbacks = [\n",
    "              ModelCheckpoint(monitor='val_acc', mode='max', filepath=model_filename, save_best_only=True)\n",
    "          ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論時にAttention Weightを出力するようにモデルをロードする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "\n",
    "# custom_objects \n",
    "# custom_objects引数を使ってロード機構にそのカスタムレイヤーなどを渡すことができます\n",
    "#model = load_model(model_filename, custom_objects=SeqSelfAttention.get_custom_objects())\n",
    "\n",
    "#ModelクラスAPI   model = Model(inputs=a, outputs=b)  model = Model(inputs=[a1, a2], outputs=[b1, b2, b3])\n",
    "# model.get_layer('attention').output 2つ目のアウトプットを指定\n",
    "#model = Model(inputs=model.input, outputs=[model.output, model.get_layer('attention').output])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測 TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_features_df = pd.read_csv('./datasets/pred_labeling/features_001.csv')\n",
    "tests_features_df.loc[0]['feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-68845e032196>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# BERTの学習したモデルの読込（ダウンロードした？勝手に保存される？）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./downloads/models/knbc_finetuning.model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_custom_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m#model = load_model(model_filename, custom_objects=SeqSelfAttention.get_custom_objects())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Encoder-12-MultiHeadSelfAttention'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m   raise IOError(\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    142\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_node\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeras_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mnodes_to_load\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeras_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m   \u001b[0mloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m   \u001b[0;31m# Finalize the loaded layers and remove the extra tracked dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0mnode\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfilter\u001b[0m \u001b[0mto\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m   \"\"\"\n\u001b[0;32m--> 765\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0m\u001b[1;32m    890\u001b[0m                             ckpt_options, filters)\n\u001b[1;32m    891\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, filters)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_export_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexport_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     self._concrete_functions = (\n\u001b[0;32m--> 131\u001b[0;31m         function_deserialization.load_function_def_library(\n\u001b[0m\u001b[1;32m    132\u001b[0m             meta_graph.graph_def.library))\n\u001b[1;32m    133\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/saved_model/function_deserialization.py\u001b[0m in \u001b[0;36mload_function_def_library\u001b[0;34m(library, load_shared_name_suffix)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0m_restore_gradient_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenamed_functions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_list_function_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibrary_function_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m       \u001b[0mfunctions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/site-packages/tensorflow/python/saved_model/function_deserialization.py\u001b[0m in \u001b[0;36m_list_function_deps\u001b[0;34m(fdef, library_function_names)\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0mdeps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWhichOneof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"func\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m           \u001b[0mdeps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/_collections_abc.py\u001b[0m in \u001b[0;36mitems\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;34m\"D.items() -> a set-like object providing a view on D's items\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mItemsView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/_collections_abc.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mapping)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0m__slots__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_mapping'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from keras import utils\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_bert import get_custom_objects\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "#sys.pathに追加（必要なのか調査が必要）\n",
    "sys.path.append('modules')\n",
    "\n",
    "# SentencePieceProccerモデルの読込\n",
    "spp = spm.SentencePieceProcessor()\n",
    "spp.Load('./downloads/bert-wiki-ja/wiki-ja.model')\n",
    "\n",
    "# BERTの学習したモデルの読込（ダウンロードした？勝手に保存される？）\n",
    "model_filename = './downloads/models/knbc_finetuning.model'\n",
    "model = load_model(model_filename, custom_objects=get_custom_objects())\n",
    "#model = load_model(model_filename, custom_objects=SeqSelfAttention.get_custom_objects())\n",
    "model = Model(inputs=model.input, outputs=[model.output, model.get_layer('Encoder-12-MultiHeadSelfAttention').output])\n",
    "# ↑ここでmodel = Model(inputs=a, outputs=b) としてAttentionも出すようにする。\n",
    "\n",
    "\n",
    "# 上のと同じのを入れると思われるため、消していいかも(ファイルを分けるなら必要)\n",
    "#SEQ_LEN = 103#206\n",
    "maxlen = SEQ_LEN\n",
    "#\n",
    "# 上にあったのと同じ？ → predictように変更\n",
    "def _get_indice_pred(feature, maxlen):\n",
    "    indices = np.zeros((maxlen), dtype=np.int32)\n",
    "\n",
    "    tokens = []\n",
    "    tokens.append('[CLS]')\n",
    "    pre_text = preprocessing_text(feature)#追加\n",
    "    tokenized_text = tokenizer_mecab(pre_text)#追加\n",
    "    tokens.extend(tokenized_text)#追加\n",
    "    #tokens.extend(spp.encode_as_pieces(feature))\n",
    "    tokens.append('[SEP]')\n",
    "\n",
    "    for t, token in enumerate(tokens):\n",
    "        if t >= maxlen:\n",
    "            break\n",
    "        try:\n",
    "            indices[t] = spp.piece_to_id(token)\n",
    "        except:\n",
    "            logging.warn('unknown')\n",
    "            indices[t] = spp.piece_to_id('<unk>')\n",
    "            \n",
    "    return indices, tokens\n",
    "\n",
    "tests_features_df = pd.read_csv('./datasets/pred_labeling/features_001.csv')\n",
    "feature = tests_features_df.loc[0]['feature']\n",
    "#feature = \"昨日は携帯電話を買いに行った。\"\n",
    "#feature = \"昨日はスポーツしに行った。\"\n",
    "\n",
    "test_features = []\n",
    "indices, tokens = _get_indice_pred(feature, maxlen)\n",
    "test_features.append(indices)\n",
    "\n",
    "#勝手に追加\n",
    "test_features = np.array(test_features)\n",
    "\n",
    "test_segments = np.zeros(\n",
    "    (len(test_features), maxlen), dtype=np.float32)\n",
    "\n",
    "\n",
    "# model = Modelを使えば推定　predict[0][0]　２次元のリストで返せる。\n",
    "predicted_test_labels = model.predict([test_features, test_segments])#.argmax(axis=1)\n",
    "#predict = model.predict(test_features)\n",
    "\n",
    "\n",
    "print(\"完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'イギリスから戻って来た\\n布袋寅泰氏はどうするのかね\\n緊急事態宣言に\\n劇場とかは含まれないみたいだけど\\n全国からファンを\\n武道館に来させるのかな\\n中止もしくは\\n武道館に来なくても払い戻しします\\n位の措置は取った方がいいと思う\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests_features_df.loc[0]['feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  2\n",
      "predict:  [[0.59652174 0.40347835]]\n",
      "predict:  [0]\n",
      "attention: (1, 214, 768)\n",
      "attention:\n",
      " [[[-0.15614392  0.11198843 -0.7218799  ... -0.43427202 -0.15799451\n",
      "    0.13034657]\n",
      "  [-0.20469886  0.40216246 -0.79378    ... -0.36917427 -0.27232108\n",
      "    0.30159944]\n",
      "  [ 0.22955036  0.48005208 -0.7393608  ... -0.8082622   0.52140796\n",
      "   -0.10452686]\n",
      "  ...\n",
      "  [-0.04775316  0.3499331  -0.30924207 ... -0.11876121 -0.22986516\n",
      "    0.13258928]\n",
      "  [-0.00219479  0.08754978 -0.21091847 ... -0.09936933 -0.21315327\n",
      "    0.09863827]\n",
      "  [-0.04547665  0.01744612 -0.15949327 ... -0.13492337 -0.17773315\n",
      "    0.08752056]]]\n",
      "attention:\n",
      " [[ 29  23  10  44  12  31   6  35  52  55  11  12  55  35  45  42   2  35\n",
      "   18  56   4  36  35  10  36  54  35  54  34  34   4  34  34  10   5  57\n",
      "    5  24  18  34  21  24  36  29  24  24  42  31  43  34   1  11  41  31\n",
      "   52  31   2  57  31  35  58  34  24  17  11   5   3  58  39  34   2  22\n",
      "   34  27  22  18  59  59  34 207  59  34   2  45  24  54   2  58  11  48\n",
      "   54   2  22  35  54   2  29  42  35  16  11   4   2  11  39  35   6   4\n",
      "   41  35  56  34  55  34  59  35  50  35  11  59  44   6   2   2   2  56\n",
      "   18   6  58  34  45  24  52  45  35  59  45  45  18   2  54  36  27  58\n",
      "   19  45  10  36  42  35  18  38  18  24  41  42   2  34  56   2  52  34\n",
      "   31  43  58  58  42  34  58  55  42  43  29  24  34  31  10  12   5  11\n",
      "   34   5  35   2  45  31   4  18   2  24   2   6  11  41  29  54  29  45\n",
      "   42  58   4  22  58  18  24   2  42  21  34  56   6  54  24   4  59   1\n",
      "   42   2  10  55  24   3  58   4   5   5  34  52  55  44   5  10  43  59\n",
      "    2  34  58  59  54  38  36  38  40   2  24  54  35  43  12  54   5  42\n",
      "    5  12  42  42   2  42  44  46  41  35  24  12  44  24  43  58  41   5\n",
      "   41  54  24  44  42  11  52  55   2  11   4   6  59  36  34  42  35  36\n",
      "    6  44  34  22  45  34   2  57  44  55  36  54   3  31  27   6  34 111\n",
      "   34  45   6  31  45   6  12   5   5  35  11  18   2  44  45  11  42  43\n",
      "   34  18  38  58  21  36  43   2  35  54  27   4  45  34  31  18  16  34\n",
      "   58  58  24  10  57  21  24  42  19  42  11  58  59  43  59  54  11  10\n",
      "   43   4  34  43   4  45  34  45  18  57   2  59   2  36   6  34  59  31\n",
      "   58  29  10  11  43  24   2  11  52  35  17  18  24  54  35  47  54  56\n",
      "   22  35  42  21  45  34  45  59  55  11  35  44   4  44  24  36   5  34\n",
      "   13  24  34  18  52  52   4  11  42  44  36  24  54 158  57  36   2  24\n",
      "   35  39  40  16   5  11  45  31  43   4  18   5  16  10   6   2  31  43\n",
      "   31  27  42  45  43  31   3  42  58  43  43   4  11   5  27  59   9  45\n",
      "   34  11  18  55  22  24  24  34  24   6   4   2  10  29  58   9  42  11\n",
      "    4  21  11  19  57  13  24  42  57   5  54  58  10  24   9  10  36  24\n",
      "   43  44  42  44  18  12  42  29   4  48   4  35   1  12   5  34  35  44\n",
      "   59  45  34   2  24  11  43   6  35  12  35  35   6  86  42  45  11  57\n",
      "   34  34  57   2  43  23   6  57  29  23  52   6  24  24 154  89   4  51\n",
      "   31  11  42  52   6  28  34  59  59   2   2   2   1  36  27  54  30   4\n",
      "   41   3  35  55  33  59  29  24   4  11  27   6  59  59  59   3  36   1\n",
      "   41  55  10  57  58  36  42   2  59  19  42   6  59  50  29   4   6  41\n",
      "   41  56  34  42  59  59  42  35  22  10  24  18  10   6  45  42  36   2\n",
      "   27  41  34  44  54  57  31  57  45   4  57  41  29  34  35   4  34   2\n",
      "   55  59  34   5  45  59  22  31  34  34  55   6  58   5  16  34   2  12\n",
      "   35  43  18  45  34   4  24  52  24  36  29  38  43  18  34  36  41  58\n",
      "    6  21  42  24  43  42  18   5  57   5   2  34  59  54  56  19   4   4\n",
      "   31  22  36  36  36   4   5  10  42  45  24  10   2  21  43  34   3  35\n",
      "   42  34  10   9  31  34  41  57  59  27  11  29  42  59   4  59  18  36\n",
      "   45  16  18  24  45  52  58   9  58   1  44   2  59  19  44  50   6  18\n",
      "   57  59  45  44  57  35  34   2  11  10   2   5]]\n",
      "attention:\n",
      " (1, 768)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"shape: \", len(predicted_test_labels))\n",
    "#print(\"predicted_test_labels: \", predicted_test_labels)\n",
    "print(\"predict: \", predicted_test_labels[0])\n",
    "print(\"predict: \", predicted_test_labels[0].argmax(axis=1))\n",
    "print(\"attention:\", predicted_test_labels[1].shape)\n",
    "print(\"attention:\\n\", predicted_test_labels[1])\n",
    "print(\"attention:\\n\", predicted_test_labels[1].argmax(axis=1))#axis=1\n",
    "print(\"attention:\\n\", predicted_test_labels[1].argmax(axis=1).shape)#axis=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "Encoder-12-MultiHeadSelfAttenti (None, 206, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
    "                                                                 Encoder-12-MultiHeadSelfAttention\n",
    "__________________________________________________________________________________________________\n",
    "Encoder-12-MultiHeadSelfAttenti (None, 206, 768)     1536        Encoder-12-MultiHeadSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_08dba252_5ee0_11eb_8651_367dda54a607row0_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row1_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row2_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row3_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row4_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row5_col3 {\n",
       "            background-color:  #cbdef1;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row6_col3 {\n",
       "            background-color:  #f5f9fe;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row7_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row8_col3 {\n",
       "            background-color:  #7ab6d9;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row9_col3 {\n",
       "            background-color:  #63a8d3;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row10_col3 {\n",
       "            background-color:  #3282be;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row11_col3 {\n",
       "            background-color:  #d9e8f5;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row12_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row13_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row14_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row15_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row16_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row17_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row18_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row19_col3 {\n",
       "            background-color:  #dce9f6;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row20_col3 {\n",
       "            background-color:  #e0ecf8;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row21_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row22_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row23_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row24_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row25_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row26_col3 {\n",
       "            background-color:  #91c3de;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row27_col3 {\n",
       "            background-color:  #d9e8f5;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row28_col3 {\n",
       "            background-color:  #d9e7f5;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row29_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row30_col3 {\n",
       "            background-color:  #f3f8fe;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row31_col3 {\n",
       "            background-color:  #d9e8f5;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row32_col3 {\n",
       "            background-color:  #2a7ab9;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row33_col3 {\n",
       "            background-color:  #d4e4f4;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row34_col3 {\n",
       "            background-color:  #b4d3e9;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row35_col3 {\n",
       "            background-color:  #ecf4fb;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row36_col3 {\n",
       "            background-color:  #ccdff1;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row37_col3 {\n",
       "            background-color:  #6caed6;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row38_col3 {\n",
       "            background-color:  #f4f9fe;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row39_col3 {\n",
       "            background-color:  #6aaed6;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row40_col3 {\n",
       "            background-color:  #9cc9e1;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row41_col3 {\n",
       "            background-color:  #08306b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row42_col3 {\n",
       "            background-color:  #d4e4f4;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row43_col3 {\n",
       "            background-color:  #f4f9fe;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row44_col3 {\n",
       "            background-color:  #bad6eb;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row45_col3 {\n",
       "            background-color:  #4292c6;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row46_col3 {\n",
       "            background-color:  #2c7cba;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row47_col3 {\n",
       "            background-color:  #b3d3e8;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row48_col3 {\n",
       "            background-color:  #eef5fc;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row49_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row50_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row51_col3 {\n",
       "            background-color:  #cddff1;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row52_col3 {\n",
       "            background-color:  #81badb;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row53_col3 {\n",
       "            background-color:  #cde0f1;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row54_col3 {\n",
       "            background-color:  #a8cee4;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row55_col3 {\n",
       "            background-color:  #d5e5f4;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row56_col3 {\n",
       "            background-color:  #8dc1dd;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row57_col3 {\n",
       "            background-color:  #92c4de;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row58_col3 {\n",
       "            background-color:  #3787c0;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row59_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_08dba252_5ee0_11eb_8651_367dda54a607row60_col3 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_08dba252_5ee0_11eb_8651_367dda54a607\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >token</th>        <th class=\"col_heading level0 col1\" >weight</th>        <th class=\"col_heading level0 col2\" >rank</th>        <th class=\"col_heading level0 col3\" >normalized</th>        <th class=\"col_heading level0 col4\" >attention</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row0_col0\" class=\"data row0 col0\" >[CLS]</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row0_col1\" class=\"data row0 col1\" >0.481102</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row0_col2\" class=\"data row0 col2\" >56.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row0_col4\" class=\"data row0 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row1_col0\" class=\"data row1 col0\" >イギリス</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row1_col1\" class=\"data row1 col1\" >0.502085</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row1_col2\" class=\"data row1 col2\" >52.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row1_col4\" class=\"data row1 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row2_col0\" class=\"data row2 col0\" >から</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row2_col1\" class=\"data row2 col1\" >0.469868</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row2_col2\" class=\"data row2 col2\" >57.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row2_col4\" class=\"data row2 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row3_col0\" class=\"data row3 col0\" >戻っ</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row3_col1\" class=\"data row3 col1\" >0.539775</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row3_col2\" class=\"data row3 col2\" >50.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row3_col4\" class=\"data row3 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row4_col0\" class=\"data row4 col0\" >て</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row4_col1\" class=\"data row4 col1\" >0.560874</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row4_col2\" class=\"data row4 col2\" >49.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row4_col3\" class=\"data row4 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row4_col4\" class=\"data row4 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row5_col0\" class=\"data row5 col0\" >来</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row5_col1\" class=\"data row5 col1\" >0.766989</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row5_col2\" class=\"data row5 col2\" >20.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row5_col3\" class=\"data row5 col3\" >0.062607</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row5_col4\" class=\"data row5 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row6_col0\" class=\"data row6 col0\" >た</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row6_col1\" class=\"data row6 col1\" >0.708416</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row6_col2\" class=\"data row6 col2\" >38.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row6_col3\" class=\"data row6 col3\" >0.004033</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row6_col4\" class=\"data row6 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row7_col0\" class=\"data row7 col0\" >布袋寅泰</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row7_col1\" class=\"data row7 col1\" >0.594768</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row7_col2\" class=\"data row7 col2\" >48.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row7_col3\" class=\"data row7 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row7_col4\" class=\"data row7 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row8_col0\" class=\"data row8 col0\" >氏</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row8_col1\" class=\"data row8 col1\" >0.834359</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row8_col2\" class=\"data row8 col2\" >10.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row8_col3\" class=\"data row8 col3\" >0.129977</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row8_col4\" class=\"data row8 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row9_col0\" class=\"data row9 col0\" >は</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row9_col1\" class=\"data row9 col1\" >0.851253</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row9_col2\" class=\"data row9 col2\" >7.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row9_col3\" class=\"data row9 col3\" >0.146870</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row9_col4\" class=\"data row9 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row10_col0\" class=\"data row10 col0\" >どう</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row10_col1\" class=\"data row10 col1\" >0.896754</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row10_col2\" class=\"data row10 col2\" >4.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row10_col3\" class=\"data row10 col3\" >0.192372</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row10_col4\" class=\"data row10 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row11_col0\" class=\"data row11 col0\" >する</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row11_col1\" class=\"data row11 col1\" >0.746680</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row11_col2\" class=\"data row11 col2\" >29.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row11_col3\" class=\"data row11 col3\" >0.042297</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row11_col4\" class=\"data row11 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row12_col0\" class=\"data row12 col0\" >の</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row12_col1\" class=\"data row12 col1\" >0.640538</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row12_col2\" class=\"data row12 col2\" >45.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row12_col3\" class=\"data row12 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row12_col4\" class=\"data row12 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row13_col0\" class=\"data row13 col0\" >か</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row13_col1\" class=\"data row13 col1\" >0.525669</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row13_col2\" class=\"data row13 col2\" >51.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row13_col3\" class=\"data row13 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row13_col4\" class=\"data row13 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row14_col0\" class=\"data row14 col0\" >ね</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row14_col1\" class=\"data row14 col1\" >0.467445</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row14_col2\" class=\"data row14 col2\" >58.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row14_col3\" class=\"data row14 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row14_col4\" class=\"data row14 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row15_col0\" class=\"data row15 col0\" >緊急事態宣言</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row15_col1\" class=\"data row15 col1\" >0.429307</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row15_col2\" class=\"data row15 col2\" >60.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row15_col3\" class=\"data row15 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row15_col4\" class=\"data row15 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row16_col0\" class=\"data row16 col0\" >に</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row16_col1\" class=\"data row16 col1\" >0.676887</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row16_col2\" class=\"data row16 col2\" >42.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row16_col3\" class=\"data row16 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row16_col4\" class=\"data row16 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row17_col0\" class=\"data row17 col0\" >劇場</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row17_col1\" class=\"data row17 col1\" >0.685778</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row17_col2\" class=\"data row17 col2\" >41.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row17_col3\" class=\"data row17 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row17_col4\" class=\"data row17 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row18_col0\" class=\"data row18 col0\" >とか</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row18_col1\" class=\"data row18 col1\" >0.650617</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row18_col2\" class=\"data row18 col2\" >44.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row18_col3\" class=\"data row18 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row18_col4\" class=\"data row18 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row19_col0\" class=\"data row19 col0\" >は</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row19_col1\" class=\"data row19 col1\" >0.743405</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row19_col2\" class=\"data row19 col2\" >31.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row19_col3\" class=\"data row19 col3\" >0.039022</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row19_col4\" class=\"data row19 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row20_col0\" class=\"data row20 col0\" >含ま</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row20_col1\" class=\"data row20 col1\" >0.736985</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row20_col2\" class=\"data row20 col2\" >32.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row20_col3\" class=\"data row20 col3\" >0.032603</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row20_col4\" class=\"data row20 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row21_col0\" class=\"data row21 col0\" >れ</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row21_col1\" class=\"data row21 col1\" >0.497371</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row21_col2\" class=\"data row21 col2\" >53.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row21_col3\" class=\"data row21 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row21_col4\" class=\"data row21 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row22_col0\" class=\"data row22 col0\" >ない</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row22_col1\" class=\"data row22 col1\" >0.604266</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row22_col2\" class=\"data row22 col2\" >46.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row22_col3\" class=\"data row22 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row22_col4\" class=\"data row22 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row23_col0\" class=\"data row23 col0\" >みたい</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row23_col1\" class=\"data row23 col1\" >0.651004</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row23_col2\" class=\"data row23 col2\" >43.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row23_col3\" class=\"data row23 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row23_col4\" class=\"data row23 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row24_col0\" class=\"data row24 col0\" >だ</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row24_col1\" class=\"data row24 col1\" >0.596751</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row24_col2\" class=\"data row24 col2\" >47.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row24_col3\" class=\"data row24 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row24_col4\" class=\"data row24 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row25_col0\" class=\"data row25 col0\" >けど</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row25_col1\" class=\"data row25 col1\" >0.462235</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row25_col2\" class=\"data row25 col2\" >59.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row25_col3\" class=\"data row25 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row25_col4\" class=\"data row25 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row26_col0\" class=\"data row26 col0\" >全国</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row26_col1\" class=\"data row26 col1\" >0.819271</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row26_col2\" class=\"data row26 col2\" >13.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row26_col3\" class=\"data row26 col3\" >0.114888</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row26_col4\" class=\"data row26 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row27_col0\" class=\"data row27 col0\" >から</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row27_col1\" class=\"data row27 col1\" >0.746181</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row27_col2\" class=\"data row27 col2\" >30.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row27_col3\" class=\"data row27 col3\" >0.041798</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row27_col4\" class=\"data row27 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row28_col0\" class=\"data row28 col0\" >ファン</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row28_col1\" class=\"data row28 col1\" >0.747831</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row28_col2\" class=\"data row28 col2\" >27.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row28_col3\" class=\"data row28 col3\" >0.043448</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row28_col4\" class=\"data row28 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row29_col0\" class=\"data row29 col0\" >を</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row29_col1\" class=\"data row29 col1\" >0.703794</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row29_col2\" class=\"data row29 col2\" >39.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row29_col3\" class=\"data row29 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row29_col4\" class=\"data row29 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row30_col0\" class=\"data row30 col0\" >武道館</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row30_col1\" class=\"data row30 col1\" >0.709949</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row30_col2\" class=\"data row30 col2\" >35.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row30_col3\" class=\"data row30 col3\" >0.005566</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row30_col4\" class=\"data row30 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row31_col0\" class=\"data row31 col0\" >に</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row31_col1\" class=\"data row31 col1\" >0.746788</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row31_col2\" class=\"data row31 col2\" >28.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row31_col3\" class=\"data row31 col3\" >0.042406</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row31_col4\" class=\"data row31 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row32_col0\" class=\"data row32 col0\" >来さ</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row32_col1\" class=\"data row32 col1\" >0.905651</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row32_col2\" class=\"data row32 col2\" >2.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row32_col3\" class=\"data row32 col3\" >0.201268</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row32_col4\" class=\"data row32 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row33_col0\" class=\"data row33 col0\" >せる</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row33_col1\" class=\"data row33 col1\" >0.754531</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row33_col2\" class=\"data row33 col2\" >25.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row33_col3\" class=\"data row33 col3\" >0.050149</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row33_col4\" class=\"data row33 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row34_col0\" class=\"data row34 col0\" >の</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row34_col1\" class=\"data row34 col1\" >0.790338</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row34_col2\" class=\"data row34 col2\" >18.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row34_col3\" class=\"data row34 col3\" >0.085955</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row34_col4\" class=\"data row34 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row35_col0\" class=\"data row35 col0\" >か</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row35_col1\" class=\"data row35 col1\" >0.720522</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row35_col2\" class=\"data row35 col2\" >33.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row35_col3\" class=\"data row35 col3\" >0.016140</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row35_col4\" class=\"data row35 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row36_col0\" class=\"data row36 col0\" >な</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row36_col1\" class=\"data row36 col1\" >0.766193</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row36_col2\" class=\"data row36 col2\" >21.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row36_col3\" class=\"data row36 col3\" >0.061810</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row36_col4\" class=\"data row36 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row37_col0\" class=\"data row37 col0\" >中止</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row37_col1\" class=\"data row37 col1\" >0.844358</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row37_col2\" class=\"data row37 col2\" >9.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row37_col3\" class=\"data row37 col3\" >0.139975</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row37_col4\" class=\"data row37 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row38_col0\" class=\"data row38 col0\" >もしくは</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row38_col1\" class=\"data row38 col1\" >0.709322</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row38_col2\" class=\"data row38 col2\" >36.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row38_col3\" class=\"data row38 col3\" >0.004940</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row38_col4\" class=\"data row38 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row39_col0\" class=\"data row39 col0\" >武道</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row39_col1\" class=\"data row39 col1\" >0.845382</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row39_col2\" class=\"data row39 col2\" >8.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row39_col3\" class=\"data row39 col3\" >0.140999</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row39_col4\" class=\"data row39 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row40_col0\" class=\"data row40 col0\" >館</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row40_col1\" class=\"data row40 col1\" >0.810958</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row40_col2\" class=\"data row40 col2\" >15.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row40_col3\" class=\"data row40 col3\" >0.106575</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row40_col4\" class=\"data row40 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row41_col0\" class=\"data row41 col0\" >に</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row41_col1\" class=\"data row41 col1\" >0.984851</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row41_col2\" class=\"data row41 col2\" >1.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row41_col3\" class=\"data row41 col3\" >0.280468</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row41_col4\" class=\"data row41 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row42_col0\" class=\"data row42 col0\" >来</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row42_col1\" class=\"data row42 col1\" >0.754556</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row42_col2\" class=\"data row42 col2\" >24.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row42_col3\" class=\"data row42 col3\" >0.050174</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row42_col4\" class=\"data row42 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row43_col0\" class=\"data row43 col0\" >なく</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row43_col1\" class=\"data row43 col1\" >0.709263</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row43_col2\" class=\"data row43 col2\" >37.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row43_col3\" class=\"data row43 col3\" >0.004880</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row43_col4\" class=\"data row43 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row44_col0\" class=\"data row44 col0\" >て</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row44_col1\" class=\"data row44 col1\" >0.785228</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row44_col2\" class=\"data row44 col2\" >19.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row44_col3\" class=\"data row44 col3\" >0.080845</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row44_col4\" class=\"data row44 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row45_col0\" class=\"data row45 col0\" >も</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row45_col1\" class=\"data row45 col1\" >0.879555</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row45_col2\" class=\"data row45 col2\" >6.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row45_col3\" class=\"data row45 col3\" >0.175172</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row45_col4\" class=\"data row45 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row46_col0\" class=\"data row46 col0\" >払い戻し</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row46_col1\" class=\"data row46 col1\" >0.903224</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row46_col2\" class=\"data row46 col2\" >3.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row46_col3\" class=\"data row46 col3\" >0.198842</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row46_col4\" class=\"data row46 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row47_col0\" class=\"data row47 col0\" >し</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row47_col1\" class=\"data row47 col1\" >0.791536</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row47_col2\" class=\"data row47 col2\" >17.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row47_col3\" class=\"data row47 col3\" >0.087153</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row47_col4\" class=\"data row47 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row48_col0\" class=\"data row48 col0\" >ます</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row48_col1\" class=\"data row48 col1\" >0.717539</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row48_col2\" class=\"data row48 col2\" >34.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row48_col3\" class=\"data row48 col3\" >0.013156</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row48_col4\" class=\"data row48 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row49_col0\" class=\"data row49 col0\" >位</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row49_col1\" class=\"data row49 col1\" >0.693690</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row49_col2\" class=\"data row49 col2\" >40.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row49_col3\" class=\"data row49 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row49_col4\" class=\"data row49 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row50\" class=\"row_heading level0 row50\" >50</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row50_col0\" class=\"data row50 col0\" >の</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row50_col1\" class=\"data row50 col1\" >0.488158</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row50_col2\" class=\"data row50 col2\" >54.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row50_col3\" class=\"data row50 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row50_col4\" class=\"data row50 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row51\" class=\"row_heading level0 row51\" >51</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row51_col0\" class=\"data row51 col0\" >措置</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row51_col1\" class=\"data row51 col1\" >0.765567</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row51_col2\" class=\"data row51 col2\" >22.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row51_col3\" class=\"data row51 col3\" >0.061184</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row51_col4\" class=\"data row51 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row52\" class=\"row_heading level0 row52\" >52</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row52_col0\" class=\"data row52 col0\" >は</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row52_col1\" class=\"data row52 col1\" >0.830301</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row52_col2\" class=\"data row52 col2\" >11.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row52_col3\" class=\"data row52 col3\" >0.125919</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row52_col4\" class=\"data row52 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row53\" class=\"row_heading level0 row53\" >53</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row53_col0\" class=\"data row53 col0\" >取っ</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row53_col1\" class=\"data row53 col1\" >0.764475</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row53_col2\" class=\"data row53 col2\" >23.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row53_col3\" class=\"data row53 col3\" >0.060093</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row53_col4\" class=\"data row53 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row54\" class=\"row_heading level0 row54\" >54</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row54_col0\" class=\"data row54 col0\" >た</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row54_col1\" class=\"data row54 col1\" >0.801531</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row54_col2\" class=\"data row54 col2\" >16.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row54_col3\" class=\"data row54 col3\" >0.097148</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row54_col4\" class=\"data row54 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row55\" class=\"row_heading level0 row55\" >55</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row55_col0\" class=\"data row55 col0\" >方</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row55_col1\" class=\"data row55 col1\" >0.753344</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row55_col2\" class=\"data row55 col2\" >26.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row55_col3\" class=\"data row55 col3\" >0.048961</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row55_col4\" class=\"data row55 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row56\" class=\"row_heading level0 row56\" >56</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row56_col0\" class=\"data row56 col0\" >が</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row56_col1\" class=\"data row56 col1\" >0.820886</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row56_col2\" class=\"data row56 col2\" >12.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row56_col3\" class=\"data row56 col3\" >0.116503</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row56_col4\" class=\"data row56 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row57\" class=\"row_heading level0 row57\" >57</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row57_col0\" class=\"data row57 col0\" >いい</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row57_col1\" class=\"data row57 col1\" >0.817315</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row57_col2\" class=\"data row57 col2\" >14.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row57_col3\" class=\"data row57 col3\" >0.112932</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row57_col4\" class=\"data row57 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row58\" class=\"row_heading level0 row58\" >58</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row58_col0\" class=\"data row58 col0\" >と</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row58_col1\" class=\"data row58 col1\" >0.890706</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row58_col2\" class=\"data row58 col2\" >5.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row58_col3\" class=\"data row58 col3\" >0.186324</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row58_col4\" class=\"data row58 col4\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row59\" class=\"row_heading level0 row59\" >59</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row59_col0\" class=\"data row59 col0\" >思う</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row59_col1\" class=\"data row59 col1\" >0.486055</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row59_col2\" class=\"data row59 col2\" >55.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row59_col3\" class=\"data row59 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row59_col4\" class=\"data row59 col4\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08dba252_5ee0_11eb_8651_367dda54a607level0_row60\" class=\"row_heading level0 row60\" >60</th>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row60_col0\" class=\"data row60 col0\" >[SEP]</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row60_col1\" class=\"data row60 col1\" >0.387316</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row60_col2\" class=\"data row60 col2\" >61.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row60_col3\" class=\"data row60 col3\" >0.000000</td>\n",
       "                        <td id=\"T_08dba252_5ee0_11eb_8651_367dda54a607row60_col4\" class=\"data row60 col4\" >False</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe627299af0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 入力シーケンスはpad_sequenceにより、以下の様に0でpre paddingしています。\n",
    "# [0 0 0 0 x1(300) x2(300) x3(300)]\n",
    "# Attention Weightは入力シーケンスに対応して計算されるため、\n",
    "# 入力シーケンスのpadding分シフトします。\n",
    "weights = [w.max() for w in predicted_test_labels[1][0][-len(tokens):]]\n",
    "df = pd.DataFrame([tokens, weights], index=['token', 'weight']).T\n",
    "\n",
    "mean = np.asarray(weights).mean()#np.asarray　参照コピー\n",
    "\n",
    "df['rank'] = df['weight'].rank(ascending=False)#ランキング\n",
    "# wから平均を引いた値が0より大きいものだけ（偏差）\n",
    "df['normalized'] = df['weight'].apply(lambda w: max(w - mean, 0))#行全体や列全体に対して、同じ操作\n",
    "df['weight'] = df['weight'].astype('float32')\n",
    "df['attention'] = df['normalized'] > 0\n",
    "# df.style.background_gradient で色つけ\n",
    "df = df.style.background_gradient(cmap='Blues', subset=['normalized'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測 Excel出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name:  features_001.csv\n",
      "pred_list:  [[-1], [-1]]\n",
      "good_ratio_list:  [[7.961538461538462], [11.466666666666667]]\n",
      "\n",
      "File Name:  features_003.csv\n",
      "pred_list:  [[-1], [-1]]\n",
      "good_ratio_list:  [[6.413897280966768], [9.019292604501608]]\n",
      "\n",
      "y_train:  [0, 0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from keras import utils\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_bert import get_custom_objects\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "#sys.pathに追加（必要なのか調査が必要）\n",
    "sys.path.append('modules')\n",
    "\n",
    "# 上にあったのと同じ？ → predict用に変更\n",
    "def _get_indice_pred(feature, maxlen):\n",
    "    indices = np.zeros((maxlen), dtype=np.int32)\n",
    "\n",
    "    tokens = []\n",
    "    tokens.append('[CLS]')\n",
    "    pre_text = preprocessing_text(feature)#追加\n",
    "    tokenized_text = tokenizer_mecab(pre_text)#追加\n",
    "    tokens.extend(tokenized_text)#追加\n",
    "    #tokens.extend(spp.encode_as_pieces(feature))\n",
    "    tokens.append('[SEP]')\n",
    "\n",
    "    for t, token in enumerate(tokens):\n",
    "        if t >= maxlen:\n",
    "            break\n",
    "        try:\n",
    "            indices[t] = spp.piece_to_id(token)\n",
    "        except:\n",
    "            logging.warn('unknown')\n",
    "            indices[t] = spp.piece_to_id('<unk>')\n",
    "            \n",
    "    return indices, tokens\n",
    "\n",
    "\n",
    "# SentencePieceProccerモデルの読込\n",
    "spp = spm.SentencePieceProcessor()\n",
    "spp.Load('./downloads/bert-wiki-ja/wiki-ja.model')\n",
    "\n",
    "# BERTの学習したモデルの読込（ダウンロードした？勝手に保存される？）\n",
    "model_filename = './downloads/models/knbc_finetuning.model'\n",
    "model = load_model(model_filename, custom_objects=get_custom_objects())\n",
    "#model = load_model(model_filename, custom_objects=SeqSelfAttention.get_custom_objects())\n",
    "model = Model(inputs=model.input, outputs=[model.output, model.get_layer('Encoder-12-MultiHeadSelfAttention').output])\n",
    "# ↑ここでmodel = Model(inputs=a, outputs=b) としてAttentionも出すようにする。\n",
    "\n",
    "\n",
    "# 上のと同じのを入れると思われるため、消していいかも(ファイルを分けるなら必要)\n",
    "#SEQ_LEN = 103#206\n",
    "maxlen = SEQ_LEN\n",
    "\n",
    "y_train = []\n",
    "#for i in range(file_count):\n",
    "for i in range(3):\n",
    "    n_file = str(i+1).zfill(3)\n",
    "    file_name = \"features_\" + n_file + \".csv\"\n",
    "    f_path = (\"./datasets/pred_labeling/\" + file_name)\n",
    "    if not os.path.isfile(f_path):\n",
    "        continue\n",
    "\n",
    "    df_tests_features = pd.read_csv(f_path)\n",
    "    print(\"File Name: \", file_name)\n",
    "    \n",
    "    #excelファイル保管用\n",
    "    excel_file = './attention_excel/attention_' + n_file + '.xlsx'\n",
    "    writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')\n",
    "        \n",
    "    wb = openpyxl.Workbook()\n",
    "    sheet = wb.active\n",
    "    sheet.title = 'Cover'\n",
    "    c1 = sheet[\"A1\"]\n",
    "    c2 = sheet[\"A2\"]\n",
    "    c1.value =  \"Attention出力用ファイルです。\"\n",
    "    c2.value =  \"詳細は次のシート以降を参照してください。\"\n",
    "    wb.save(excel_file)\n",
    "        \n",
    "        \n",
    "    pred_list = []\n",
    "    good_ratio_list = []\n",
    "#    for j in range(len(df_tests_features)):\n",
    "    for j in range(2):\n",
    "        feature = df_tests_features.loc[j]['feature']\n",
    "\n",
    "        test_features = []\n",
    "        indices, tokens = _get_indice_pred(feature, maxlen)\n",
    "        test_features.append(indices)\n",
    "\n",
    "        #勝手に追加\n",
    "        test_features = np.array(test_features)\n",
    "\n",
    "        test_segments = np.zeros(\n",
    "            (len(test_features), maxlen), dtype=np.float32)\n",
    "\n",
    "        # model = Modelを使えば推定　predict[0][0]　２次元のリストで返せる。\n",
    "        predicted = model.predict([test_features, test_segments])#.argmax(axis=1)\n",
    "        #predict = model.predict(test_features)\n",
    "\n",
    "        y_pred = predicted[0].argmax(axis=1)\n",
    "        #print(\"tokens: \", tokens)\n",
    "        #print(\"predict: \", y_pred[0])\n",
    "        \n",
    "        if y_pred[0] > 0.5:\n",
    "            pred_list.append([1])\n",
    "        else:\n",
    "            pred_list.append([-1])\n",
    "        \n",
    "        \n",
    "        # 高評価度算出\n",
    "        good = df_tests_features.loc[j]['good']\n",
    "        bad = df_tests_features.loc[j]['bad']\n",
    "        \n",
    "        if bad == 0:\n",
    "            good_ratio = [0]\n",
    "        else:\n",
    "            good_ratio = [good/bad]\n",
    "        \n",
    "        good_ratio_list.append(good_ratio)\n",
    "        \n",
    "        \n",
    "        # 入力シーケンスはpad_sequenceにより、以下の様に0でpre paddingしています。\n",
    "        # [0 0 0 0 x1(300) x2(300) x3(300)] ←３００は (None, 11, 300) の\n",
    "        # Attention Weightは入力シーケンスに対応して計算されるため、\n",
    "        # 入力シーケンスのpadding分シフトします。\n",
    "        #weights = [w.max() for w in predicted[1][0][-len(tokens):]]\n",
    "        weights = [w.max() for w in predicted[1][0]]#[-len(tokens):]\n",
    "        df = pd.DataFrame([tokens, weights], index=['token', 'weight']).T\n",
    "\n",
    "        mean = np.asarray(weights).mean()#np.asarray　参照コピー\n",
    "\n",
    "        df['rank'] = df['weight'].rank(ascending=False)#ランキング\n",
    "        # wから平均を引いた値が0より大きいものだけ（偏差）\n",
    "        df['normalized'] = df['weight'].apply(lambda w: max(w - mean, 0))#行全体や列全体に対して、同じ操作\n",
    "        df['weight'] = df['weight'].astype('float32')\n",
    "        df['attention'] = df['normalized'] > 0\n",
    "        # df.style.background_gradient で色つけ\n",
    "        df = df.style.background_gradient(cmap='Blues', subset=['normalized'])\n",
    "\n",
    "        # excel に保存\n",
    "        sheetname=\"comment\" + str(j)\n",
    "        with pd.ExcelWriter(excel_file, engine=\"openpyxl\", mode=\"a\") as writer:\n",
    "            df.to_excel(writer, sheet_name=sheetname, index=False)\n",
    "        \n",
    "        #display(df)\n",
    "    \n",
    "    # 加重平均が０より大きいか\n",
    "    y = np.array(pred_list)*np.array(good_ratio_list)\n",
    "    \n",
    "    if y.mean() > 0:\n",
    "        y_train.append(1) \n",
    "    else:\n",
    "        y_train.append(0) \n",
    "        \n",
    "    print(\"pred_list: \", pred_list)\n",
    "    print(\"good_ratio_list: \", good_ratio_list)\n",
    "    print()\n",
    "\n",
    "y_train_df = pd.DataFrame(y_train)\n",
    "y_train_df.columns = [\"label\"]\n",
    "y_train_df.to_csv(\"./datasets/y_train.csv\")\n",
    "print(\"y_train: \", y_train)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ↑完成"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
